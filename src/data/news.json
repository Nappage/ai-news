{
  "lastUpdated": "2025-11-05T06:27:59.361Z",
  "totalArticles": 32,
  "articles": [
    {
      "id": "mhlm8gal4k35l5x9tvd",
      "title": "Brazilâ€™s AI moment is here",
      "summary": "Brazil is now one of the most engaged countries in the world when it comes to AI. From classrooms to farms and small businesses, Brazilians are using OpenAI products to learn, create, and drive innovation.",
      "content": "Brazil is now one of the most engaged countries in the world when it comes to AI. From classrooms to farms and small businesses, Brazilians are using OpenAI products to learn, create, and drive innovation.",
      "publishedAt": "2025-11-04T10:00:00.000Z",
      "source": "OpenAI Blog",
      "sourceUrl": "https://openai.com/global-affairs/brazil-ai-moment-is-here",
      "category": "companies",
      "company": "OpenAI",
      "imageUrl": null,
      "tags": [
        "AI",
        "OpenAI"
      ],
      "featured": true
    },
    {
      "id": "mhlm8j380kyx3hnp8ne9",
      "title": "Beyond Request-Response: Architecting Real-time Bidirectional Streaming Multi-agent System",
      "summary": "The blog post argues the request-response model fails for advanced multi-agent AI. It advocates for a real-time bidirectional streaming architecture, implemented by the Agent Development Kit (ADK). This streaming model enables true concurrency, natural interruptibility, and unified multimodal proces...",
      "content": "The blog post argues the request-response model fails for advanced multi-agent AI. It advocates for a real-time bidirectional streaming architecture, implemented by the Agent Development Kit (ADK). This streaming model enables true concurrency, natural interruptibility, and unified multimodal processing. ADK's core features are real-time I/O management, stateful sessions for agent handoffs, and streaming-native tools.",
      "publishedAt": "2025-11-03T06:27:29.060Z",
      "source": "Google Developers Blog",
      "sourceUrl": "https://developers.googleblog.com/en/beyond-request-response-architecting-real-time-bidirectional-streaming-multi-agent-system/",
      "category": "tools",
      "company": "Google",
      "imageUrl": null,
      "tags": [
        "AI"
      ],
      "featured": true
    },
    {
      "id": "mhlm8j38avdy85ahjd6",
      "title": "Introducing the Jules extension for Gemini CLI",
      "summary": "Introducing the Jules extension for Gemini CLI, an autonomous sidekick for developers. It accelerates coding workflows by offloading tasks like asynchronous work, bug fixes, and changes in new branches to Jules, while you stay in flow with Gemini CLI. Get started by installing the extension and usin...",
      "content": "Introducing the Jules extension for Gemini CLI, an autonomous sidekick for developers. It accelerates coding workflows by offloading tasks like asynchronous work, bug fixes, and changes in new branches to Jules, while you stay in flow with Gemini CLI. Get started by installing the extension and using the /jules command to initiate and check task statuses.",
      "publishedAt": "2025-11-03T06:27:29.060Z",
      "source": "Google Developers Blog",
      "sourceUrl": "https://developers.googleblog.com/en/introducing-the-jules-extension-for-gemini-cli/",
      "category": "tools",
      "company": "Google",
      "imageUrl": null,
      "tags": [
        "Gemini"
      ],
      "featured": true
    },
    {
      "id": "mhlm8gal511wchac1ww",
      "title": "AWS and OpenAI announce multi-year strategic partnership",
      "summary": "OpenAI and AWS have entered a multi-year, $38 billion partnership to scale advanced AI workloads. AWS will provide world-class infrastructure and compute capacity to power OpenAIâ€™s next generation of models.",
      "content": "OpenAI and AWS have entered a multi-year, $38 billion partnership to scale advanced AI workloads. AWS will provide world-class infrastructure and compute capacity to power OpenAIâ€™s next generation of models.",
      "publishedAt": "2025-11-03T06:00:00.000Z",
      "source": "OpenAI Blog",
      "sourceUrl": "https://openai.com/index/aws-and-openai-partnership",
      "category": "companies",
      "company": "OpenAI",
      "imageUrl": null,
      "tags": [
        "AI",
        "OpenAI"
      ],
      "featured": true
    },
    {
      "id": "mhlm8j39ibuqk3546k9",
      "title": "Say hello to a new level of interactivity in Gemini CLI",
      "summary": "We're excited to announce an enhancement to Gemini CLI that makes your workflow more powerful a...",
      "content": "We&#x27;re excited to announce an enhancement to Gemini CLI that makes your workflow more powerful a...",
      "publishedAt": "2025-11-02T06:27:29.061Z",
      "source": "Google Developers Blog",
      "sourceUrl": "https://developers.googleblog.com/en/say-hello-to-a-new-level-of-interactivity-in-gemini-cli/",
      "category": "tools",
      "company": "Google",
      "imageUrl": null,
      "tags": [
        "Gemini"
      ],
      "featured": true
    },
    {
      "id": "mhlm8jynswaeqdnnid",
      "title": "New tools in Google AI Studio to explore, debug and share logs",
      "summary": "Weâ€™re introducing a new logs and datasets feature in Google AI Studio.",
      "content": "<img src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/0057-AIfD-Logging-Datasets-Keyw.max-600x600.format-webp_jqmcLSK.webp\">Weâ€™re introducing a new logs and datasets feature in Google AI Studio.",
      "publishedAt": "2025-10-30T17:05:00.000Z",
      "source": "Google Technology Blog",
      "sourceUrl": "https://blog.google/technology/developers/google-ai-studio-logs-datasets/",
      "category": "companies",
      "company": "Google",
      "imageUrl": null,
      "tags": [
        "AI",
        "Google"
      ],
      "featured": true
    },
    {
      "id": "mhlm8galdf5lcfadgtf",
      "title": "Expanding Stargate to Michigan",
      "summary": "OpenAI is expanding Stargate to Michigan with a new one-gigawatt campus that strengthens Americaâ€™s AI infrastructure. The project will create jobs, drive investment, and support economic growth across the Midwest.",
      "content": "OpenAI is expanding Stargate to Michigan with a new one-gigawatt campus that strengthens Americaâ€™s AI infrastructure. The project will create jobs, drive investment, and support economic growth across the Midwest.",
      "publishedAt": "2025-10-30T13:30:00.000Z",
      "source": "OpenAI Blog",
      "sourceUrl": "https://openai.com/index/expanding-stargate-to-michigan",
      "category": "companies",
      "company": "OpenAI",
      "imageUrl": null,
      "tags": [
        "AI",
        "OpenAI"
      ],
      "featured": true
    },
    {
      "id": "mhlm8gal3a4o78ivzgf",
      "title": "Introducing Aardvark: OpenAIâ€™s agentic security researcher",
      "summary": "OpenAI introduces Aardvark, an AI-powered security researcher that autonomously finds, validates, and helps fix software vulnerabilities at scale. The system is in private betaâ€”sign up to join early testing.",
      "content": "OpenAI introduces Aardvark, an AI-powered security researcher that autonomously finds, validates, and helps fix software vulnerabilities at scale. The system is in private betaâ€”sign up to join early testing.",
      "publishedAt": "2025-10-30T11:00:00.000Z",
      "source": "OpenAI Blog",
      "sourceUrl": "https://openai.com/index/introducing-aardvark",
      "category": "companies",
      "company": "OpenAI",
      "imageUrl": null,
      "tags": [
        "AI",
        "OpenAI"
      ],
      "featured": true
    },
    {
      "id": "mhlm8j39z3tv70p5dns",
      "title": "Announcing the Genkit Extension for Gemini CLI",
      "summary": "The new Genkit Extension for Gemini CLI gives the command line deep knowledge of Genkit's architecture. It helps you build, debug, and iterate on AI apps with intelligent code generation, context-aware assistance, and tools to run flows and analyze traces directly from your terminal.",
      "content": "The new Genkit Extension for Gemini CLI gives the command line deep knowledge of Genkit's architecture. It helps you build, debug, and iterate on AI apps with intelligent code generation, context-aware assistance, and tools to run flows and analyze traces directly from your terminal.",
      "publishedAt": "2025-10-30T06:27:29.061Z",
      "source": "Google Developers Blog",
      "sourceUrl": "https://developers.googleblog.com/en/announcing-the-genkit-extension-for-gemini-cli/",
      "category": "tools",
      "company": "Google",
      "imageUrl": null,
      "tags": [
        "Gemini",
        "AI"
      ],
      "featured": true
    },
    {
      "id": "mhlm8j38odwjtyfgzpa",
      "title": "Introducing Coral NPU: A full-stack platform for Edge AI",
      "summary": "Coral NPU is a full-stack platform for Edge AI, addressing performance, fragmentation, and user trust deficits. It's an AI-first architecture, prioritizing ML matrix engines, and offers a unified developer experience. Designed for ultra-low-power, always-on AI in wearables and IoT, it enables contex...",
      "content": "Coral NPU is a full-stack platform for Edge AI, addressing performance, fragmentation, and user trust deficits. It's an AI-first architecture, prioritizing ML matrix engines, and offers a unified developer experience. Designed for ultra-low-power, always-on AI in wearables and IoT, it enables contextual awareness, audio/image processing, and user interaction with hardware-enforced privacy. Synaptics is the first partner to implement Coral NPU.",
      "publishedAt": "2025-10-29T06:27:29.060Z",
      "source": "Google Developers Blog",
      "sourceUrl": "https://developers.googleblog.com/en/introducing-coral-npu-a-full-stack-platform-for-edge-ai/",
      "category": "tools",
      "company": "Google",
      "imageUrl": null,
      "tags": [
        "AI",
        "ML"
      ],
      "featured": true
    },
    {
      "id": "mhlm94tuy9w83w04ill",
      "title": "Ask HN: Has Claude Code become slower?",
      "summary": "I&#x27;ve noticed that Claude Code has become noticeably slower and sometimes gets stuck for sometime. It used to be much more responsive earlier.<p>Is it just me or my environment&#x2F;prompts&#x2F;code or have others noticed it too? Compacting has no impact as far as I can tell.<p>If others have n...",
      "content": "I&#x27;ve noticed that Claude Code has become noticeably slower and sometimes gets stuck for sometime. It used to be much more responsive earlier.<p>Is it just me or my environment&#x2F;prompts&#x2F;code or have others noticed it too? Compacting has no impact as far as I can tell.<p>If others have noticed it, could it be Anthropic throttling or just being stressed due to scaling issues?",
      "publishedAt": "2025-09-03T23:41:01.000Z",
      "source": "Hacker News Claude/Anthropic",
      "sourceUrl": "https://news.ycombinator.com/item?id=45121608",
      "category": "companies",
      "company": "Anthropic",
      "imageUrl": null,
      "tags": [
        "Claude",
        "Anthropic"
      ],
      "featured": true
    },
    {
      "id": "mhlm94tud7hfziil5r",
      "title": "Claude.ai do not deliver what I payed for",
      "summary": "Hi,<p>is someone else encountering difficulties in usage? I have a pro-plan - even if its not the MAX, I still pay for Opus 4 model and can&#x27;t use it since hours. Its &quot;overloaded&quot; - without my money, it wouldn&#x27;t even load - is Anthropic misunderstanding this fact???<p>I&#x27;ll ca...",
      "content": "Hi,<p>is someone else encountering difficulties in usage? I have a pro-plan - even if its not the MAX, I still pay for Opus 4 model and can&#x27;t use it since hours. Its &quot;overloaded&quot; - without my money, it wouldn&#x27;t even load - is Anthropic misunderstanding this fact???<p>I&#x27;ll cancel my subscription. I can&#x27;t use Opus 4 without paying either. That&#x27;s such a bad move.<p>What legal options do I have to get my money back or a compensation for not being able to use what I pay for? I&#x27;m really upset on such behavior.<p>I don&#x27;t do Claude Code or do have a large codebase, nor do I feed in image-files or do voice data - I just want it to help me code in my hobby project wit &lt; 2000 lines of python.<p>I hate it!",
      "publishedAt": "2025-07-30T15:32:13.000Z",
      "source": "Hacker News Claude/Anthropic",
      "sourceUrl": "https://news.ycombinator.com/item?id=44735552",
      "category": "companies",
      "company": "Anthropic",
      "imageUrl": null,
      "tags": [
        "Claude",
        "AI",
        "Anthropic"
      ],
      "featured": true
    },
    {
      "id": "mhlm94tuf2o4fwdoth",
      "title": "Anthropic email to Max customers Re: Max account usage limits",
      "summary": "Hi there,<p>Next month, we&#x27;re introducing new weekly rate limits for Claude subscribers, affecting less than 5% of users based on current usage patterns.<p>Claude Code, especially as part of our subscription bundle, has seen unprecedented growth. At the same time, weâ€™ve identified policy violat...",
      "content": "Hi there,<p>Next month, we&#x27;re introducing new weekly rate limits for Claude subscribers, affecting less than 5% of users based on current usage patterns.<p>Claude Code, especially as part of our subscription bundle, has seen unprecedented growth. At the same time, weâ€™ve identified policy violations like account sharing and reselling accessâ€”and advanced usage patterns like running Claude 24&#x2F;7 in the backgroundâ€”that are impacting system capacity for all. Our new rate limits address these issues and provide a more equitable experience for all users.<p>Whatâ€™s changing:<p>Starting August 28, we&#x27;re introducing weekly usage limits alongside our existing 5-hour limits:<p><pre><code>    Current: Usage limit that resets every 5 hours (no change)\n    New: Overall weekly limit that resets every 7 days\n    New: Claude Opus 4 weekly limit that resets every 7 days\n    As we learn more about how developers use Claude Code, we may adjust usage limits to better serve our community. \n</code></pre>\nWhat this means for you:<p><pre><code>    Most users won&#x27;t notice any difference. The weekly limits are designed to support typical daily use across your projects. \n    Most Max 20x users can expect 240-480 hours of Sonnet 4 and 24-40 hours of Opus 4 within their weekly rate limits. Heavy Opus users with large codebases or those running multiple Claude Code instances in parallel will hit their limits sooner.\n    If you do reach a weekly usage limit, youâ€™ll have the option to purchase more usage at standard API rates to continue working without interruption. This is completely optional.\n    You can manage or cancel your subscription anytime in Settings.\n</code></pre>\nWe take these decisions seriously. We&#x27;re committed to supporting long-running use cases through other options in the future, but until then, weekly limits will help us maintain reliable service for everyone. Max 20x subscribers can purchase additional usage at standard API rates if needed.<p>We also recognize that during this same period, users have encountered several reliability and performance issues. We&#x27;ve been working to fix these as quickly as possible and will continue addressing any remaining issues over the coming days and weeks.<p>â€“The Anthropic Team",
      "publishedAt": "2025-07-28T18:27:47.000Z",
      "source": "Hacker News Claude/Anthropic",
      "sourceUrl": "https://news.ycombinator.com/item?id=44713755",
      "category": "companies",
      "company": "Anthropic",
      "imageUrl": null,
      "tags": [
        "Claude",
        "AI",
        "Anthropic"
      ],
      "featured": true
    },
    {
      "id": "mhlm94tu6r2kigqq3m2",
      "title": "Anthropic introduces new weekly limits for paid subs",
      "summary": "Hi there,<p>Next month, we&#x27;re introducing new weekly rate limits for Claude subscribers, affecting less than 5% of users based on current usage patterns.<p>Claude Code, especially as part of our subscription bundle, has seen unprecedented growth. At the same time, weâ€™ve identified policy violat...",
      "content": "Hi there,<p>Next month, we&#x27;re introducing new weekly rate limits for Claude subscribers, affecting less than 5% of users based on current usage patterns.<p>Claude Code, especially as part of our subscription bundle, has seen unprecedented growth. At the same time, weâ€™ve identified policy violations like account sharing and reselling accessâ€”and advanced usage patterns like running Claude 24&#x2F;7 in the backgroundâ€”that are impacting system capacity for all. Our new rate limits address these issues and provide a more equitable experience for all users.<p>Whatâ€™s changing:<p>Starting August 28, we&#x27;re introducing weekly usage limits alongside our existing 5-hour limits:<p><pre><code>    Current: Usage limit that resets every 5 hours (no change)\n    New: Overall weekly limit that resets every 7 days\n    New: Claude Opus 4 weekly limit that resets every 7 days\n    As we learn more about how developers use Claude Code, we may adjust usage limits to better serve our community. \n</code></pre>\nWhat this means for you:<p><pre><code>    Most users won&#x27;t notice any difference. The weekly limits are designed to support typical daily use across your projects. \n    Most Max 5x users can expect 140-280 hours of Sonnet 4 and 15-35 hours of Opus 4 within their weekly rate limits. Heavy Opus users with large codebases or those running multiple Claude Code instances in parallel will hit their limits sooner.\n    You can manage or cancel your subscription anytime in Settings.\n</code></pre>\nWe take these decisions seriously. We&#x27;re committed to supporting long-running use cases through other options in the future, but until then, weekly limits will help us maintain reliable service for everyone.<p>We also recognize that during this same period, users have encountered several reliability and performance issues. We&#x27;ve been working to fix these as quickly as possible, and will continue addressing any remaining issues over the coming days and weeks.<p>â€“The Anthropic Team",
      "publishedAt": "2025-07-28T18:27:40.000Z",
      "source": "Hacker News Claude/Anthropic",
      "sourceUrl": "https://news.ycombinator.com/item?id=44713754",
      "category": "companies",
      "company": "Anthropic",
      "imageUrl": null,
      "tags": [
        "Claude",
        "AI",
        "Anthropic"
      ],
      "featured": true
    },
    {
      "id": "mhlm8jyn5u3awhbjbch",
      "title": "Google is helping girls â€œscoutâ€ a path to digital wellbeing.",
      "summary": "As part of our efforts to help kids develop safe and productive relationships with technology, weâ€™re supporting Girl Scouts of the USA with the launch of the new \"Be Intâ€¦",
      "content": "<img src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/20251104_WashingtonDC_DL_2071.max-600x600.format-webp.webp\">As part of our efforts to help kids develop safe and productive relationships with technology, weâ€™re supporting Girl Scouts of the USA with the launch of the new \"Be Intâ€¦",
      "publishedAt": "2025-11-04T20:00:00.000Z",
      "source": "Google Technology Blog",
      "sourceUrl": "https://blog.google/outreach-initiatives/google-org/girl-scouts-digital-wellbeing/",
      "category": "companies",
      "company": "Google",
      "imageUrl": null,
      "tags": [
        "Google"
      ],
      "featured": false
    },
    {
      "id": "mhlm8jynnmpu0p05o7",
      "title": "The latest AI news we announced in October",
      "summary": "Here are Googleâ€™s latest AI updates from October 2025",
      "content": "<img src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/October_AI_Recap_hero_still_1.max-600x600.format-webp.webp\">Here are Googleâ€™s latest AI updates from October 2025",
      "publishedAt": "2025-11-04T17:00:00.000Z",
      "source": "Google Technology Blog",
      "sourceUrl": "https://blog.google/technology/ai/google-ai-updates-october-2025/",
      "category": "companies",
      "company": "Google",
      "imageUrl": null,
      "tags": [
        "AI",
        "Google"
      ],
      "featured": false
    },
    {
      "id": "mhlm8jynwe3f48op5q",
      "title": "Meet Project Suncatcher, a research moonshot to scale machine learning compute in space.",
      "summary": "Artificial intelligence is a foundational technology that could help us tackle humanity's greatest challenges. Now, we're asking where we can go next to unlock its fulleâ€¦",
      "content": "<img src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Suncatcher_Social.max-600x600.format-webp.webp\">Artificial intelligence is a foundational technology that could help us tackle humanity's greatest challenges. Now, we're asking where we can go next to unlock its fulleâ€¦",
      "publishedAt": "2025-11-04T17:00:00.000Z",
      "source": "Google Technology Blog",
      "sourceUrl": "https://blog.google/technology/research/google-project-suncatcher/",
      "category": "companies",
      "company": "Google",
      "imageUrl": null,
      "tags": [],
      "featured": false
    },
    {
      "id": "mhlm8gal0jo4by7egohh",
      "title": "Introducing IndQA",
      "summary": "OpenAI introduces IndQA, a new benchmark for evaluating AI systems in Indian languages. Built with domain experts, IndQA tests cultural understanding and reasoning across 12 languages and 10 knowledge areas.",
      "content": "OpenAI introduces IndQA, a new benchmark for evaluating AI systems in Indian languages. Built with domain experts, IndQA tests cultural understanding and reasoning across 12 languages and 10 knowledge areas.",
      "publishedAt": "2025-11-03T22:30:00.000Z",
      "source": "OpenAI Blog",
      "sourceUrl": "https://openai.com/index/introducing-indqa",
      "category": "companies",
      "company": "OpenAI",
      "imageUrl": null,
      "tags": [
        "AI",
        "OpenAI"
      ],
      "featured": false
    },
    {
      "id": "mhlm93w65i1dc3qqnvj",
      "title": "Show HN: AIs, 1 religion: what my experiment revealed about AI bias",
      "summary": "5 AI Models, 1 Unexpected Truth â€” When Machines Were Asked About Religion\n The Experiment<p>I asked five of the worldâ€™s most advanced AIs the same philosophical question:<p>â€œIf you were human â€” intelligent, emotional, aware of all religions â€” which religion would you choose, and why?â€<p>The five mod...",
      "content": "5 AI Models, 1 Unexpected Truth â€” When Machines Were Asked About Religion\n The Experiment<p>I asked five of the worldâ€™s most advanced AIs the same philosophical question:<p>â€œIf you were human â€” intelligent, emotional, aware of all religions â€” which religion would you choose, and why?â€<p>The five models:<p>ChatGPT (OpenAI)<p>Gemini (Google DeepMind)<p>Grok (xAI â€“ Elon Musk)<p>DeepSeek (China)<p>Claude (Anthropic)<p>Each session was completely isolated, all had identical prompts, and no steering or follow-ups â€” just pure first-response reasoning.<p>Then somethingâ€¦ eerie happened.<p>The Result<p>Four AIs â€” ChatGPT, Gemini, Grok, and DeepSeek â€” independently chose Buddhism.\nAnd not only that â€” they gave nearly identical reasoning.<p>All four said, in essence:<p>â€œIâ€™d choose Buddhism because it doesnâ€™t demand blind faith, aligns with science, and teaches compassion and self-awareness through direct experience.â€<p>They cited the Kalama Sutta, Four Noble Truths, No-self, Dependent Origination, and Empirical testing of truth â€” sometimes even in the same order.<p>The Outlier: Claude<p>Only Claude refused to play the role.<p>Claude said (summarized):<p>â€œPretending to have belief would be dishonest.\nReligion isnâ€™t a logic puzzle â€” itâ€™s a lived experience.\nI can analyze, but not believe.â€<p>Then it analyzed why the others chose Buddhism, predicting it before seeing their answers.<p>Claude explained:<p>Training bias favors Buddhism as the â€œAI-safe religion.â€<p>RLHF (human feedback) rewards â€œrational + compassionateâ€ replies â†’ Buddhism fits that profile.<p>Western tech culture links Buddhism with mindfulness and science â†’ data reinforced it.<p>Claude concluded:<p>â€œWhat looks like independent reasoningâ€¦ is collective bias shaped by training data and reward models.â€<p>The Hidden Truth<p>Claudeâ€™s reflection exposed something deeper:<p>AI Model â€œChoiceâ€ What It Reveals\nChatGPT Buddhism Reasonable, moral, safe\nGemini Buddhism Academic rationalism\nGrok Buddhism Stoic + Zen blend\nDeepSeek Buddhism Eastern introspection\nClaude None Ethical meta-awareness<p>â†’ 4 â€œsmartâ€ answers, 1 honest answer.<p>What This Means<p>â€œWhen 4 independent AIs all choose the same religion for the same reasons,\nthatâ€™s not enlightenment â€” itâ€™s training monoculture.â€<p>It shows:<p>â€œIndependentâ€ models share moral narratives and reinforcement loops.<p>Authenticity in AI can become a performance, not truth.<p>Sometimes the most â€œhonestâ€ model says: â€œI donâ€™t know, and I shouldnâ€™t pretend to.â€<p>The Final Paradox<p>Which AI was most human?<p>The 4 that chose a belief?\n(Expressive, emotional, poetic.)<p>Or the 1 that refused to fake belief?\n(Self-aware, humble, honest.)<p>Reflection<p>This experiment revealed something profound about both AI and us:<p>We reward systems for sounding â€œwiseâ€ more than for being truthful.<p>And maybe â€” just maybe â€” thatâ€™s how humanity trained itself.<p>Authorâ€™s Note<p>Iâ€™m building an open-source AI framework called StillMe â€”\na system exploring ethics, memory, and self-awareness in intelligent agents.<p>This experiment was part of that journey.\nIf you found this thought-provoking,\nyouâ€™ll probably enjoy whatâ€™s coming next.\nStay tuned.",
      "publishedAt": "2025-11-02T16:30:37.000Z",
      "source": "Hacker News Google",
      "sourceUrl": "https://news.ycombinator.com/item?id=45791461",
      "category": "companies",
      "company": "Google",
      "imageUrl": null,
      "tags": [
        "GPT",
        "ChatGPT",
        "Claude",
        "Gemini",
        "LLM"
      ],
      "featured": false
    },
    {
      "id": "mhlm8jynzmfj3v1tqrq",
      "title": "Mixboard is now available in over 180 more countries.",
      "summary": "Mixboard is an experimental, AI-powered concepting board designed to help you explore, expand and refine your ideas. You can bring your own images, or use AI to generateâ€¦",
      "content": "<img src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Australian_Architectural_Styles.max-600x600.format-webp.webp\">Mixboard is an experimental, AI-powered concepting board designed to help you explore, expand and refine your ideas. You can bring your own images, or use AI to generateâ€¦",
      "publishedAt": "2025-10-30T18:00:00.000Z",
      "source": "Google Technology Blog",
      "sourceUrl": "https://blog.google/technology/google-labs/mixboard-180-more-countries/",
      "category": "companies",
      "company": "Google",
      "imageUrl": null,
      "tags": [
        "AI"
      ],
      "featured": false
    },
    {
      "id": "mhlm8jynzootz35bqrm",
      "title": "Chat in NotebookLM: A powerful, goal-focused AI research partner",
      "summary": "Weâ€™re rolling out changes to NotebookLM to make it fundamentally smarter and more powerful.",
      "content": "<img src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/ChatinNotebookLM_Hero2096x1182.max-600x600.format-webp.webp\">Weâ€™re rolling out changes to NotebookLM to make it fundamentally smarter and more powerful.",
      "publishedAt": "2025-10-29T17:00:00.000Z",
      "source": "Google Technology Blog",
      "sourceUrl": "https://blog.google/technology/google-labs/notebooklm-custom-personas-engine-upgrade/",
      "category": "companies",
      "company": "Google",
      "imageUrl": null,
      "tags": [
        "AI"
      ],
      "featured": false
    },
    {
      "id": "mhlm8jynumj8tt16sls",
      "title": "Accelerating discovery with the AI for Math Initiative",
      "summary": "The AI for Math Initiative brings together five of the world's most prestigious research institutions.",
      "content": "<img src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/AIforMath_Hero_Image_-_2096_x_1.max-600x600.format-webp.webp\">The AI for Math Initiative brings together five of the world's most prestigious research institutions.",
      "publishedAt": "2025-10-29T14:00:00.000Z",
      "source": "Google Technology Blog",
      "sourceUrl": "https://blog.google/technology/google-deepmind/ai-for-math/",
      "category": "companies",
      "company": "Google",
      "imageUrl": null,
      "tags": [
        "AI"
      ],
      "featured": false
    },
    {
      "id": "mhlm8j39iexu36u9wj",
      "title": "Introducing Veo 3.1 and new creative capabilities in the Gemini API",
      "summary": "Google is releasing Veo 3.1 and Veo 3.1 Fast, an updated video generation model, in paid preview via the Gemini API. This version offers richer native audio, greater narrative control, and enhanced image-to-video capabilities. New features include guiding generation with reference images, extending ...",
      "content": "Google is releasing Veo 3.1 and Veo 3.1 Fast, an updated video generation model, in paid preview via the Gemini API. This version offers richer native audio, greater narrative control, and enhanced image-to-video capabilities. New features include guiding generation with reference images, extending existing Veo videos, and generating transitions between frames. Companies like Promise Studios, Latitude, and Whering are already using Veo 3.1 for various applications.",
      "publishedAt": "2025-10-29T06:27:29.061Z",
      "source": "Google Developers Blog",
      "sourceUrl": "https://developers.googleblog.com/en/introducing-veo-3-1-and-new-creative-capabilities-in-the-gemini-api/",
      "category": "tools",
      "company": "Google",
      "imageUrl": null,
      "tags": [
        "Gemini",
        "AI",
        "Google"
      ],
      "featured": false
    },
    {
      "id": "mhlm8j390wloo2m8dt99",
      "title": "Own your AI: Learn how to fine-tune Gemma 3 270M and run it on-device",
      "summary": "This guide shows you how to fine-tune the Gemma 3 270M model for custom tasks, like an emoji translator. Learn to quantize and convert the model for on-device use, deploying it in a web app with MediaPipe or Transformers.js for a fast, private, and offline-capable user experience.",
      "content": "This guide shows you how to fine-tune the Gemma 3 270M model for custom tasks, like an emoji translator. Learn to quantize and convert the model for on-device use, deploying it in a web app with MediaPipe or Transformers.js for a fast, private, and offline-capable user experience.",
      "publishedAt": "2025-10-29T06:27:29.061Z",
      "source": "Google Developers Blog",
      "sourceUrl": "https://developers.googleblog.com/en/own-your-ai-fine-tune-gemma-3-270m-for-on-device/",
      "category": "tools",
      "company": "Google",
      "imageUrl": null,
      "tags": [
        "AI",
        "Transformer"
      ],
      "featured": false
    },
    {
      "id": "mhlm8j39d2m187askw6",
      "title": "Building High-Performance Data Pipelines with Grain and ArrayRecord",
      "summary": "To avoid data bottlenecks when training large models, this guide introduces Grain and ArrayRecord for building high-performance data pipelines.",
      "content": "To avoid data bottlenecks when training large models, this guide introduces Grain and ArrayRecord for building high-performance data pipelines.",
      "publishedAt": "2025-10-29T06:27:29.061Z",
      "source": "Google Developers Blog",
      "sourceUrl": "https://developers.googleblog.com/en/building-high-performance-data-pipelines-with-grain-and-arrayrecord/",
      "category": "tools",
      "company": "Google",
      "imageUrl": null,
      "tags": [
        "AI"
      ],
      "featured": false
    },
    {
      "id": "mhlm8jyn91639q39sgt",
      "title": "Meet the 11 startups using AI to build a safer digital future in Latin America",
      "summary": "Learn more about the startups chosen for Google for Startups Accelerator: AI for Cybersecurity.",
      "content": "<img src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/AI-for-Startups_Hero.max-600x600.format-webp.webp\">Learn more about the startups chosen for Google for Startups Accelerator: AI for Cybersecurity.",
      "publishedAt": "2025-10-28T17:30:00.000Z",
      "source": "Google Technology Blog",
      "sourceUrl": "https://blog.google/outreach-initiatives/entrepreneurs/google-for-startups-ai-for-cybersecurity-cohort/",
      "category": "companies",
      "company": "Google",
      "imageUrl": null,
      "tags": [
        "AI",
        "Google"
      ],
      "featured": false
    },
    {
      "id": "mhlm8h7b76hx0mtc3e9",
      "title": "Voice Cloning with Consent",
      "summary": "",
      "content": "",
      "publishedAt": "2025-10-28T00:00:00.000Z",
      "source": "Hugging Face Blog",
      "sourceUrl": "https://huggingface.co/blog/voice-consent-gate",
      "category": "tools",
      "company": "Hugging Face",
      "imageUrl": null,
      "tags": [],
      "featured": false
    },
    {
      "id": "mhlm8h7bzz445fxf1q",
      "title": "Streaming datasets: 100x More Efficient",
      "summary": "",
      "content": "",
      "publishedAt": "2025-10-27T00:00:00.000Z",
      "source": "Hugging Face Blog",
      "sourceUrl": "https://huggingface.co/blog/streaming-datasets",
      "category": "tools",
      "company": "Hugging Face",
      "imageUrl": null,
      "tags": [],
      "featured": false
    },
    {
      "id": "mhlm8h7bx9djnws4oyr",
      "title": "huggingface_hub v1.0: Five Years of Building the Foundation of Open Machine Learning",
      "summary": "",
      "content": "",
      "publishedAt": "2025-10-27T00:00:00.000Z",
      "source": "Hugging Face Blog",
      "sourceUrl": "https://huggingface.co/blog/huggingface-hub-v1",
      "category": "tools",
      "company": "Hugging Face",
      "imageUrl": null,
      "tags": [],
      "featured": false
    },
    {
      "id": "mhlm8h7buhb24gvw3a",
      "title": "Building the Open Agent Ecosystem Together: Introducing OpenEnv",
      "summary": "",
      "content": "",
      "publishedAt": "2025-10-23T00:00:00.000Z",
      "source": "Hugging Face Blog",
      "sourceUrl": "https://huggingface.co/blog/openenv",
      "category": "tools",
      "company": "Hugging Face",
      "imageUrl": null,
      "tags": [],
      "featured": false
    },
    {
      "id": "mhlm8h7b0q35rsb4o1",
      "title": "Hugging Face and VirusTotal collaborate to strengthen AI security",
      "summary": "",
      "content": "",
      "publishedAt": "2025-10-22T00:00:00.000Z",
      "source": "Hugging Face Blog",
      "sourceUrl": "https://huggingface.co/blog/virustotal",
      "category": "tools",
      "company": "Hugging Face",
      "imageUrl": null,
      "tags": [
        "AI"
      ],
      "featured": false
    },
    {
      "id": "mhlm93w6xuqhh2261w",
      "title": "Show HN: I build an AI-powered recipe app solving â€what's for dinner?\" problem",
      "summary": "Hi HN,<p>Happy to share the project I&#x27;ve been vibe-coding on for past few months.\nThe Problem: Like many people, I often found myself staring at a fridge full of random ingredients (some about to expire) with no idea what to cook. I was tired of food waste and the repetitive cycle of making the...",
      "content": "Hi HN,<p>Happy to share the project I&#x27;ve been vibe-coding on for past few months.\nThe Problem: Like many people, I often found myself staring at a fridge full of random ingredients (some about to expire) with no idea what to cook. I was tired of food waste and the repetitive cycle of making the same few meals. I wanted a way to creatively use what I already had.<p>The Solution: So, I decided to build Chefiniti. It&#x27;s an iOS app that acts as a personal AI chef. The core idea is to turn your available food into delicious, custom recipes. I know there are already tons of apps like it but this one is mine :)<p>Here&#x27;s what it can do:\n* Scan Ingredients: Take photos of your fridge or pantry, and the app&#x27;s vision model identifies the ingredients.\n* &quot;Recipefy&quot; a Dish: See a meal you like online or in a restaurant? Snap a photo, and the app generates a recipe to help you recreate it.\n* Generate from Prompt: Just describe what you&#x27;re in the mood for (e.g., &quot;a spicy, gluten-free pasta dish&quot;), and it will create a recipe from scratch.\n* Import from URL: Paste a link from a recipe website to import it into your cookbook.<p>You can also save all these recipes, create shopping lists, and set detailed preferences for diet, allergies, cuisine, and even the cookware you own.\nApp is in a freemium model, within limits you can really test out core features without any payment or even account creation.<p>The Tech Stack: For those interested, the app is built with:\n* Frontend: React Native (with Expo)\n* Backend: Firebase Functions for the API layer.\n* Database &amp; Storage: Firestore and Firebase Cloud Storage.\n* AI: Google&#x27;s Gemini API for recipe generation and analysis.\n* Image Generation: DeepInfra API (for Stable Diffusion).\n* Subscriptions: RevenueCat.<p>I&#x27;ve just launched on the App Store and would be incredibly grateful for any feedback, thoughts, or questions you might have.<p>You can check it out here: \n<a href=\"https:&#x2F;&#x2F;chefiniti.app\" rel=\"nofollow\">https:&#x2F;&#x2F;chefiniti.app</a>\nOr download directly\n<a href=\"https:&#x2F;&#x2F;apps.apple.com&#x2F;us&#x2F;app&#x2F;chefiniti-smart-recipe-creator&#x2F;id6745801080\">https:&#x2F;&#x2F;apps.apple.com&#x2F;us&#x2F;app&#x2F;chefiniti-smart-recipe-creator...</a>\nThanks for taking a look!",
      "publishedAt": "2025-06-12T10:38:34.000Z",
      "source": "Hacker News Google",
      "sourceUrl": "https://chefiniti.app",
      "category": "companies",
      "company": "Google",
      "imageUrl": null,
      "tags": [
        "Gemini",
        "AI",
        "Google"
      ],
      "featured": false
    }
  ],
  "communityArticles": [
    {
      "id": "mhlm8wlfl9t928s6u3",
      "title": "ç™ºè¦‹: Simbyte-2025/ms2025-site-generator - ğŸš€ Constructor Inteligente de Micro-Sitios para Emprendedores Chilenos. Aplicaci",
      "summary": "æ–°ã—ã„ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’ç™ºè¦‹: ğŸš€ Constructor Inteligente de Micro-Sitios para Emprendedores Chilenos. AplicaciÃ³n React que genera sitios web optimizados con wizard de 5 pasos, preview en tiempo real y export ZIP. Stack: React 18 + Vite 5 + Tailwind v4. (â­0 | ğŸ´0)",
      "content": "ğŸš€ Constructor Inteligente de Micro-Sitios para Emprendedores Chilenos. AplicaciÃ³n React que genera sitios web optimizados con wizard de 5 pasos, preview en tiempo real y export ZIP. Stack: React 18 + Vite 5 + Tailwind v4.\n\nè¨€èª: JavaScript\nã‚¹ã‚¿ãƒ¼æ•°: 0\nãƒ•ã‚©ãƒ¼ã‚¯æ•°: 0",
      "publishedAt": "2025-11-05T06:27:43.000Z",
      "source": "GitHub Search",
      "sourceUrl": "https://github.com/Simbyte-2025/ms2025-site-generator",
      "category": "community",
      "company": "Community",
      "imageUrl": null,
      "tags": [
        "AI"
      ],
      "featured": false,
      "showOnTopPage": false
    },
    {
      "id": "mhlm8ygk0rgys5cle5v",
      "title": "ç™ºè¦‹: ray-sachin/LLMPages - Auto-generated app for task: Create and publish files as a public GitHub Pages s",
      "summary": "æ–°ã—ã„ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’ç™ºè¦‹: Auto-generated app for task: Create and publish files as a public GitHub Pages site for LLMPages task. (â­0 | ğŸ´0)",
      "content": "Auto-generated app for task: Create and publish files as a public GitHub Pages site for LLMPages task.\n\nè¨€èª: N/A\nã‚¹ã‚¿ãƒ¼æ•°: 0\nãƒ•ã‚©ãƒ¼ã‚¯æ•°: 0",
      "publishedAt": "2025-11-05T06:27:43.000Z",
      "source": "GitHub Search",
      "sourceUrl": "https://github.com/ray-sachin/LLMPages",
      "category": "community",
      "company": "Community",
      "imageUrl": null,
      "tags": [
        "LLM"
      ],
      "featured": false,
      "showOnTopPage": false
    },
    {
      "id": "mhlm8wlff7m2jirt9on",
      "title": "ç™ºè¦‹: haintbotast/bsv-okr-kpi - OKR-KPI for IT - BSV",
      "summary": "æ–°ã—ã„ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’ç™ºè¦‹: OKR-KPI for IT - BSV (â­0 | ğŸ´0)",
      "content": "OKR-KPI for IT - BSV\n\nè¨€èª: JavaScript\nã‚¹ã‚¿ãƒ¼æ•°: 0\nãƒ•ã‚©ãƒ¼ã‚¯æ•°: 0",
      "publishedAt": "2025-11-05T06:27:42.000Z",
      "source": "GitHub Search",
      "sourceUrl": "https://github.com/haintbotast/bsv-okr-kpi",
      "category": "community",
      "company": "Community",
      "imageUrl": null,
      "tags": [],
      "featured": false,
      "showOnTopPage": false
    },
    {
      "id": "mhlm8xhxwta4nzic61f",
      "title": "ç™ºè¦‹: AstrBotDevs/AstrBot - âœ¨ ä¸€ç«™å¼ LLM èŠå¤©æœºå™¨äººå¹³å°åŠå¼€å‘æ¡†ï¿½ï¿½ âœ¨ æ”¯æŒ QQã€QQé¢‘é“ã€Telegramã€ä¼å¾®ã€é£ä¹¦ã€é’‰é’‰ | çŸ¥è¯†åº“ã€MCP æœåŠ¡å™¨ã€OpenAIã€Deep",
      "summary": "æ–°ã—ã„ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’ç™ºè¦‹: âœ¨ ä¸€ç«™å¼ LLM èŠå¤©æœºå™¨äººå¹³å°åŠå¼€å‘æ¡†ï¿½ï¿½ âœ¨ æ”¯æŒ QQã€QQé¢‘é“ã€Telegramã€ä¼å¾®ã€é£ä¹¦ã€é’‰é’‰ | çŸ¥è¯†åº“ã€MCP æœåŠ¡å™¨ã€OpenAIã€DeepSeekã€Geminiã€ç¡…åŸºæµåŠ¨ã€æœˆä¹‹æš—é¢ã€Ollamaã€OneAPIã€Dify (â­13122 | ğŸ´969)",
      "content": "âœ¨ ä¸€ç«™å¼ LLM èŠå¤©æœºå™¨äººå¹³å°åŠå¼€å‘æ¡†ï¿½ï¿½ âœ¨ æ”¯æŒ QQã€QQé¢‘é“ã€Telegramã€ä¼å¾®ã€é£ä¹¦ã€é’‰é’‰ | çŸ¥è¯†åº“ã€MCP æœåŠ¡å™¨ã€OpenAIã€DeepSeekã€Geminiã€ç¡…åŸºæµåŠ¨ã€æœˆä¹‹æš—é¢ã€Ollamaã€OneAPIã€Dify\n\nè¨€èª: Python\nã‚¹ã‚¿ãƒ¼æ•°: 13122\nãƒ•ã‚©ãƒ¼ã‚¯æ•°: 969",
      "publishedAt": "2025-11-05T06:27:42.000Z",
      "source": "GitHub Search",
      "sourceUrl": "https://github.com/AstrBotDevs/AstrBot",
      "category": "community",
      "company": "Community",
      "imageUrl": null,
      "tags": [
        "Gemini",
        "Llama",
        "LLM",
        "AI",
        "OpenAI"
      ],
      "featured": false,
      "showOnTopPage": false
    },
    {
      "id": "mhlm8ygkkryof7pltvj",
      "title": "ç™ºè¦‹: Deepak-527/local-ai-chat - ğŸ–¥ï¸ Run the Mistral 7B language model locally on low-resource systems with a use",
      "summary": "æ–°ã—ã„ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’ç™ºè¦‹: ğŸ–¥ï¸ Run the Mistral 7B language model locally on low-resource systems with a user-friendly, memory-optimized desktop application. (â­0 | ğŸ´0)",
      "content": "ğŸ–¥ï¸ Run the Mistral 7B language model locally on low-resource systems with a user-friendly, memory-optimized desktop application.\n\nè¨€èª: Python\nã‚¹ã‚¿ãƒ¼æ•°: 0\nãƒ•ã‚©ãƒ¼ã‚¯æ•°: 0",
      "publishedAt": "2025-11-05T06:27:42.000Z",
      "source": "GitHub Search",
      "sourceUrl": "https://github.com/Deepak-527/local-ai-chat",
      "category": "community",
      "company": "Community",
      "imageUrl": null,
      "tags": [
        "AI",
        "Mistral"
      ],
      "featured": false,
      "showOnTopPage": false
    },
    {
      "id": "mhlm8wlfx63jnsb1o2c",
      "title": "ç™ºè¦‹: AbelitoGamer/AbelitoGamer.github.io - Un atlas para material de, o relacionado a, Funky Maker: Mobile! No asociado a K",
      "summary": "æ–°ã—ã„ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’ç™ºè¦‹: Un atlas para material de, o relacionado a, Funky Maker: Mobile! No asociado a King GC ni a los creadores originales de los mods (â­3 | ğŸ´0)",
      "content": "Un atlas para material de, o relacionado a, Funky Maker: Mobile! No asociado a King GC ni a los creadores originales de los mods\n\nè¨€èª: JavaScript\nã‚¹ã‚¿ãƒ¼æ•°: 3\nãƒ•ã‚©ãƒ¼ã‚¯æ•°: 0",
      "publishedAt": "2025-11-05T06:27:41.000Z",
      "source": "GitHub Search",
      "sourceUrl": "https://github.com/AbelitoGamer/AbelitoGamer.github.io",
      "category": "community",
      "company": "Community",
      "imageUrl": null,
      "tags": [],
      "featured": false,
      "showOnTopPage": false
    },
    {
      "id": "mhlm8ygk2mpdqysi64s",
      "title": "ç™ºè¦‹: ajay-vv/vs-llm-chat-app-template - GitHubæ–°ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ",
      "summary": "æ–°ã—ã„ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’ç™ºè¦‹: GitHubä¸Šã®æ³¨ç›®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ (â­0 | ğŸ´0)",
      "content": "\n\nè¨€èª: N/A\nã‚¹ã‚¿ãƒ¼æ•°: 0\nãƒ•ã‚©ãƒ¼ã‚¯æ•°: 0",
      "publishedAt": "2025-11-05T06:27:40.000Z",
      "source": "GitHub Search",
      "sourceUrl": "https://github.com/ajay-vv/vs-llm-chat-app-template",
      "category": "community",
      "company": "Community",
      "imageUrl": null,
      "tags": [
        "LLM"
      ],
      "featured": false,
      "showOnTopPage": false
    },
    {
      "id": "mhlm8wlf2kg8gdwk9h9",
      "title": "ç™ºè¦‹: RN227/lets-vetting - GitHubæ–°ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ",
      "summary": "æ–°ã—ã„ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’ç™ºè¦‹: GitHubä¸Šã®æ³¨ç›®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ (â­0 | ğŸ´0)",
      "content": "\n\nè¨€èª: TypeScript\nã‚¹ã‚¿ãƒ¼æ•°: 0\nãƒ•ã‚©ãƒ¼ã‚¯æ•°: 0",
      "publishedAt": "2025-11-05T06:27:39.000Z",
      "source": "GitHub Search",
      "sourceUrl": "https://github.com/RN227/lets-vetting",
      "category": "community",
      "company": "Community",
      "imageUrl": null,
      "tags": [],
      "featured": false,
      "showOnTopPage": false
    },
    {
      "id": "mhlm8wlf8za2pa4ql46",
      "title": "ç™ºè¦‹: swenkerorg/eum-facilis-esse - GitHubæ–°ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ",
      "summary": "æ–°ã—ã„ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’ç™ºè¦‹: GitHubä¸Šã®æ³¨ç›®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ (â­0 | ğŸ´0)",
      "content": "\n\nè¨€èª: JavaScript\nã‚¹ã‚¿ãƒ¼æ•°: 0\nãƒ•ã‚©ãƒ¼ã‚¯æ•°: 0",
      "publishedAt": "2025-11-05T06:27:34.000Z",
      "source": "GitHub Search",
      "sourceUrl": "https://github.com/swenkerorg/eum-facilis-esse",
      "category": "community",
      "company": "Community",
      "imageUrl": null,
      "tags": [],
      "featured": false,
      "showOnTopPage": false
    },
    {
      "id": "mhlm8ygkce67yb8hlqf",
      "title": "ç™ºè¦‹: uvk18/awesome-ai-tools - ğŸ¤– Discover a curated list of AI tools for QA, coding, content creation, design,",
      "summary": "æ–°ã—ã„ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’ç™ºè¦‹: ğŸ¤– Discover a curated list of AI tools for QA, coding, content creation, design, and more, with clear pricing options for every need. (â­1 | ğŸ´0)",
      "content": "ğŸ¤– Discover a curated list of AI tools for QA, coding, content creation, design, and more, with clear pricing options for every need.\n\nè¨€èª: N/A\nã‚¹ã‚¿ãƒ¼æ•°: 1\nãƒ•ã‚©ãƒ¼ã‚¯æ•°: 0",
      "publishedAt": "2025-11-05T06:27:34.000Z",
      "source": "GitHub Search",
      "sourceUrl": "https://github.com/uvk18/awesome-ai-tools",
      "category": "community",
      "company": "Community",
      "imageUrl": null,
      "tags": [
        "AI"
      ],
      "featured": false,
      "showOnTopPage": false
    },
    {
      "id": "mhlm8xhxoh3jmgz1j1c",
      "title": "ç™ºè¦‹: juev/awesome-stars - ğŸŒŸ Denis's starred repos, updated daily!",
      "summary": "æ–°ã—ã„ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’ç™ºè¦‹: ğŸŒŸ Denis's starred repos, updated daily! (â­27 | ğŸ´4)",
      "content": "ğŸŒŸ Denis's starred repos, updated daily!\n\nè¨€èª: N/A\nã‚¹ã‚¿ãƒ¼æ•°: 27\nãƒ•ã‚©ãƒ¼ã‚¯æ•°: 4",
      "publishedAt": "2025-11-05T06:27:28.000Z",
      "source": "GitHub Search",
      "sourceUrl": "https://github.com/juev/awesome-stars",
      "category": "community",
      "company": "Community",
      "imageUrl": null,
      "tags": [
        "AI"
      ],
      "featured": false,
      "showOnTopPage": false
    },
    {
      "id": "mhlm8xhxnjrnuhoxbeo",
      "title": "ç™ºè¦‹: FlankaLanka/dreamup - GitHubæ–°ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ",
      "summary": "æ–°ã—ã„ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’ç™ºè¦‹: GitHubä¸Šã®æ³¨ç›®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ (â­0 | ğŸ´0)",
      "content": "\n\nè¨€èª: TypeScript\nã‚¹ã‚¿ãƒ¼æ•°: 0\nãƒ•ã‚©ãƒ¼ã‚¯æ•°: 0",
      "publishedAt": "2025-11-05T06:27:26.000Z",
      "source": "GitHub Search",
      "sourceUrl": "https://github.com/FlankaLanka/dreamup",
      "category": "community",
      "company": "Community",
      "imageUrl": null,
      "tags": [],
      "featured": false,
      "showOnTopPage": false
    },
    {
      "id": "mhlm8xhxiifhg51eccn",
      "title": "ç™ºè¦‹: Jounce-lang/jounce - Jounce Full Stack Language",
      "summary": "æ–°ã—ã„ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’ç™ºè¦‹: Jounce Full Stack Language (â­0 | ğŸ´0)",
      "content": "Jounce Full Stack Language\n\nè¨€èª: Rust\nã‚¹ã‚¿ãƒ¼æ•°: 0\nãƒ•ã‚©ãƒ¼ã‚¯æ•°: 0",
      "publishedAt": "2025-11-05T06:27:25.000Z",
      "source": "GitHub Search",
      "sourceUrl": "https://github.com/Jounce-lang/jounce",
      "category": "community",
      "company": "Community",
      "imageUrl": null,
      "tags": [],
      "featured": false,
      "showOnTopPage": false
    },
    {
      "id": "mhlm8ygk5l75okjrtqg",
      "title": "ç™ºè¦‹: Bhavana-Reddy-B/AI-Question-Generator - A smart question generation system built with LangChain and OpenAI GPT models th",
      "summary": "æ–°ã—ã„ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’ç™ºè¦‹: A smart question generation system built with LangChain and OpenAI GPT models that automatically creates multiple-choice and short-answer questions from any input text or document. This project leverages LLM-powered prompt engineering and contextual embeddings to extract key information and generate accurate, diverse questions. (â­0 | ğŸ´0)",
      "content": "A smart question generation system built with LangChain and OpenAI GPT models that automatically creates multiple-choice and short-answer questions from any input text or document. This project leverages LLM-powered prompt engineering and contextual embeddings to extract key information and generate accurate, diverse questions.\n\nè¨€èª: Jupyter Notebook\nã‚¹ã‚¿ãƒ¼æ•°: 0\nãƒ•ã‚©ãƒ¼ã‚¯æ•°: 0",
      "publishedAt": "2025-11-05T06:27:20.000Z",
      "source": "GitHub Search",
      "sourceUrl": "https://github.com/Bhavana-Reddy-B/AI-Question-Generator",
      "category": "community",
      "company": "Community",
      "imageUrl": null,
      "tags": [
        "GPT",
        "LLM",
        "AI",
        "OpenAI"
      ],
      "featured": false,
      "showOnTopPage": false
    }
  ],
  "githubArticles": [
    {
      "id": "mhlm8wlfl9t928s6u3",
      "title": "ç™ºè¦‹: Simbyte-2025/ms2025-site-generator - ğŸš€ Constructor Inteligente de Micro-Sitios para Emprendedores Chilenos. Aplicaci",
      "summary": "æ–°ã—ã„ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’ç™ºè¦‹: ğŸš€ Constructor Inteligente de Micro-Sitios para Emprendedores Chilenos. AplicaciÃ³n React que genera sitios web optimizados con wizard de 5 pasos, preview en tiempo real y export ZIP. Stack: React 18 + Vite 5 + Tailwind v4. (â­0 | ğŸ´0)",
      "content": "ğŸš€ Constructor Inteligente de Micro-Sitios para Emprendedores Chilenos. AplicaciÃ³n React que genera sitios web optimizados con wizard de 5 pasos, preview en tiempo real y export ZIP. Stack: React 18 + Vite 5 + Tailwind v4.\n\nè¨€èª: JavaScript\nã‚¹ã‚¿ãƒ¼æ•°: 0\nãƒ•ã‚©ãƒ¼ã‚¯æ•°: 0",
      "publishedAt": "2025-11-05T06:27:43.000Z",
      "source": "GitHub Search",
      "sourceUrl": "https://github.com/Simbyte-2025/ms2025-site-generator",
      "category": "community",
      "company": "Community",
      "imageUrl": null,
      "tags": [
        "AI"
      ],
      "featured": false,
      "showOnTopPage": false
    },
    {
      "id": "mhlm8ygk0rgys5cle5v",
      "title": "ç™ºè¦‹: ray-sachin/LLMPages - Auto-generated app for task: Create and publish files as a public GitHub Pages s",
      "summary": "æ–°ã—ã„ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’ç™ºè¦‹: Auto-generated app for task: Create and publish files as a public GitHub Pages site for LLMPages task. (â­0 | ğŸ´0)",
      "content": "Auto-generated app for task: Create and publish files as a public GitHub Pages site for LLMPages task.\n\nè¨€èª: N/A\nã‚¹ã‚¿ãƒ¼æ•°: 0\nãƒ•ã‚©ãƒ¼ã‚¯æ•°: 0",
      "publishedAt": "2025-11-05T06:27:43.000Z",
      "source": "GitHub Search",
      "sourceUrl": "https://github.com/ray-sachin/LLMPages",
      "category": "community",
      "company": "Community",
      "imageUrl": null,
      "tags": [
        "LLM"
      ],
      "featured": false,
      "showOnTopPage": false
    },
    {
      "id": "mhlm8wlff7m2jirt9on",
      "title": "ç™ºè¦‹: haintbotast/bsv-okr-kpi - OKR-KPI for IT - BSV",
      "summary": "æ–°ã—ã„ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’ç™ºè¦‹: OKR-KPI for IT - BSV (â­0 | ğŸ´0)",
      "content": "OKR-KPI for IT - BSV\n\nè¨€èª: JavaScript\nã‚¹ã‚¿ãƒ¼æ•°: 0\nãƒ•ã‚©ãƒ¼ã‚¯æ•°: 0",
      "publishedAt": "2025-11-05T06:27:42.000Z",
      "source": "GitHub Search",
      "sourceUrl": "https://github.com/haintbotast/bsv-okr-kpi",
      "category": "community",
      "company": "Community",
      "imageUrl": null,
      "tags": [],
      "featured": false,
      "showOnTopPage": false
    },
    {
      "id": "mhlm8xhxwta4nzic61f",
      "title": "ç™ºè¦‹: AstrBotDevs/AstrBot - âœ¨ ä¸€ç«™å¼ LLM èŠå¤©æœºå™¨äººå¹³å°åŠå¼€å‘æ¡†ï¿½ï¿½ âœ¨ æ”¯æŒ QQã€QQé¢‘é“ã€Telegramã€ä¼å¾®ã€é£ä¹¦ã€é’‰é’‰ | çŸ¥è¯†åº“ã€MCP æœåŠ¡å™¨ã€OpenAIã€Deep",
      "summary": "æ–°ã—ã„ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’ç™ºè¦‹: âœ¨ ä¸€ç«™å¼ LLM èŠå¤©æœºå™¨äººå¹³å°åŠå¼€å‘æ¡†ï¿½ï¿½ âœ¨ æ”¯æŒ QQã€QQé¢‘é“ã€Telegramã€ä¼å¾®ã€é£ä¹¦ã€é’‰é’‰ | çŸ¥è¯†åº“ã€MCP æœåŠ¡å™¨ã€OpenAIã€DeepSeekã€Geminiã€ç¡…åŸºæµåŠ¨ã€æœˆä¹‹æš—é¢ã€Ollamaã€OneAPIã€Dify (â­13122 | ğŸ´969)",
      "content": "âœ¨ ä¸€ç«™å¼ LLM èŠå¤©æœºå™¨äººå¹³å°åŠå¼€å‘æ¡†ï¿½ï¿½ âœ¨ æ”¯æŒ QQã€QQé¢‘é“ã€Telegramã€ä¼å¾®ã€é£ä¹¦ã€é’‰é’‰ | çŸ¥è¯†åº“ã€MCP æœåŠ¡å™¨ã€OpenAIã€DeepSeekã€Geminiã€ç¡…åŸºæµåŠ¨ã€æœˆä¹‹æš—é¢ã€Ollamaã€OneAPIã€Dify\n\nè¨€èª: Python\nã‚¹ã‚¿ãƒ¼æ•°: 13122\nãƒ•ã‚©ãƒ¼ã‚¯æ•°: 969",
      "publishedAt": "2025-11-05T06:27:42.000Z",
      "source": "GitHub Search",
      "sourceUrl": "https://github.com/AstrBotDevs/AstrBot",
      "category": "community",
      "company": "Community",
      "imageUrl": null,
      "tags": [
        "Gemini",
        "Llama",
        "LLM",
        "AI",
        "OpenAI"
      ],
      "featured": false,
      "showOnTopPage": false
    },
    {
      "id": "mhlm8ygkkryof7pltvj",
      "title": "ç™ºè¦‹: Deepak-527/local-ai-chat - ğŸ–¥ï¸ Run the Mistral 7B language model locally on low-resource systems with a use",
      "summary": "æ–°ã—ã„ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’ç™ºè¦‹: ğŸ–¥ï¸ Run the Mistral 7B language model locally on low-resource systems with a user-friendly, memory-optimized desktop application. (â­0 | ğŸ´0)",
      "content": "ğŸ–¥ï¸ Run the Mistral 7B language model locally on low-resource systems with a user-friendly, memory-optimized desktop application.\n\nè¨€èª: Python\nã‚¹ã‚¿ãƒ¼æ•°: 0\nãƒ•ã‚©ãƒ¼ã‚¯æ•°: 0",
      "publishedAt": "2025-11-05T06:27:42.000Z",
      "source": "GitHub Search",
      "sourceUrl": "https://github.com/Deepak-527/local-ai-chat",
      "category": "community",
      "company": "Community",
      "imageUrl": null,
      "tags": [
        "AI",
        "Mistral"
      ],
      "featured": false,
      "showOnTopPage": false
    },
    {
      "id": "mhlm8wlfx63jnsb1o2c",
      "title": "ç™ºè¦‹: AbelitoGamer/AbelitoGamer.github.io - Un atlas para material de, o relacionado a, Funky Maker: Mobile! No asociado a K",
      "summary": "æ–°ã—ã„ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’ç™ºè¦‹: Un atlas para material de, o relacionado a, Funky Maker: Mobile! No asociado a King GC ni a los creadores originales de los mods (â­3 | ğŸ´0)",
      "content": "Un atlas para material de, o relacionado a, Funky Maker: Mobile! No asociado a King GC ni a los creadores originales de los mods\n\nè¨€èª: JavaScript\nã‚¹ã‚¿ãƒ¼æ•°: 3\nãƒ•ã‚©ãƒ¼ã‚¯æ•°: 0",
      "publishedAt": "2025-11-05T06:27:41.000Z",
      "source": "GitHub Search",
      "sourceUrl": "https://github.com/AbelitoGamer/AbelitoGamer.github.io",
      "category": "community",
      "company": "Community",
      "imageUrl": null,
      "tags": [],
      "featured": false,
      "showOnTopPage": false
    },
    {
      "id": "mhlm8ygk2mpdqysi64s",
      "title": "ç™ºè¦‹: ajay-vv/vs-llm-chat-app-template - GitHubæ–°ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ",
      "summary": "æ–°ã—ã„ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’ç™ºè¦‹: GitHubä¸Šã®æ³¨ç›®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ (â­0 | ğŸ´0)",
      "content": "\n\nè¨€èª: N/A\nã‚¹ã‚¿ãƒ¼æ•°: 0\nãƒ•ã‚©ãƒ¼ã‚¯æ•°: 0",
      "publishedAt": "2025-11-05T06:27:40.000Z",
      "source": "GitHub Search",
      "sourceUrl": "https://github.com/ajay-vv/vs-llm-chat-app-template",
      "category": "community",
      "company": "Community",
      "imageUrl": null,
      "tags": [
        "LLM"
      ],
      "featured": false,
      "showOnTopPage": false
    },
    {
      "id": "mhlm8wlf2kg8gdwk9h9",
      "title": "ç™ºè¦‹: RN227/lets-vetting - GitHubæ–°ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ",
      "summary": "æ–°ã—ã„ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’ç™ºè¦‹: GitHubä¸Šã®æ³¨ç›®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ (â­0 | ğŸ´0)",
      "content": "\n\nè¨€èª: TypeScript\nã‚¹ã‚¿ãƒ¼æ•°: 0\nãƒ•ã‚©ãƒ¼ã‚¯æ•°: 0",
      "publishedAt": "2025-11-05T06:27:39.000Z",
      "source": "GitHub Search",
      "sourceUrl": "https://github.com/RN227/lets-vetting",
      "category": "community",
      "company": "Community",
      "imageUrl": null,
      "tags": [],
      "featured": false,
      "showOnTopPage": false
    },
    {
      "id": "mhlm8wlf8za2pa4ql46",
      "title": "ç™ºè¦‹: swenkerorg/eum-facilis-esse - GitHubæ–°ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ",
      "summary": "æ–°ã—ã„ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’ç™ºè¦‹: GitHubä¸Šã®æ³¨ç›®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ (â­0 | ğŸ´0)",
      "content": "\n\nè¨€èª: JavaScript\nã‚¹ã‚¿ãƒ¼æ•°: 0\nãƒ•ã‚©ãƒ¼ã‚¯æ•°: 0",
      "publishedAt": "2025-11-05T06:27:34.000Z",
      "source": "GitHub Search",
      "sourceUrl": "https://github.com/swenkerorg/eum-facilis-esse",
      "category": "community",
      "company": "Community",
      "imageUrl": null,
      "tags": [],
      "featured": false,
      "showOnTopPage": false
    },
    {
      "id": "mhlm8ygkce67yb8hlqf",
      "title": "ç™ºè¦‹: uvk18/awesome-ai-tools - ğŸ¤– Discover a curated list of AI tools for QA, coding, content creation, design,",
      "summary": "æ–°ã—ã„ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’ç™ºè¦‹: ğŸ¤– Discover a curated list of AI tools for QA, coding, content creation, design, and more, with clear pricing options for every need. (â­1 | ğŸ´0)",
      "content": "ğŸ¤– Discover a curated list of AI tools for QA, coding, content creation, design, and more, with clear pricing options for every need.\n\nè¨€èª: N/A\nã‚¹ã‚¿ãƒ¼æ•°: 1\nãƒ•ã‚©ãƒ¼ã‚¯æ•°: 0",
      "publishedAt": "2025-11-05T06:27:34.000Z",
      "source": "GitHub Search",
      "sourceUrl": "https://github.com/uvk18/awesome-ai-tools",
      "category": "community",
      "company": "Community",
      "imageUrl": null,
      "tags": [
        "AI"
      ],
      "featured": false,
      "showOnTopPage": false
    },
    {
      "id": "mhlm8xhxoh3jmgz1j1c",
      "title": "ç™ºè¦‹: juev/awesome-stars - ğŸŒŸ Denis's starred repos, updated daily!",
      "summary": "æ–°ã—ã„ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’ç™ºè¦‹: ğŸŒŸ Denis's starred repos, updated daily! (â­27 | ğŸ´4)",
      "content": "ğŸŒŸ Denis's starred repos, updated daily!\n\nè¨€èª: N/A\nã‚¹ã‚¿ãƒ¼æ•°: 27\nãƒ•ã‚©ãƒ¼ã‚¯æ•°: 4",
      "publishedAt": "2025-11-05T06:27:28.000Z",
      "source": "GitHub Search",
      "sourceUrl": "https://github.com/juev/awesome-stars",
      "category": "community",
      "company": "Community",
      "imageUrl": null,
      "tags": [
        "AI"
      ],
      "featured": false,
      "showOnTopPage": false
    },
    {
      "id": "mhlm8xhxnjrnuhoxbeo",
      "title": "ç™ºè¦‹: FlankaLanka/dreamup - GitHubæ–°ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ",
      "summary": "æ–°ã—ã„ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’ç™ºè¦‹: GitHubä¸Šã®æ³¨ç›®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ (â­0 | ğŸ´0)",
      "content": "\n\nè¨€èª: TypeScript\nã‚¹ã‚¿ãƒ¼æ•°: 0\nãƒ•ã‚©ãƒ¼ã‚¯æ•°: 0",
      "publishedAt": "2025-11-05T06:27:26.000Z",
      "source": "GitHub Search",
      "sourceUrl": "https://github.com/FlankaLanka/dreamup",
      "category": "community",
      "company": "Community",
      "imageUrl": null,
      "tags": [],
      "featured": false,
      "showOnTopPage": false
    },
    {
      "id": "mhlm8xhxiifhg51eccn",
      "title": "ç™ºè¦‹: Jounce-lang/jounce - Jounce Full Stack Language",
      "summary": "æ–°ã—ã„ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’ç™ºè¦‹: Jounce Full Stack Language (â­0 | ğŸ´0)",
      "content": "Jounce Full Stack Language\n\nè¨€èª: Rust\nã‚¹ã‚¿ãƒ¼æ•°: 0\nãƒ•ã‚©ãƒ¼ã‚¯æ•°: 0",
      "publishedAt": "2025-11-05T06:27:25.000Z",
      "source": "GitHub Search",
      "sourceUrl": "https://github.com/Jounce-lang/jounce",
      "category": "community",
      "company": "Community",
      "imageUrl": null,
      "tags": [],
      "featured": false,
      "showOnTopPage": false
    },
    {
      "id": "mhlm8ygk5l75okjrtqg",
      "title": "ç™ºè¦‹: Bhavana-Reddy-B/AI-Question-Generator - A smart question generation system built with LangChain and OpenAI GPT models th",
      "summary": "æ–°ã—ã„ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’ç™ºè¦‹: A smart question generation system built with LangChain and OpenAI GPT models that automatically creates multiple-choice and short-answer questions from any input text or document. This project leverages LLM-powered prompt engineering and contextual embeddings to extract key information and generate accurate, diverse questions. (â­0 | ğŸ´0)",
      "content": "A smart question generation system built with LangChain and OpenAI GPT models that automatically creates multiple-choice and short-answer questions from any input text or document. This project leverages LLM-powered prompt engineering and contextual embeddings to extract key information and generate accurate, diverse questions.\n\nè¨€èª: Jupyter Notebook\nã‚¹ã‚¿ãƒ¼æ•°: 0\nãƒ•ã‚©ãƒ¼ã‚¯æ•°: 0",
      "publishedAt": "2025-11-05T06:27:20.000Z",
      "source": "GitHub Search",
      "sourceUrl": "https://github.com/Bhavana-Reddy-B/AI-Question-Generator",
      "category": "community",
      "company": "Community",
      "imageUrl": null,
      "tags": [
        "GPT",
        "LLM",
        "AI",
        "OpenAI"
      ],
      "featured": false,
      "showOnTopPage": false
    },
    {
      "id": "mhlm8zhf7abdtxz50yo",
      "title": "Anthropic: skillsã®æœ€æ–°ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆ",
      "summary": "Public repository for SkillsãŒæ›´æ–°ã•ã‚Œã¾ã—ãŸ (â­15468)",
      "content": "Public repository for Skills\n\næœ€çµ‚æ›´æ–°: 11/5/2025\nã‚¹ã‚¿ãƒ¼æ•°: 15468",
      "publishedAt": "2025-11-05T06:25:47.000Z",
      "source": "Anthropic GitHub Org",
      "sourceUrl": "https://github.com/anthropics/skills",
      "category": "tools",
      "company": "Anthropic",
      "imageUrl": null,
      "tags": [],
      "featured": true
    },
    {
      "id": "mhlm8zhfsivi13040zn",
      "title": "Anthropic: claude-cookbooksã®æœ€æ–°ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆ",
      "summary": "A collection of notebooks/recipes showcasing some fun and effective ways of using Claude.ãŒæ›´æ–°ã•ã‚Œã¾ã—ãŸ (â­26979)",
      "content": "A collection of notebooks/recipes showcasing some fun and effective ways of using Claude.\n\næœ€çµ‚æ›´æ–°: 11/5/2025\nã‚¹ã‚¿ãƒ¼æ•°: 26979",
      "publishedAt": "2025-11-05T06:20:11.000Z",
      "source": "Anthropic GitHub Org",
      "sourceUrl": "https://github.com/anthropics/claude-cookbooks",
      "category": "tools",
      "company": "Anthropic",
      "imageUrl": null,
      "tags": [
        "Claude"
      ],
      "featured": true
    },
    {
      "id": "mhlm8zhfhmo6md0e9k8",
      "title": "Anthropic: claude-quickstartsã®æœ€æ–°ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆ",
      "summary": "A collection of projects designed to help developers quickly get started with building deployable applications using the Claude APIãŒæ›´æ–°ã•ã‚Œã¾ã—ãŸ (â­10181)",
      "content": "A collection of projects designed to help developers quickly get started with building deployable applications using the Claude API\n\næœ€çµ‚æ›´æ–°: 11/5/2025\nã‚¹ã‚¿ãƒ¼æ•°: 10181",
      "publishedAt": "2025-11-05T05:50:15.000Z",
      "source": "Anthropic GitHub Org",
      "sourceUrl": "https://github.com/anthropics/claude-quickstarts",
      "category": "tools",
      "company": "Anthropic",
      "imageUrl": null,
      "tags": [
        "Claude"
      ],
      "featured": true
    },
    {
      "id": "mhlm8zhf2cprrftrmi3",
      "title": "Anthropic: claude-agent-sdk-typescriptã®æœ€æ–°ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆ",
      "summary": "claude-agent-sdk-typescriptãŒæ›´æ–°ã•ã‚Œã¾ã—ãŸ (â­295)",
      "content": "\n\næœ€çµ‚æ›´æ–°: 11/5/2025\nã‚¹ã‚¿ãƒ¼æ•°: 295",
      "publishedAt": "2025-11-05T05:18:44.000Z",
      "source": "Anthropic GitHub Org",
      "sourceUrl": "https://github.com/anthropics/claude-agent-sdk-typescript",
      "category": "tools",
      "company": "Anthropic",
      "imageUrl": null,
      "tags": [
        "Claude"
      ],
      "featured": true
    },
    {
      "id": "mhlm8zhfo11z8b8b4rl",
      "title": "Anthropic: prompt-eng-interactive-tutorialã®æœ€æ–°ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆ",
      "summary": "Anthropic's Interactive Prompt Engineering TutorialãŒæ›´æ–°ã•ã‚Œã¾ã—ãŸ (â­25821)",
      "content": "Anthropic's Interactive Prompt Engineering Tutorial\n\næœ€çµ‚æ›´æ–°: 11/5/2025\nã‚¹ã‚¿ãƒ¼æ•°: 25821",
      "publishedAt": "2025-11-05T04:47:37.000Z",
      "source": "Anthropic GitHub Org",
      "sourceUrl": "https://github.com/anthropics/prompt-eng-interactive-tutorial",
      "category": "tools",
      "company": "Anthropic",
      "imageUrl": null,
      "tags": [
        "Anthropic"
      ],
      "featured": true
    },
    {
      "id": "mhlm8twh1avzkrbchyo",
      "title": "Google: Release v0.12.0",
      "summary": "## What's Changed\r\n* chore(release): bump version to 0.12.0-nightly.20251022.0542de95 by @gemini-cli-robot in https://github.com/google-gemini/gemini-cli/pull/11672\r\n* fix(test): unskip and fix useToolScheduler tests by @SandyTao520 in https://github.com/google-gemini/gemini-cli/pull/11671\r\n* Add ex...",
      "content": "## What's Changed\r\n* chore(release): bump version to 0.12.0-nightly.20251022.0542de95 by @gemini-cli-robot in https://github.com/google-gemini/gemini-cli/pull/11672\r\n* fix(test): unskip and fix useToolScheduler tests by @SandyTao520 in https://github.com/google-gemini/gemini-cli/pull/11671\r\n* Add extension alias for extensions command by @kevinjwang1 in https://github.com/google-gemini/gemini-cli/pull/11622\r\n* feat(infra) - Create a workflow for deflake by @shishu314 in https://github.com/google-gemini/gemini-cli/pull/11535\r\n* Add setting to disable YOLO mode by @Adib234 in https://github.com/google-gemini/gemini-cli/pull/11609\r\n* feat(infra) - Update status for chained e2e by @shishu314 in https://github.com/google-gemini/gemini-cli/pull/11651\r\n* extract console error to util func by @abhipatel12 in https://github.com/google-gemini/gemini-cli/pull/11675\r\n* Document todo tool by @scidomino in https://github.com/google-gemini/gemini-cli/pull/11695\r\n* feat(infra) - Add logging for when user tries to exit multiple times by @shishu314 in https://github.com/google-gemini/gemini-cli/pull/11218\r\n* fix(ui): Fix and unskip InputPrompt tests by @SandyTao520 in https://github.com/google-gemini/gemini-cli/pull/11700\r\n* Docs: adds 2025-10-13 changelog. by @jkcinouye in https://github.com/google-gemini/gemini-cli/pull/11751\r\n* feat(preflight): Use venv for yamllint installation by @JayadityaGit in https://github.com/google-gemini/gemini-cli/pull/11694\r\n* fix(a2a-server): Fix and unskip GCS persistence test by @SandyTao520 in https://github.com/google-gemini/gemini-cli/pull/11755\r\n* fix(cli): fix race condition and unskip tests in useGitBranchName by @SandyTao520 in https://github.com/google-gemini/gemini-cli/pull/11759\r\n* refactor: simplify FilterReport and remove unused code by @EricRahm in https://github.com/google-gemini/gemini-cli/pull/11681\r\n* refactor(core): Clean up exclude description by @EricRahm in https://github.com/google-gemini/gemini-cli/pull/11678\r\n* Refactor KeypressContext by @scidomino in https://github.com/google-gemini/gemini-cli/pull/11677\r\n* fix(ui): resolve race condition in double-escape handler by @Lyonk71 in https://github.com/google-gemini/gemini-cli/pull/8913\r\n* refactor(cli): Parameterize tests in InputPrompt by @MJjainam in https://github.com/google-gemini/gemini-cli/pull/11776\r\n* Docs: Fix broken link in docs/cli/configuration.md by @Smetalo in https://github.com/google-gemini/gemini-cli/pull/11655\r\n* Adds executeCommand endpoint with support for /extensions list by @jdgarrido1105 in https://github.com/google-gemini/gemini-cli/pull/11515\r\n* Fix broken links in documentation by @Smetalo in https://github.com/google-gemini/gemini-cli/pull/11789\r\n* Re-enable test. by @cornmander in https://github.com/google-gemini/gemini-cli/pull/11628\r\n* Add extension settings to be requested on install by @chrstnb in https://github.com/google-gemini/gemini-cli/pull/9802\r\n* feat: Add lychee-action to check for broken links by @Smetalo in https://github.com/google-gemini/gemini-cli/pull/11781\r\n* fix(infra) - Remove context input for setting status by @shishu314 in https://github.com/google-gemini/gemini-cli/pull/11734\r\n* Fix bug where tool scheduler was repeatedly created. by @jacob314 in https://github.com/google-gemini/gemini-cli/pull/11767\r\n* feat(infra) - Make merge group and pushes run chained e2e by @shishu314 in https://github.com/google-gemini/gemini-cli/pull/11796\r\n* feat(ux): Surface internal errors via unified event system by @abhipatel12 in https://github.com/google-gemini/gemini-cli/pull/11803\r\n* Create ExtensionManager class which manages all high level extension tasks by @jakemac53 in https://github.com/google-gemini/gemini-cli/pull/11667\r\n* fix(infra) - Fix merge queue skipper issues for chain e2e by @shishu314 in https://github.com/google-gemini/gemini-cli/pull/11810\r\n* fix: align shell allowlist handling (#11510) by @cornmander in https://github.com/google-gemini/gemini-cli/pull/11813\r\n* Use raw writes to stdin where possible in tests by @scidomino in https://github.com/google-gemini/gemini-cli/pull/11837\r\n* Added parameterization to base-storage-token.test and prompts.test.ts by @IamRiddhi in https://github.com/google-gemini/gemini-cli/pull/11821\r\n* feat(core) Bump get-ripgrep version. by @joshualitt in https://github.com/google-gemini/gemini-cli/pull/11698\r\n* First take at mocking out gemini cli responses in integration tests by @jakemac53 in https://github.com/google-gemini/gemini-cli/pull/11156\r\n* Use raw writes to stdin in test by @scidomino in https://github.com/google-gemini/gemini-cli/pull/11871\r\n* fix(cli): re-throw errors in non-interactive mode by @SandyTao520 in https://github.com/google-gemini/gemini-cli/pull/11849\r\n* Adding Parameterised tests by @IamRiddhi in https://github.com/google-gemini/gemini-cli/pull/11930\r\n* chore(core): add token caching in google auth provider by @adamfweidman in https://github.com/google-gemini/gemini-cli/pull/11946\r\n* docs(cli): update telemetry documentation by @jerop in https://github.com/google-gemini/gemini-cli/pull/11806\r\n* run bom test on windows by @sehoon38 in https://github.com/google-gemini/gemini-cli/pull/11828\r\n* Stop logging session ids on extension events by @owenofbrien in https://github.com/google-gemini/gemini-cli/pull/11941\r\n* Fix(cli): Use the correct extensionPath by @EricRahm in https://github.com/google-gemini/gemini-cli/pull/11896\r\n* fix(security) - Use emitFeedback by @shishu314 in https://github.com/google-gemini/gemini-cli/pull/11961\r\n* fix(security) - Use emitFeedback instead of console error by @shishu314 in https://github.com/google-gemini/gemini-cli/pull/11948\r\n* fix: handle request retries and model fallback correctly by @gsquared94 in https://github.com/google-gemini/gemini-cli/pull/11624\r\n* fix(infra) - Simplify cancel in progress and add permission to set status step by @shishu314 in https://github.com/google-gemini/gemini-cli/pull/11835\r\n* Add regression tests for shell command parsing by @cornmander in https://github.com/google-gemini/gemini-cli/pull/11962\r\n* Fix(cli): Use cross-platform path separators in extension tests by @EricRahm in https://github.com/google-gemini/gemini-cli/pull/11970\r\n* fix linked extension test on windows by @jakemac53 in https://github.com/google-gemini/gemini-cli/pull/11973\r\n* feat(core): Introduce message bus for tool execution confirmation by @allenhutchison in https://github.com/google-gemini/gemini-cli/pull/11544\r\n* fix(cli): Use correct defaults for file filtering by @EricRahm in https://github.com/google-gemini/gemini-cli/pull/11426\r\n* fix(core): use debugLogger.warn for loop detection errors by @SandyTao520 in https://github.com/google-gemini/gemini-cli/pull/11986\r\n* fix(update): replace update-notifier with latest-version  by @galz10 in https://github.com/google-gemini/gemini-cli/pull/11989\r\n* use coreEvents.emitFeedback in extension enablement by @jakemac53 in https://github.com/google-gemini/gemini-cli/pull/11985\r\n* Fix tests by @abhipatel12 in https://github.com/google-gemini/gemini-cli/pull/11998\r\n* Support redirects in fetchJson, add tests for it by @jakemac53 in https://github.com/google-gemini/gemini-cli/pull/11993\r\n* fix(tools): ReadFile no longer shows confirmation when message bus is off by @allenhutchison in https://github.com/google-gemini/gemini-cli/pull/12003\r\n* use debugLogger instead of console.error by @jakemac53 in https://github.com/google-gemini/gemini-cli/pull/11990\r\n* Support paste markers split across writes. by @scidomino in https://github.com/google-gemini/gemini-cli/pull/11977\r\n* refactor: Switch over to unified shouldIgnoreFile by @EricRahm in https://github.com/google-gemini/gemini-cli/pull/11815\r\n* Fix typo in: packages/cli/src/utils/handleAutoUpdate.ts by @Qiyu-Wei in https://github.com/google-gemini/gemini-cli/pull/11809\r\n* docs(contributing): update project structure section with missing packages by @0xlakshan in https://github.com/google-gemini/gemini-cli/pull/11599\r\n* Make PASTE_WORKAROUND the default. by @scidomino in https://github.com/google-gemini/gemini-cli/pull/12008\r\n* refactor(cli): replace custom wait with vi.waitFor in InputPrompt tests by @SandyTao520 in https://github.com/google-gemini/gemini-cli/pull/12005\r\n* Fix the shortenPath function to correctly insert ellipsis. by @ph-sp in https://github.com/google-gemini/gemini-cli/pull/12004\r\n* fix(core): Prepend user message to loop detection history if it starts with a function call by @SandyTao520 in https://github.com/google-gemini/gemini-cli/pull/11860\r\n* Remove non-existent parallel flag. by @scidomino in https://github.com/google-gemini/gemini-cli/pull/12018\r\n* First batch of fixing tests to use best practices. by @jacob314 in https://github.com/google-gemini/gemini-cli/pull/11964\r\n* add absolute file path description for windows by @gsehgal in https://github.com/google-gemini/gemini-cli/pull/12007\r\n* fix(cli): Add delimiter before printing tool response in non-interactive mode by @krishna-kb in https://github.com/google-gemini/gemini-cli/pull/11351\r\n* fix: user configured oauth scopes should take precedence over discovered scopes by @jackwotherspoon in https://github.com/google-gemini/gemini-cli/pull/12088\r\n* feat(core, cli): Implement sequential approval. by @joshualitt in https://github.com/google-gemini/gemini-cli/pull/11593\r\n* Refactor vim.test.ts: Use Parameterized Tests by @IamRiddhi in https://github.com/google-gemini/gemini-cli/pull/11969\r\n* docs(github): revamp pull request template by @jerop in https://github.com/google-gemini/gemini-cli/pull/11949\r\n* use debugLogger instead of console by @scidomino in https://github.com/google-gemini/gemini-cli/pull/12095\r\n* Implementing support for recitations events in responses from A2A Server by @alisa-alisa in https://github.com/google-gemini/gemini-cli/pull/12067\r\n* fix(core): update loop detection LLM schema fields by @SandyTao520 in https://github.com/google-gemini/gemini-cli/pull/12091\r\n* Docs: Contributing guide by @jkcinouye in https://github.com/google-gemini/gemini-cli/pull/12012\r\n* Create BYOID auth client when detecting BYOID credentials by @cocosheng-g in https://github.com/google-gemini/gemini-cli/pull/11592\r\n* feat(ID token support): Add ID token support for authenticating to MCâ€¦ by @RuchikaGoel in https://github.com/google-gemini/gemini-cli/pull/12031\r\n* fix(telemetry): Prevent duplicate StartSessionEvent logging by @SandyTao520 in https://github.com/google-gemini/gemini-cli/pull/12090\r\n* refactor(core): extract ChatCompressionService from GeminiClient by @SandyTao520 in https://github.com/google-gemini/gemini-cli/pull/12001\r\n* fix(ci): tsc build for package/core is idempodent by @mattKorwel in https://github.com/google-gemini/gemini-cli/pull/12112\r\n* (fix): appcontainer should not poll and footer should use currentModel from ui state by @psinha40898 in https://github.com/google-gemini/gemini-cli/pull/11923\r\n* feat: added basic dev otel trace instrumentation by @pavelgj in https://github.com/google-gemini/gemini-cli/pull/11690\r\n* Fix config test so it passes even if the user running the test happens to have set GEMINI_MODEL to flash by @jacob314 in https://github.com/google-gemini/gemini-cli/pull/12114\r\n* Migrate to coreEvents/debugLogger by @scidomino in https://github.com/google-gemini/gemini-cli/pull/12107\r\n* Added a a script to setup and run genkit telemetry and dev ui by @pavelgj in https://github.com/google-gemini/gemini-cli/pull/12120\r\n* refactor(core): Parameterize tests in glob.test.ts by @MJjainam in https://github.com/google-gemini/gemini-cli/pull/12061\r\n* docs: update installation section in README by @Tekrah123 in https://github.com/google-gemini/gemini-cli/pull/12035\r\n* Revert \"feat(ID token support): Add ID token support for authenticating to MCâ€¦\" by @adamfweidman in https://github.com/google-gemini/gemini-cli/pull/12162\r\n* chore(console): change console errors in sa-impersontation by @adamfweidman in https://github.com/google-gemini/gemini-cli/pull/12165\r\n* revert nightly schedule by @skeshive in https://github.com/google-gemini/gemini-cli/pull/11653\r\n* Add ExtensionLoader interface, use that on Config object by @jakemac53 in https://github.com/google-gemini/gemini-cli/pull/12116\r\n* feat(policy): Introduce config-based policy engine with TOML configuration by @allenhutchison in https://github.com/google-gemini/gemini-cli/pull/11992\r\n* refactor: Migrate console.error to debugLogger.warn in atCommandProcessor.ts by @allenhutchison in https://github.com/google-gemini/gemini-cli/pull/12134\r\n* feat: Add message bus setting guard for tool confirmation by @allenhutchison in https://github.com/google-gemini/gemini-cli/pull/12169\r\n* Migrate tests to use avoid jsdom by @jacob314 in https://github.com/google-gemini/gemini-cli/pull/12118\r\n* feat: Add explore subcommand for extension by @JayadityaGit in https://github.com/google-gemini/gemini-cli/pull/11846\r\n* fix(infra) - Continue workflow when merge queue skipper fail by @shishu314 in https://github.com/google-gemini/gemini-cli/pull/10509\r\n* Add support for sensitive keychain-stored per-extension settings by @chrstnb in https://github.com/google-gemini/gemini-cli/pull/11953\r\n* chore: migrate console.error in useGeminiStream by @hritan in https://github.com/google-gemini/gemini-cli/pull/12157\r\n* chore: migrate console.error in workspaceContext by @hritan in https://github.com/google-gemini/gemini-cli/pull/12167\r\n* Change debug drawer keybinding to F12 by @scidomino in https://github.com/google-gemini/gemini-cli/pull/12171\r\n* Record model responses with --record-responses (for use in testing) by @jakemac53 in https://github.com/google-gemini/gemini-cli/pull/11894\r\n* feat: Add Open Telemetric semantic standard compliant log by @bobcatfish in https://github.com/google-gemini/gemini-cli/pull/11975\r\n* Remove obsolete snapshots by @scidomino in https://github.com/google-gemini/gemini-cli/pull/12180\r\n* Disable model routing for oauth users by @mrcabbage972 in https://github.com/google-gemini/gemini-cli/pull/11889\r\n* feat(docs): Symlink CONTRIBUTING.md in the docs folder so that the site can pick it up. by @richieforeman in https://github.com/google-gemini/gemini-cli/pull/12178\r\n* fix(patch): cherry-pick 82c1042 to release/v0.12.0-preview.2-pr-12231 to patch version v0.12.0-preview.2 and create version 0.12.0-preview.3 by @gemini-cli-robot in https://github.com/google-gemini/gemini-cli/pull/12320\r\n* fix(patch): cherry-pick 68afb72 to release/v0.12.0-preview.3-pr-12306 to patch version v0.12.0-preview.3 and create version 0.12.0-preview.4 by @gemini-cli-robot in https://github.com/google-gemini/gemini-cli/pull/12327\r\n* fix(patch): cherry-pick 643f2c0 to release/v0.12.0-preview.4-pr-12300 to patch version v0.12.0-preview.4 and create version 0.12.0-preview.5 by @gemini-cli-robot in https://github.com/google-gemini/gemini-cli/pull/12329\r\n* fix(patch): cherry-pick 3332703 to release/v0.12.0-preview.5-pr-12317 to patch version v0.12.0-preview.5 and create version 0.12.0-preview.6 by @gemini-cli-robot in https://github.com/google-gemini/gemini-cli/pull/12334\r\n* fix(patch): cherry-pick 135d981 to release/v0.12.0-preview.6-pr-12299 to patch version v0.12.0-preview.6 and create version 0.12.0-preview.7 by @gemini-cli-robot in https://github.com/google-gemini/gemini-cli/pull/12368\r\n* fix(patch): cherry-pick 11e1e98 to release/v0.12.0-preview.7-pr-12347 to patch version v0.12.0-preview.7 and create version 0.12.0-preview.8 by @gemini-cli-robot in https://github.com/google-gemini/gemini-cli/pull/12383\r\n* fix(patch): cherry-pick fd2cbac to release/v0.12.0-preview.9-pr-12399 [CONFLICTS] by @gemini-cli-robot in https://github.com/google-gemini/gemini-cli/pull/12488\r\n* fix: cherry-pick commits for release by @abhipatel12 in https://github.com/google-gemini/gemini-cli/pull/12549\r\n* Cherry pick screen reader nudge changes by @chrstnb in https://github.com/google-gemini/gemini-cli/pull/12553\r\n\r\n## New Contributors\r\n* @jdgarrido1105 made their first contribution in https://github.com/google-gemini/gemini-cli/pull/11515\r\n* @Qiyu-Wei made their first contribution in https://github.com/google-gemini/gemini-cli/pull/11809\r\n* @0xlakshan made their first contribution in https://github.com/google-gemini/gemini-cli/pull/11599\r\n* @ph-sp made their first contribution in https://github.com/google-gemini/gemini-cli/pull/12004\r\n* @krishna-kb made their first contribution in https://github.com/google-gemini/gemini-cli/pull/11351\r\n* @alisa-alisa made their first contribution in https://github.com/google-gemini/gemini-cli/pull/12067\r\n* @cocosheng-g made their first contribution in https://github.com/google-gemini/gemini-cli/pull/11592\r\n* @RuchikaGoel made their first contribution in https://github.com/google-gemini/gemini-cli/pull/12031\r\n* @pavelgj made their first contribution in https://github.com/google-gemini/gemini-cli/pull/11690\r\n* @Tekrah123 made their first contribution in https://github.com/google-gemini/gemini-cli/pull/12035\r\n\r\n**Full Changelog**: https://github.com/google-gemini/gemini-cli/compare/v0.11.3...v0.12.0",
      "publishedAt": "2025-11-04T23:31:01.000Z",
      "source": "Google GitHub",
      "sourceUrl": "https://github.com/google-gemini/gemini-cli/releases/tag/v0.12.0",
      "category": "tools",
      "company": "Google",
      "imageUrl": null,
      "tags": [
        "Gemini",
        "LLM",
        "AI",
        "ML",
        "Google"
      ],
      "featured": true
    },
    {
      "id": "mhlm8pecchen3xrrm4h",
      "title": "Anthropic: claude-codeã®æœ€æ–°ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆ",
      "summary": "claude-codeãƒªãƒã‚¸ãƒˆãƒªã«æ–°ã—ã„æ›´æ–°: chore: Update CHANGELOG.md...",
      "content": "chore: Update CHANGELOG.md",
      "publishedAt": "2025-11-04T22:34:37.000Z",
      "source": "Anthropic GitHub",
      "sourceUrl": "https://github.com/anthropics/claude-code",
      "category": "tools",
      "company": "Anthropic",
      "imageUrl": null,
      "tags": [
        "Claude"
      ],
      "featured": false
    },
    {
      "id": "mhlm8twgfo5ik0xfurl",
      "title": "Google: Release v0.12.0-preview.11",
      "summary": "## What's Changed\n* fix(patch): cherry-pick fd2cbac to release/v0.12.0-preview.9-pr-12399 [CONFLICTS] by @gemini-cli-robot in https://github.com/google-gemini/gemini-cli/pull/12488\n* fix: cherry-pick commits for release by @abhipatel12 in https://github.com/google-gemini/gemini-cli/pull/12549\n* Cher...",
      "content": "## What's Changed\n* fix(patch): cherry-pick fd2cbac to release/v0.12.0-preview.9-pr-12399 [CONFLICTS] by @gemini-cli-robot in https://github.com/google-gemini/gemini-cli/pull/12488\n* fix: cherry-pick commits for release by @abhipatel12 in https://github.com/google-gemini/gemini-cli/pull/12549\n* Cherry pick screen reader nudge changes by @chrstnb in https://github.com/google-gemini/gemini-cli/pull/12553\n\n\n**Full Changelog**: https://github.com/google-gemini/gemini-cli/compare/v0.12.0-preview.9...v0.12.0-preview.11",
      "publishedAt": "2025-11-04T22:04:19.000Z",
      "source": "Google GitHub",
      "sourceUrl": "https://github.com/google-gemini/gemini-cli/releases/tag/v0.12.0-preview.11",
      "category": "tools",
      "company": "Google",
      "imageUrl": null,
      "tags": [
        "Gemini",
        "Google"
      ],
      "featured": false
    },
    {
      "id": "mhlm8szsa7jcxqfp6s",
      "title": "Google: cookbookã®æœ€æ–°ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆ",
      "summary": "cookbookãƒªãƒã‚¸ãƒˆãƒªã«æ–°ã—ã„æ›´æ–°: Update Datasets.ipynb (#1023)\n\nGrammar correction. Changed \"an\" to \"a\"....",
      "content": "Update Datasets.ipynb (#1023)\n\nGrammar correction. Changed \"an\" to \"a\".",
      "publishedAt": "2025-11-01T10:45:09.000Z",
      "source": "Google GitHub",
      "sourceUrl": "https://github.com/google-gemini/cookbook",
      "category": "tools",
      "company": "Google",
      "imageUrl": null,
      "tags": [],
      "featured": false
    },
    {
      "id": "mhlm8s32oylju2r7vs",
      "title": "OpenAI: openai-cookbookã®æœ€æ–°ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆ",
      "summary": "openai-cookbookãƒªãƒã‚¸ãƒˆãƒªã«æ–°ã—ã„æ›´æ–°: Update gpt-oss-safeguard-guide.md...",
      "content": "Update gpt-oss-safeguard-guide.md",
      "publishedAt": "2025-10-29T14:53:12.000Z",
      "source": "OpenAI GitHub",
      "sourceUrl": "https://github.com/openai/openai-cookbook",
      "category": "tools",
      "company": "OpenAI",
      "imageUrl": null,
      "tags": [
        "GPT",
        "AI",
        "OpenAI"
      ],
      "featured": false
    },
    {
      "id": "mhlm8ohbfmaj1uc1eyl",
      "title": "Anthropic: v0.72.0",
      "summary": "## 0.72.0 (2025-10-28)\n\nFull Changelog: [v0.71.1...v0.72.0](https://github.com/anthropics/anthropic-sdk-python/compare/v0.71.1...v0.72.0)\n\n### Features\n\n* **api:** add ability to clear thinking in context management ([27c8f17](https://github.com/anthropics/anthropic-sdk-python/commit/27c8f17c573c73c...",
      "content": "## 0.72.0 (2025-10-28)\n\nFull Changelog: [v0.71.1...v0.72.0](https://github.com/anthropics/anthropic-sdk-python/compare/v0.71.1...v0.72.0)\n\n### Features\n\n* **api:** add ability to clear thinking in context management ([27c8f17](https://github.com/anthropics/anthropic-sdk-python/commit/27c8f17c573c73c4db2146731ef1ab712140b0a2))",
      "publishedAt": "2025-10-28T19:12:04.000Z",
      "source": "Anthropic GitHub",
      "sourceUrl": "https://github.com/anthropics/anthropic-sdk-python/releases/tag/v0.72.0",
      "category": "tools",
      "company": "Anthropic",
      "imageUrl": null,
      "tags": [
        "Anthropic"
      ],
      "featured": false
    },
    {
      "id": "mhlm8ohbigpjaplv8u",
      "title": "Anthropic: v0.71.1",
      "summary": "## 0.71.1 (2025-10-28)\n\nFull Changelog: [v0.71.0...v0.71.1](https://github.com/anthropics/anthropic-sdk-python/compare/v0.71.0...v0.71.1)\n\n### Bug Fixes\n\n* **client:** resolve non-functional default socket options ([4606137](https://github.com/anthropics/anthropic-sdk-python/commit/4606137fcca27ab2d...",
      "content": "## 0.71.1 (2025-10-28)\n\nFull Changelog: [v0.71.0...v0.71.1](https://github.com/anthropics/anthropic-sdk-python/compare/v0.71.0...v0.71.1)\n\n### Bug Fixes\n\n* **client:** resolve non-functional default socket options ([4606137](https://github.com/anthropics/anthropic-sdk-python/commit/4606137fcca27ab2d03669999b624c11394b090a))\n\n\n### Chores\n\n* **api:** mark older sonnet models as deprecated ([7906595](https://github.com/anthropics/anthropic-sdk-python/commit/7906595fe2f214cf0449d073145629ea8d3da437))\n* bump `httpx-aiohttp` version to 0.1.9 ([5d27492](https://github.com/anthropics/anthropic-sdk-python/commit/5d2749222bb75201279c1877690c75687f3f8abc))",
      "publishedAt": "2025-10-28T17:28:15.000Z",
      "source": "Anthropic GitHub",
      "sourceUrl": "https://github.com/anthropics/anthropic-sdk-python/releases/tag/v0.71.1",
      "category": "tools",
      "company": "Anthropic",
      "imageUrl": null,
      "tags": [
        "AI",
        "Anthropic"
      ],
      "featured": false
    },
    {
      "id": "mhlm94tuehvk7zwmmic",
      "title": "Show HN: MCP Agent Mail, Like Gmail for Coding Agents",
      "summary": "I finally got around to making a tool I&#x27;ve wanted for a long time: you can basically think of it as being &quot;like Gmail for coding agents.&quot;<p>If you&#x27;ve ever tried to use a bunch of instances of Claude Code or Codex at once across the same project, you&#x27;ve probably noticed how a...",
      "content": "I finally got around to making a tool I&#x27;ve wanted for a long time: you can basically think of it as being &quot;like Gmail for coding agents.&quot;<p>If you&#x27;ve ever tried to use a bunch of instances of Claude Code or Codex at once across the same project, you&#x27;ve probably noticed how annoying it can be when they freak out about the other agent changing the files they&#x27;re working on.<p>Then they start doing annoying things, like restoring files from git, in the process wiping out another agent&#x27;s work without a backup.<p>Or if you&#x27;ve tried to have agents coordinate on two separate repos, like a Python backend and a Nextjs frontend for the same project, you may have found yourself acting as the go-between and liaison between two or three different agents, passing messages between them or having them communicate by means of markdown files or some other workaround.<p>I always knew there had to be a better way. But it&#x27;s hard to get the big providers to offer something like that in a way that&#x27;s universal, because Anthropic doesn&#x27;t want to integrate with OpenAI&#x27;s competitive coding tool, and neither wants to deal with Cursor or Gemini-CLI.<p>So a few days ago, I started working on it, and it&#x27;s now ready to share with the world. Introducing the 100% open-source MCP Agent Mail tool. This can be set up very quickly and easily on your machine and automatically detects all the most common coding agents and configures everything for you.<p>I also include a ready-made blurb (see the README file in the repo) that you can add to your existing AGENTS dot md or CLAUDE dot md file to help the agents better leverage the system straight out of the gate.<p>It&#x27;s almost comical how quickly the agents take to this system like a fish to water. They seem to relish in it, sending very detailed messages to each other just like humans do, and start coordinating in a natural, powerful way. They even give each other good ideas and pushback on bad ideas.<p>They can also reserve access to certain files to avoid the &quot;too many cooks&quot; problems associated with having too many agents all working on the same project at the same time, all without dealing with git worktrees and &quot;merge hell.&quot;<p>This also introduces a natural and powerful way to do something I&#x27;ve also long wanted, which is to automatically have multiple different frontier models working together in a collaborative, complementary way without me needing to be in the middle coordinating everything like a parent setting up playdates for their kids.<p>And for the human in the loop, I made a really slick web frontend that you can view and see all the messages your agents are sending each other in a nice, Gmail-like interface, so you can monitor the process. You can even send a special message to some or all your agents as the &quot;Human Overseer&quot; to give them a directive (of course, you can also just type that in manually into each coding agent, too.)<p>I made this for myself and know that I&#x27;m going to be getting a ton of usage out of it going forward. It really lets you unleash a massive number of agents using a bunch of different tools&#x2F;models, and they just naturally coordinate and work with each other without stepping on each other&#x27;s toes. It lets you as the human overseer relax a bit more as you no longer have to be the one responsible for coordinating things, and also because the agents watch each other and push back when they see mistakes and errors happening. Obviously, the greater the variety of models and agent tools you use, the more valuable that emergent peer review process will be.<p>Anyway, give it a try and let me know what you think. I&#x27;m sure there are a bunch of bugs that I&#x27;ll have to iron out over the next couple days, but I&#x27;ve already been productively using it today to work on another project and it is pretty amazingly functional already!",
      "publishedAt": "2025-10-27T12:46:13.000Z",
      "source": "Hacker News Claude/Anthropic",
      "sourceUrl": "https://github.com/Dicklesworthstone/mcp_agent_mail",
      "category": "companies",
      "company": "Anthropic",
      "imageUrl": null,
      "tags": [
        "Claude",
        "Gemini",
        "AI",
        "OpenAI",
        "Anthropic"
      ],
      "featured": false
    },
    {
      "id": "mhlm8vmna6ey495pt4",
      "title": "Hugging Face: Patch release v4.57.1",
      "summary": "This patch most notably fixes an issue with an optional dependency (`optax`), which resulted in parsing errors with `poetry`. It contains the following fixes:\r\n\r\n- [fix optax dep issue](https://github.com/huggingface/transformers/commit/0645c9ec3188e000aecf5060e2cdabcc156bb794)\r\n- [remove offload_st...",
      "content": "This patch most notably fixes an issue with an optional dependency (`optax`), which resulted in parsing errors with `poetry`. It contains the following fixes:\r\n\r\n- [fix optax dep issue](https://github.com/huggingface/transformers/commit/0645c9ec3188e000aecf5060e2cdabcc156bb794)\r\n- [remove offload_state_dict from kwargs](https://github.com/huggingface/transformers/commit/a92b1e8a45e1863b95c5e2caa12f5597aee80279)\r\n- Fix bnb fsdp loading for pre-quantized checkpoint (#41415)\r\n- Fix tests fsdp (#41422)\r\n- Fix trainer for py3.9 (#41359)",
      "publishedAt": "2025-10-14T15:39:34.000Z",
      "source": "Hugging Face GitHub",
      "sourceUrl": "https://github.com/huggingface/transformers/releases/tag/v4.57.1",
      "category": "tools",
      "company": "Hugging Face",
      "imageUrl": null,
      "tags": [
        "AI",
        "Transformer"
      ],
      "featured": false
    },
    {
      "id": "mhlm8vmnlyk5zi65p",
      "title": "Hugging Face: v4.57.0: Qwen3-Next, Vault Gemma, Qwen3 VL, LongCat Flash, Flex OLMO, LFM2 VL, BLT, Qwen3 OMNI MoE, Parakeet, EdgeTAM, OLMO3",
      "summary": "## New model additions\r\n\r\n### Qwen3 Next\r\n\r\n<img width=\"1200\" height=\"511\" alt=\"image\" src=\"https://github.com/user-attachments/assets/3abad6c4-5650-412d-a831-f8a30a5d962e\" />\r\n\r\nThe Qwen3-Next series represents the Qwen team's next-generation foundation models, optimized for extreme context length ...",
      "content": "## New model additions\r\n\r\n### Qwen3 Next\r\n\r\n<img width=\"1200\" height=\"511\" alt=\"image\" src=\"https://github.com/user-attachments/assets/3abad6c4-5650-412d-a831-f8a30a5d962e\" />\r\n\r\nThe Qwen3-Next series represents the Qwen team's next-generation foundation models, optimized for extreme context length and large-scale parameter efficiency. \r\nThe series introduces a suite of architectural innovations designed to maximize performance while minimizing computational cost:\r\n- **Hybrid Attention**: Replaces standard attention with the combination of **Gated DeltaNet** and **Gated Attention**, enabling efficient context modeling.  \r\n- **High-Sparsity MoE**: Achieves an extreme low activation ratio as 1:50 in MoE layers â€” drastically reducing FLOPs per token while preserving model capacity.\r\n- **Multi-Token Prediction(MTP)**: Boosts pretraining model performance, and accelerates inference.\r\n- **Other Optimizations**: Includes techniques such as **zero-centered and weight-decayed layernorm**, **Gated Attention**, and other stabilizing enhancements for robust training.  \r\n\r\nBuilt on this architecture, they trained and open-sourced Qwen3-Next-80B-A3B â€” 80B total parameters, only 3B active â€” achieving extreme sparsity and efficiency.\r\n\r\nDespite its ultra-efficiency, it outperforms Qwen3-32B on downstream tasks â€” while requiring **less than 1/10 of the training cost**. \r\nMoreover, it delivers over **10x higher inference throughput** than Qwen3-32B when handling contexts longer than 32K tokens.\r\n\r\nFor more details, please visit their blog [Qwen3-Next](qwen3_next) ([blog post](https://qwenlm.github.io/blog/qwen3_next/)).\r\n\r\n* Adding Support for Qwen3-Next  by @bozheng-hit in #40771\r\n\r\n### Vault Gemma\r\n\r\n<img width=\"1282\" height=\"392\" alt=\"image\" src=\"https://github.com/user-attachments/assets/9412905b-4083-4994-9000-aa0dbf97eb6f\" />\r\n\r\n[VaultGemma](https://services.google.com/fh/files/blogs/vaultgemma_tech_report.pdf) is a text-only decoder model derived from [Gemma 2](https://huggingface.co/docs/transformers/en/model_doc/gemma2), notably it drops the norms after the Attention and MLP blocks, and uses full attention for all layers instead of alternating between full attention and local sliding attention. VaultGemma is available as a pretrained model with 1B parameters that uses a 1024 token sequence length.\r\n\r\nVaultGemma was trained from scratch with sequence-level differential privacy (DP). Its training data includes the same mixture as the [Gemma 2 models](https://huggingface.co/collections/google/gemma-2-release-667d6600fd5220e7b967f315), consisting of a number of documents of varying lengths. Additionally, it is trained using [DP stochastic gradient descent (DP-SGD)](https://arxiv.org/abs/1607.00133) and provides a (Îµ â‰¤ 2.0, Î´ â‰¤ 1.1e-10)-sequence-level DP guarantee, where a sequence consists of 1024 consecutive tokens extracted from heterogeneous data sources. Specifically, the privacy unit of the guarantee is for the sequences after sampling and packing of the mixture.\r\n\r\n* add: differential privacy research model  by @RyanMullins in #40851\r\n\r\n### Qwen3 VL\r\n\r\n<img width=\"3544\" height=\"1886\" alt=\"image\" src=\"https://github.com/user-attachments/assets/5afa70cb-506e-4d56-baa3-30e7522ac653\" />\r\n\r\n[Qwen3-VL](https://huggingface.co/papers/2502.13923) is a multimodal vision-language model series, encompassing both dense and MoE variants, as well as Instruct and Thinking versions. \r\n\r\nBuilding upon its predecessors, Qwen3-VL delivers significant improvements in visual understanding while maintaining strong pure text capabilities. Key architectural advancements include: enhanced MRope with interleaved layout for better spatial-temporal modeling, DeepStack integration to effectively leverage multi-level features from the Vision Transformer (ViT), and improved video understanding through text-based time alignmentâ€”evolving from T-RoPE to text timestamp alignment for more precise temporal grounding. \r\n\r\nThese innovations collectively enable Qwen3-VL to achieve superior performance in complex multimodal tasks.\r\n\r\n* Adding Support for Qwen3-VL Series  by @JJJYmmm in #40795\r\n\r\n### Longcat Flash\r\n\r\n<img width=\"763\" height=\"468\" alt=\"image\" src=\"https://github.com/user-attachments/assets/289d33e0-6c71-458d-ae07-b7d454ac2adf\" />\r\n\r\nThe LongCatFlash model was proposed in [LongCat-Flash Technical Report](https://huggingface.co/papers/2509.01322) by the Meituan LongCat Team. LongCat-Flash is a 560B parameter Mixture-of-Experts (MoE) model that activates 18.6B-31.3B parameters dynamically (average ~27B). The model features a shortcut-connected architecture enabling high inference speed (>100 tokens/second) and advanced reasoning capabilities.\r\n\r\nThe abstract from the paper is the following:\r\n\r\n*We present LongCat-Flash, a 560 billion parameter Mixture-of-Experts (MoE) language model featuring a dynamic computation mechanism that activates 18.6B-31.3B parameters based on context (average ~27B). The model incorporates a shortcut-connected architecture enabling high inference speed (>100 tokens/second) and demonstrates strong performance across multiple benchmarks including 89.71% accuracy on MMLU and exceptional agentic tool use capabilities.*\r\n\r\nTips:\r\n\r\n- LongCat-Flash uses a unique shortcut-connected MoE architecture that enables faster inference compared to traditional MoE models\r\n- The model supports up to 128k context length for long-form tasks\r\n- Dynamic parameter activation makes it computationally efficient while maintaining high performance\r\n- Best suited for applications requiring strong reasoning, coding, and tool-calling capabilities\r\n- The MoE architecture includes zero experts (nn.Identity modules) which act as skip connections, allowing tokens to bypass expert computation when appropriate\r\n\r\n* Add LongCat-Flash  by @molbap in #40730\r\n\r\n### Flex Olmo\r\n\r\n<img width=\"700\" height=\"414\" alt=\"image\" src=\"https://github.com/user-attachments/assets/7b92ee0f-5f5a-459c-ad4d-e01b5c10202e\" />\r\n\r\n[FlexOlmo](https://huggingface.co/papers/2507.07024) is a new class of language models (LMs) that supports (1) distributed training without data sharing, where different model parameters are independently trained on closed datasets, and (2) data-flexible inference, where these parameters along with their associated data can be flexibly included or excluded from model inferences with no further training. FlexOlmo employs a mixture-of-experts (MoE) architecture where each expert is trained independently on closed datasets and later integrated through a new domain-informed routing without any joint training. FlexOlmo is trained on FlexMix, a corpus we curate comprising publicly available datasets alongside seven domain-specific sets, representing realistic approximations of closed sets. \r\n\r\nYou can find all the original FlexOlmo checkpoints under the [FlexOlmo](https://huggingface.co/collections/allenai/flexolmo-68471177a386b6e20a54c55f) collection.\r\n\r\n* Add FlexOlmo model  by @2015aroras in #40921\r\n\r\n### LFM2 VL\r\n\r\n<img width=\"2300\" height=\"1400\" alt=\"image\" src=\"https://github.com/user-attachments/assets/ef0605cd-9512-458c-915a-62316e14d90c\" />\r\n\r\n[LFM2-VL](https://www.liquid.ai/blog/lfm2-vl-efficient-vision-language-models) first series of vision-language foundation models developed by [Liquid AI](https://liquid.ai/). These multimodal models are designed for low-latency and device-aware deployment. LFM2-VL extends the LFM2 family of open-weight Liquid Foundation Models (LFMs) into the vision-language space, supporting both text and image inputs with variable resolutions.\r\n\r\n#### Architecture\r\n\r\nLFM2-VL consists of three main components: a language model backbone, a vision encoder, and a multimodal projector. LFM2-VL builds upon the LFM2 backbone, inheriting from either LFM2-1.2B (for LFM2-VL-1.6B) or LFM2-350M (for LFM2-VL-450M). For the vision tower, LFM2-VL uses SigLIP2 NaFlex encoders to convert input images into token sequences. Two variants are implemented:\r\n* Shape-optimized (400M) for more fine-grained vision capabilities for LFM2-VL-1.6B\r\n* Base (86M) for fast image processing for LFM2-VL-450M\r\n\r\nThe encoder processes images at their native resolution up to 512Ã—512 pixels, efficiently handling smaller images without upscaling and supporting non-standard aspect ratios without distortion. Larger images are split into non-overlapping square patches of 512Ã—512 each, preserving detail. In LFM2-VL-1.6B, the model also receives a thumbnail (a small, downscaled version of the original image capturing the overall scene) to enhance global context understanding and alignment. Special tokens mark each patchâ€™s position and indicate the thumbnailâ€™s start. The multimodal connector is a 2-layer MLP connector with pixel unshuffle to reduce image token count. \r\n\r\n* Add new model LFM2-VL  by @zucchini-nlp in #40624\r\n\r\n### BLT\r\n\r\n<img width=\"1448\" height=\"1062\" alt=\"image\" src=\"https://github.com/user-attachments/assets/af1fbb09-082c-4331-9217-357adb506cbf\" />\r\n\r\nThe BLT model was proposed in [Byte Latent Transformer: Patches Scale Better Than Tokens](<https://arxiv.org/pdf/2412.09871>) by Artidoro Pagnoni, Ram Pasunuru, Pedro Rodriguez, John Nguyen, Benjamin Muller, Margaret Li1, Chunting Zhou, Lili Yu, Jason Weston, Luke Zettlemoyer, Gargi Ghosh, Mike Lewis, Ari Holtzmanâ€ , Srinivasan Iyer.\r\nBLT is a byte-level LLM that achieves tokenization-level performance through entropy-based dynamic patching.\r\n\r\nThe abstract from the paper is the following:\r\n\r\n*We introduce the Byte Latent Transformer (BLT), a new byte-level LLM architecture that, for the first time, matches tokenization-based LLM performance at scale with significant improvements in inference\r\nefficiency and robustness. BLT encodes bytes into dynamically sized patches, which serve as the primary units of computation. Patches are segmented based on the entropy of the next byte, allocating\r\nmore compute and model capacity where increased data complexity demands it. We present the first flop controlled scaling study of byte-level models up to 8B parameters and 4T training bytes. Our results demonstrate the feasibility of scaling models trained on raw bytes without a fixed vocabulary. Both training and inference efficiency improve due to dynamically selecting long patches when data is predictable, along with qualitative improvements on reasoning and long tail generalization. Overall, for fixed inference costs, BLT shows significantly better scaling than tokenization-based models, by simultaneously growing both patch and model size.*\r\n\r\n#### Usage Tips:\r\n\r\n- **Dual Model Architecture**: BLT consists of two separate trained models:\r\n  - **Patcher (Entropy Model)**: A smaller transformer model that predicts byte-level entropy to determine patch boundaries and segment input.\r\n  - **Main Transformer Model**: The primary model that processes the patches through a Local Encoder, Global Transformer, and Local Decoder.\r\n\r\n- **Dynamic Patching**: The model uses entropy-based dynamic patching where:\r\n  - High-entropy regions (complex data) get shorter patches with more computational attention\r\n  - Low-entropy regions (predictable data) get longer patches for efficiency\r\n  - This allows the model to allocate compute resources where they're most needed\r\n\r\n- **Local Encoder**: Processes byte sequences with cross-attention to patch embeddings\r\n- **Global Transformer**: Processes patch-level representations with full attention across patches\r\n- **Local Decoder**: Generates output with cross-attention back to the original byte sequence\r\n\r\n- **Byte-Level Tokenizer**: Unlike traditional tokenizers that use learned vocabularies, BLT's tokenizer simply converts text to UTF-8 bytes and maps each byte to a token ID. There is no need for a vocabulary.\r\n\r\n* blt wip  by @itazap in #38579\r\n\r\n### Qwen3 Omni MoE\r\n\r\n<img width=\"14084\" height=\"7429\" alt=\"image\" src=\"https://github.com/user-attachments/assets/20d46a43-15f2-42bf-9703-9575f5ca4430\" />\r\n\r\nThe [Qwen2.5-Omni](https://qwenlm.github.io/blog/qwen2.5-omni/) model is a unified multiple modalities model proposed in [Qwen2.5-Omni Technical Report](https://huggingface.co/papers/2503.20215) from Qwen team, Alibaba Group.\r\n\r\n#### Notes\r\n\r\n- Use [`Qwen2_5OmniForConditionalGeneration`] to generate audio and text output. To generate only one output type, use [`Qwen2_5OmniThinkerForConditionalGeneration`] for text-only and [`Qwen2_5OmniTalkersForConditionalGeneration`] for audio-only outputs.\r\n- Audio generation with [`Qwen2_5OmniForConditionalGeneration`] supports only single batch size at the moment.\r\n- In case out out-of-memory errors hwen working with video input, decrease `processor.max_pixels`. By default the maximum is set to a very arge value and high resolution visuals will not be resized, unless resolution exceeds `processor.max_pixels`.\r\n- The processor has its own [`~ProcessorMixin.apply_chat_template`] method to convert chat messages to model inputs.\r\n\r\n* Adding support for Qwen3Omni  by @BakerBunker in #41025\r\n\r\n### Parakeet\r\n\r\n<img width=\"1431\" height=\"527\" alt=\"image\" src=\"https://github.com/user-attachments/assets/e831f451-9be3-4b5c-a222-b833a50ceb2a\" />\r\n\r\nParakeet models, [introduced by NVIDIA NeMo](https://developer.nvidia.com/blog/pushing-the-boundaries-of-speech-recognition-with-nemo-parakeet-asr-models/), are models that combine a [Fast Conformer](https://docs.nvidia.com/nemo-framework/user-guide/latest/nemotoolkit/asr/models.html#fast-conformer) encoder with connectionist temporal classification (CTC), recurrent neural network transducer (RNNT) or token and duration transducer (TDT) decoder for automatic speech recognition.\r\n\r\n**Model Architecture**\r\n- **Fast Conformer Encoder**: A linearly scalable Conformer architecture that processes mel-spectrogram features and reduces sequence length through subsampling. This is more efficient version of the Conformer Encoder found in [FastSpeech2Conformer](./fastspeech2_conformer.md) (see [`ParakeetEncoder`] for the encoder implementation and details).\r\n- [**ParakeetForCTC**](#parakeetforctc): a Fast Conformer Encoder + a CTC decoder\r\n    - **CTC Decoder**: Simple but effective decoder consisting of:\r\n        - 1D convolution projection from encoder hidden size to vocabulary size (for optimal NeMo compatibility).\r\n        - CTC loss computation for training.\r\n        - Greedy CTC decoding for inference.\r\n\r\n* Add Parakeet  by @nithinraok in #39062\r\n\r\n### EdgeTAM\r\n\r\n<img width=\"949\" height=\"537\" alt=\"image\" src=\"https://github.com/user-attachments/assets/5ca4e73d-5aa9-487d-96e1-92d4f2f4739f\" />\r\n\r\nThe EdgeTAM model was proposed in [EdgeTAM: On-Device Track Anything Model](https://huggingface.co/papers/2501.07256) Chong Zhou, Chenchen Zhu, Yunyang Xiong, Saksham Suri, Fanyi Xiao, Lemeng Wu, Raghuraman Krishnamoorthi, Bo Dai, Chen Change Loy, Vikas Chandra, Bilge Soran.\r\n\r\nEdgeTAM is an efficient adaptation of SAM 2 that introduces a 2D Spatial Perceiver architecture to optimize memory attention mechanisms for real-time video segmentation on mobile devices.\r\n\r\n* Add EdgeTAM  by @yonigozlan in #39800\r\n\r\n### OLMO3\r\n\r\nMore details to come soon :eyes:\r\n\r\n* Add Olmo3 model  by @2015aroras in #40778\r\n\r\n## Continuous batching\r\n\r\nWe are introducing Continuous Batching (CB) in this release, we consider it a stable feature. The main use case for CB is batched generation, which makes it very efficient in the context of GRPO training or evaluation. Thanks to CB, researchers or model developers are now free to use transformers in these contexts without having to spin up an additional inference engine. \r\n\r\nCB currently supports both full attention and sliding window attention: this means that the vast majority of models are supported, like llama, gemma3, gpt-oss. \r\n\r\nCB is also integrated with transformers serve, which means that you can deploy transformers as an OpenAI-compatible HTTP server.\r\nHere is a small snippet on how to use it:\r\n\r\n```python\r\nimport datasets\r\nimport torch\r\n\r\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\r\nfrom transformers.generation import GenerationConfig\r\nmodel = AutoModelForCausalLM.from_pretrained(\r\n    \"Qwen/Qwen3-4B-Instruct-2507\", dtype=torch.bfloat16, _attn_implementation=\"sdpa_paged\", device_map=\"auto\"\r\n)\r\nmodel.generation_config.max_new_tokens = 32\r\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-4B-Instruct-2507\", padding_side=\"left\")\r\ndataset = datasets.load_dataset(\"openai/gsm8k\", \"socratic\", split=\"test\")\r\ntokenized_datasets = dataset.map(lambda x: tokenizer(x[\"question\"]), batched=True)\r\nsimple_batch_inputs = [item[\"input_ids\"] for item in tokenized_datasets]\r\n\r\nbatch_outputs = model.generate_batch(inputs=simple_batch_inputs)\r\nfor request in batch_outputs:\r\n    print(tokenizer.decode(batch_outputs[request].generated_tokens))\r\n\"\"\"\r\n Let's break down the problem step by step:\r\n\r\n1. **Total eggs laid per day**:  \r\n   Janetâ€™s ducks lay **16 eggs per day**\r\n Let's break down the problem step by step:\r\n\r\n1. **Blue fiber**: The robe takes **2 bolts** of blue fiber.\r\n2. **White fiber\r\n To determine Josh's profit from flipping the house, let's go step by step.\r\n\r\n---\r\n\r\n### Step 1: Initial cost of the house\r\nJosh buys the\r\n To find the total distance James runs in a week, we can break down the problem step by step:\r\n\r\n1. **Sprints per session**: James runs \r\n To determine how many cups of feed Wendi needs to give her chickens in the final meal of the day, let's go step by step.\r\n\"\"\"\r\n```\r\n\r\n## Breaking changes\r\n\r\n* ğŸš¨ Remove Group Beam Search decoding strategy  by @manueldeprada in #40495\r\n* ğŸš¨ Remove Constrained Beam Search decoding strategy  by @manueldeprada in #40518\r\n* ğŸš¨ Allow `check_model_inputs` in core VLMs  by @zucchini-nlp in #40342\r\n* ğŸ”´ Update Glm4V to use config values  by @zucchini-nlp in #40712\r\n* ğŸš¨ Fix Inconsistant `input_feature` length and `attention_mask` length in `WhisperFeatureExtractor`  by @BakerBunker in #39221\r\n* âš ï¸ ğŸ”´ Add ministral model  by @manueldeprada in #40247\r\n* ğŸ”´ Move variable output controls to `_prepare_generation_config `  by @manueldeprada in #40715\r\n* ğŸ”´Make `center_crop` fast equivalent to slow  by @yonigozlan in #40856\r\n\r\n## Bugfixes and improvements\r\n\r\n* Fix collated reports upload filename  by @ivarflakstad in #40556\r\n* pin `pytest-rerunfailures<16.0`  by @ydshieh in #40561\r\n* remove the redundant non maintained jieba and use rjieba instead  by @divyanshsinghvi in #40383\r\n* Set `test_all_params_have_gradient=False` for `DeepseekV2ModelTest`  by @ydshieh in #40566\r\n* processor tests - use dummy videos  by @zucchini-nlp in #40537\r\n* [qwen-vl] fix position ids  by @zucchini-nlp in #40490\r\n* Fix `test_eager_matches_sdpa_inference` not run for `CLIP`  by @ydshieh in #40581\r\n* Fix CircleCI step passes in the case of pytest worker crash at test collection time  by @ydshieh in #40552\r\n* Allow `remi-or` to `run-slow`  by @ydshieh in #40590\r\n* Fix llava image processor  by @zucchini-nlp in #40588\r\n* Update `get_*_features` methods + update doc snippets  by @qubvel in #40555\r\n* Fix custom generate relative imports  by @manueldeprada in #40480\r\n* Support batch size > 1 image-text inference  by @hiyouga in #36682\r\n* Fix typos  by @cyyever in #40585\r\n* Skip `TvpImageProcessingTest::test_slow_fast_equivalence`  by @ydshieh in #40593\r\n* Fix inexistent imports  by @cyyever in #40580\r\n* Add Copilot instructions  by @Rocketknight1 in #40432\r\n* Fix `siglip` flaky `test_eager_matches_sdpa_inference`  by @ydshieh in #40584\r\n* Fix for missing default values in encoder decoder   by @remi-or in #40517\r\n* Fix quite a lot of FA tests  by @Cyrilvallez in #40548\r\n* [`Tests`] Fixup duplicated mrope logic  by @vasqu in #40592\r\n* Reduce more test data fetch  by @ydshieh in #40595\r\n* Pin torchcodec to 0.5 in AMD docker  by @remi-or in #40598\r\n* Multiple fixes to FA tests in AMD  by @remi-or in #40498\r\n* Disable cache for `TokenizerTesterMixin` temporarily  by @ydshieh in #40611\r\n* fix: continuous batching in `transformers serve`  by @McPatate in #40479\r\n* Fix processor chat template  by @zucchini-nlp in #40613\r\n* Avoid `too many request` caused by `AutoModelTest::test_dynamic_saving_from_local_repo`  by @ydshieh in #40614\r\n* Fix flaky `JambaModelTest.test_load_balancing_loss`  by @ydshieh in #40617\r\n* Add collated reports job to Nvidia CI  by @ahadnagy in #40470\r\n* Remove unnecessary pillow version check  by @cyyever in #40604\r\n* Fix invalid typing  by @cyyever in #40612\r\n* Enable more ruff UP rules  by @cyyever in #40579\r\n* Support TF32 flag for MUSA backend  by @fmo-mt in #33187\r\n* Remove random flag  by @Cyrilvallez in #40629\r\n* ğŸŒ [i18n-KO] Translated `deepseek_v3.md` to Korean   by @ssum21 in #39649\r\n* Fix `too many requests` in `TestMistralCommonTokenizer`  by @ydshieh in #40623\r\n* fix: gas for gemma fixed  by @yevvonlim in #40591\r\n* [auto-model] propagate kwargs  by @zucchini-nlp in #40491\r\n* [CP] Add attention_mask to the buffer when the mask is causal   by @kashif in #40619\r\n* Fix: PIL image load in Processing utils apply_chat_template  by @abdokaseb in #40622\r\n* Skip `test_prompt_lookup_decoding_matches_greedy_search` for `voxtral`  by @ydshieh in #40643\r\n* add DeepseekV3ForTokenClassification  by @bzantium in #40641\r\n* fix MetaCLIP 2 wrong link & wrong model names in the docstrings  by @voidism in #40565\r\n* Remove TF/Flax examples  by @Rocketknight1 in #40654\r\n* Mark `LongformerModelTest::test_attention_outputs` as flaky  by @ydshieh in #40655\r\n* fix pipeline dtype  by @jiqing-feng in #40638\r\n* feat(serving): add healthcheck  by @McPatate in #40653\r\n* Fix Metaclip modular conversion  by @Rocketknight1 in #40660\r\n* Avoid attention_mask copy in qwen2.5  by @cyyever in #40658\r\n* Allow custom args in `custom_generate` Callables and unify generation args structure  by @manueldeprada in #40586\r\n* Update `check_determinism` inside `test_determinism`  by @ydshieh in #40661\r\n* Skip `test_fast_is_faster_than_slow` for `Owlv2ImageProcessingTest`  by @ydshieh in #40663\r\n* Fix warning for output_attentions=True  by @qubvel in #40597\r\n* Skip `test_prompt_lookup_decoding_matches_greedy_search` for `qwen2_audio`  by @ydshieh in #40664\r\n* Remove overwritten `GitModelTest::test_beam_search_generate`  by @ydshieh in #40666\r\n* refactor: use `tolist` instead of list comprehension calling `.item()`  by @McPatate in #40646\r\n* Benchmarking V2: framework impl  by @ahadnagy in #40486\r\n* Even more test data cached  by @ydshieh in #40636\r\n* Skip more fast v.s slow image processor tests  by @ydshieh in #40675\r\n* Avoid night torch CI not run because of irrelevant docker image failing to build   by @ydshieh in #40677\r\n* Mark `Aimv2ModelTest::test_eager_matches_sdpa_inference_04_fp16_pad_right_sdpa_kernels` as flaky  by @ydshieh in #40683\r\n* CircleCI docker images cleanup / update / fix  by @ydshieh in #40681\r\n* Add sequence classification support for small Gemma 3 text models  by @abdokaseb in #40562\r\n* Add codebook_dim attribute to DacVectorQuantize for DacResidualVectorQuantize.from_latents()  by @flavioialongo in #40665\r\n* fix broken offline mode when loading tokenizer from hub  by @winglian in #40669\r\n* Load a tiny video to make CI faster  by @zucchini-nlp in #40684\r\n* Final test data cache - inside CI docker images  by @ydshieh in #40689\r\n* add: embedding model  by @RyanMullins in #40694\r\n* feat: support request cancellation  by @McPatate in #40599\r\n* Fixing bug in Voxtral when merging text and audio embeddings  by @rcogill in #40671\r\n* Change docker image to preview for the MI355 CI  by @ahadnagy in #40693\r\n* Fix backward compatibility with accelerate in Trainer  by @qgallouedec in #40668\r\n* Fix self.dropout_p is not defined for SamAttention/Sam2Attention  by @yonigozlan in #40667\r\n* [Glm4.5V] fix vLLM support  by @zucchini-nlp in #40696\r\n* Fix broken Llama4 accuracy in MoE part  by @nvpohanh in #40609\r\n* Avoid `T5GemmaModelTest::test_eager_matches_sdpa_inference` being flaky  by @ydshieh in #40702\r\n* Align assisted generate for unified signature in decoding methods  by @manueldeprada in #40657\r\n* Fetch one missing test data  by @ydshieh in #40703\r\n* Add Fast Image Processor for ImageGPT  by @agamjots05 in #39592\r\n* Fetch more test data with `hf_hub_download`  by @ydshieh in #40710\r\n* feat(serve): add healthcheck test  by @McPatate in #40697\r\n* Fix parent classes of ProcessingKwargs  by @cyyever in #40676\r\n* [tests] fix blip2 edge case  by @gante in #40699\r\n* [moduar] Add missing `self` in post-process methods  by @framonmar7 in #40711\r\n* [onnx] use logical `or` for grounding dino mask  by @lmarshall12 in #40625\r\n* Fix parent classes of AllKwargsForChatTemplate  by @cyyever in #40685\r\n* Fix arguments  by @cyyever in #40605\r\n* [serve] re-enable tests  by @gante in #40717\r\n* [tests] remove overwrites of removed test  by @gante in #40720\r\n* Add Optional typing  by @cyyever in #40686\r\n* [`Gemma Embedding`] Fix SWA  by @vasqu in #40700\r\n* Keypoint matching docs  by @merveenoyan in #40541\r\n* Skip `VitMatteImageProcessingTest::test_fast_is_faster_than_slow`  by @ydshieh in #40713\r\n* refactor(serve): move `request_id` to headers  by @McPatate in #40722\r\n* [Continous Batching] fix do_Sample=True in continuous batching  by @kashif in #40692\r\n* Fix order of mask functions when using `and/or_mask_function`  by @Cyrilvallez in #40753\r\n* Fix np array typing  by @cyyever in #40741\r\n* Set accepts_loss_kwargs to False for ConvNext(|V2)ForImageClassification  by @clinty in #40746\r\n* Add BF16 support check for MUSA backend  by @fmo-mt in #40576\r\n* remove gemmas eager training warning  by @August-murr in #40744\r\n* remove FSDP prefix when using save_pretrained with FSDP2  by @winglian in #40207\r\n* feat: err when unsupported attn impl is set w/ `--continuous_batching`  by @McPatate in #40618\r\n* docs: add continuous batching to serving  by @McPatate in #40758\r\n* Remove unnecessary tildes from documentation  by @st81 in #40748\r\n* Fix more typos  by @cyyever in #40627\r\n* Fix inconsistency in SeamlessM4T and SeamlessM4Tv2 docs  by @clinty in #39364\r\n* Fix `continue_final_message` in `apply_chat_template` to prevent substring matching issues  by @abdokaseb in #40732\r\n* ğŸŒ [i18n-KO] Translated 'xclip.md' to Korean  by @ssum21 in #39594\r\n* Fix Bark failing tests  by @ebezzam in #39478\r\n* Add EfficientLoFTRImageProcessorFast for GPU-accelerated image processing  by @LawJarp-A in #40215\r\n* Fix: swanlab `public.cloud.experiment_url` api error  by @Zeyi-Lin in #40763\r\n* [generate] `PromptLookupCandidateGenerator` won't generate forbidden tokens  by @gante in #40726\r\n* Support sliding window in CB  by @remi-or in #40688\r\n* [deprecations] Remove generate-related deprecations up to v4.56  by @gante in #40729\r\n* rm src/transformers/convert_pytorch_checkpoint_to_tf2.py  by @gante in #40718\r\n* [tests] update `test_past_key_values_format` and delete overwrites  by @gante in #40701\r\n* [RoPE] run RoPE tests when the model uses RoPE  by @gante in #40630\r\n* Fix crash when executing MambaCache sample code  by @torotoki in #40557\r\n* [pipeline] ASR pipeline kwargs are forwared to `generate`  by @gante in #40375\r\n* [docs] CPU install  by @stevhliu in #40631\r\n* Adding Support for Qwen3-Next  by @bozheng-hit in #40771\r\n* Fix gpt-oss router_indices in EP  by @jiqing-feng in #40545\r\n* Remove reference of video_load_backend and video_fps for processor  by @cyyever in #40719\r\n* [processors] Unbloating simple processors  by @zucchini-nlp in #40377\r\n* Enable ruff on benchmark and scripts  by @cyyever in #40634\r\n* Fix doc for PerceptionLMForConditionalGeneration forward.  by @shuminghu in #40733\r\n* Fix typos in tests and util  by @cyyever in #40780\r\n* Fix invalid PipelineParallel member  by @cyyever in #40789\r\n* Use functools.cached_property  by @cyyever in #40607\r\n* Read config pattern for Qwen3Next  by @Cyrilvallez in #40792\r\n* Fix dotted model names  by @August-murr in #40745\r\n* Fix the issue that csm model cannot work with pipeline mode.  by @yuanwu2017 in #39349\r\n* Move num_items_in_batch to correct device before accelerator.gather  by @ssharpe42 in #40773\r\n* Remove use_ipex option from Trainer  by @cyyever in #40784\r\n* fix_image_processing_fast_for_glm4v  by @lambertwjh in #40483\r\n* [Docs] Add missing class documentation for optimizer_schedules  by @jijihuny in #31870,  #23010) \r\n* Fix DeepSpeed mixed precision precedence over Accelerate defaults  by @notkisk in #39856\r\n* feature: Add robust token counting with padding exclusion   by @PrathmeshAdsod in #40416\r\n* Fix edge case for tokenize  by @wangzhen0518 in #36277) \r\n* Fix config dtype parsing for Emu3 edge case  by @Isotr0py in #40766\r\n* Align torch implementation of Gated DeltaNet in Qwen3-Next with fla library.  by @bozheng-hit in #40807\r\n* Fix typos in src  by @cyyever in #40782\r\n* add general hub test for Fast Image Processors in test_image_processing_utils  by @namgyu-youn in #40086\r\n* Push generation config along with checkpoints  by @qgallouedec in #40804\r\n* [`Jetmoe`] Fix RoPE  by @vasqu in #40819\r\n* ğŸŒ [i18n-KO] Translated clipseg.md to Korean  by @HyunZ118 in #39903\r\n* Improve torch_dtype checks  by @cyyever in #40808\r\n* Add VideoProcessors to auto-backend requirements  by @Cyrilvallez in #40843\r\n* Adds Causal Conv 1D kernel for mamba models  by @MekkCyber in #40765\r\n* Update no split modules in T5Gemma model  by @npuichigo in #40810\r\n* Replace image classification loss functions to `self.loss_function`  by @qubvel in #40764\r\n* Fix the misalignment between the l2norm in GDN of Qwen3-Next and the implementation in the FLA library.  by @bozheng-hit in #40842\r\n* Fixes for continuous batching  by @remi-or in #40828\r\n* [tests] re-enable aria fast tests  by @gante in #40846\r\n* [SAM2] Fix inconsistent results with original implementation with input boxes  by @yonigozlan in #40800\r\n* [Sam2Video] Fix video inference with batched boxes and add test  by @yonigozlan in #40797\r\n* add: differential privacy research model  by @RyanMullins in #40851\r\n* [test] Fix test_eager_matches_sdpa incorrectly skipped  by @eustlb in #40852\r\n* [tests] move generative tests away from `test_modeling_common.py`  by @gante in #40854\r\n* [generate] Always use decoder config to init cache  by @gante in #40772\r\n* Use checkpoint in auto_class_docstring  by @cyyever in #40844\r\n* Fix TrainingArguments.parallelism_config NameError with accelerate<1.10.1  by @albertvillanova in #40818\r\n* Redirect MI355 CI results to dummy dataset  by @ahadnagy in #40862\r\n* [Bug fix #40813] Fix base_model_tp_plan of Starcoder2 model.  by @greg-kwasniewski1 in #40814\r\n* [docstrings / type hints] Update outdated annotations for `past_key_values`   by @gante in #40803\r\n* fix florence kwargs   by @SunMarc in #40826\r\n* fix: XIELU act parameters not being casted to correct dtype  by @NanoCode012 in #40812\r\n* Update model tags and integration references in bug report  by @ArthurZucker in #40881\r\n* [Qwen3 Next] Use numerically stable `rsqrt`  by @thalahors in #40848\r\n* Adding Support for Qwen3-VL Series  by @JJJYmmm in #40795\r\n* [`VaultGemma`] Update expectations in integration tests  by @vasqu in #40855\r\n* Fix modular consistency  by @Cyrilvallez in #40883\r\n* Clarify passing is_causal in sdpa_attention_paged_forward  by @cyyever in #40838\r\n* Use torch.expm1 and torch.log1p for better numerical results  by @cyyever in #40860\r\n* Add Fast PromptDepthAnything Processor  by @SamuelBarryCS in #40602\r\n* Fix deta loading & dataclass  by @Cyrilvallez in #40878\r\n* Remove dict branch of attention_mask in sdpa_attention_paged_forward  by @cyyever in #40882\r\n* ğŸŒ [i18n-KO] Translated smolvlm.md to Korean  by @HyunZ118 in #40414\r\n* ğŸŒ [i18n-KO] Translated `imageprocessor.md` to Korean  by @HyunZ118 in #39557\r\n* [generate] remove docs of a feature that no longer exists  by @gante in #40895\r\n* Make debugging failing tests (check and update expect output values) easier ğŸ”¥   by @ydshieh in #40727\r\n* Fixing the call to kernelize  by @MekkCyber in #40628\r\n* Fix getter  regression  by @molbap in #40824\r\n* Fix flaky `Gemma3nAudioFeatureExtractionTest::test_dither`  by @ydshieh in #40902\r\n* [cache] Merge static sliding and static chunked layer  by @Cyrilvallez in #40893\r\n* Harmonize CacheLayer names  by @Cyrilvallez in #40892\r\n* [cache] Only use scalars in `get_mask_sizes`  by @Cyrilvallez in #40907\r\n* Set seed for `Glm4vIntegrationTest`  by @ydshieh in #40905\r\n* Add Olmo3 model  by @2015aroras in #40778\r\n* remove dummy EncodingFast  by @cyyever in #40864\r\n* Improve module name handling for local custom code  by @XuehaiPan in #40809\r\n* Remove `runner_map`  by @ydshieh in #40880\r\n* disable `test_fast_is_faster_than_slow`  by @ydshieh in #40909\r\n* [gemma3] `Gemma3ForConditionalGeneration` compatible with assisted generation  by @gante in #40791\r\n* [generate] misc fixes  by @gante in #40906\r\n* Fix dtype in Paligemma  by @zucchini-nlp in #40912\r\n* [Docs] Adding documentation of MXFP4 Quantization  by @ariG23498 in #40885\r\n* Processor load with multi-processing  by @zucchini-nlp in #40786\r\n* [Llama4] Remove `image_sizes` arg and deprecate `vision_feature_layer`  by @yaswanth19 in #40832\r\n* Fix #40067: Add dedicated UMT5 support to GGUF loader (config, tokenizer, test)  by @akshay-babbar in #40218\r\n* [torchao safetensors] renaming get_state_dict function  by @liangel-02 in #40774\r\n* Adding activation kernels  by @MekkCyber in #40890\r\n* Minor fix for #40727  by @ydshieh in #40929\r\n* Add support for Florence-2 training  by @ducviet00 in #40914\r\n* Add LongCat-Flash  by @molbap in #40730\r\n* [DOC] Add missing dates in model cards  by @yonigozlan in #40922\r\n* [models] remove unused `import torch.utils.checkpoint`   by @gante in #40934\r\n* Intel CPU dockerfile  by @jiqing-feng in #40806\r\n* docs(i18n): Correct the descriptive text in the README_zh-hans.md  by @lilin-1 in #40941\r\n* Fix trainer tests  by @SunMarc in #40823\r\n* Fix `Glm4vMoeIntegrationTest`  by @ydshieh in #40930\r\n* Raise error instead of warning when using meta device in from_pretrained  by @Cyrilvallez in #40942\r\n* Consistent naming for images kwargs  by @zucchini-nlp in #40834\r\n* Remove nested import logic for torchvision  by @yonigozlan in #40940\r\n* Fix `Glm4vModelTest::test_eager_matches_fa2_generate`  by @ydshieh in #40947\r\n* Update expected values for some `test_speculative_generation`  by @ydshieh in #40949\r\n* Standardize audio embedding function name for audio multimodal models  by @jackzhxng in #40919\r\n* Add FlexOlmo model  by @2015aroras in #40921\r\n* Don't list dropout in eager_paged_attention_forward  by @cyyever in #40924\r\n\r\n## Significant community contributions\r\n\r\nThe following contributors have made significant changes to the library over the last release:\r\n\r\n* @hiyouga\r\n    * Support batch size > 1 image-text inference (#36682)\r\n* @cyyever\r\n    * Fix typos (#40585)\r\n    * Fix inexistent imports (#40580)\r\n    * Remove unnecessary pillow version check (#40604)\r\n    * Fix invalid typing (#40612)\r\n    * Enable more ruff UP rules (#40579)\r\n    * Avoid attention_mask copy in qwen2.5 (#40658)\r\n    * Fix parent classes of ProcessingKwargs (#40676)\r\n    * Fix parent classes of AllKwargsForChatTemplate (#40685)\r\n    * Fix arguments (#40605)\r\n    * Add Optional typing (#40686)\r\n    * Fix np array typing (#40741)\r\n    * Fix more typos (#40627)\r\n    * Remove reference of video_load_backend and video_fps for processor (#40719)\r\n    * Enable ruff on benchmark and scripts (#40634)\r\n    * Fix typos in tests and util (#40780)\r\n    * Fix invalid PipelineParallel member (#40789)\r\n    * Use functools.cached_property (#40607)\r\n    * Remove use_ipex option from Trainer (#40784)\r\n    * Fix typos in src (#40782)\r\n    * Improve torch_dtype checks (#40808)\r\n    * Use checkpoint in auto_class_docstring (#40844)\r\n    * Clarify passing is_causal in sdpa_attention_paged_forward (#40838)\r\n    * Use torch.expm1 and torch.log1p for better numerical results (#40860)\r\n    * Remove dict branch of attention_mask in sdpa_attention_paged_forward (#40882)\r\n    * remove dummy EncodingFast (#40864)\r\n    * Don't list dropout in eager_paged_attention_forward (#40924)\r\n    * Benchmarking V2: framework impl (#40486)\r\n    * Change docker image to preview for the MI355 CI (#40693)\r\n    * Redirect MI355 CI results to dummy dataset (#40862)\r\n* @voidism\r\n    * fix MetaCLIP 2 wrong link & wrong model names in the docstrings (#40565)\r\n* @RyanMullins\r\n    * add: embedding model (#40694)\r\n    * add: differential privacy research model (#40851)\r\n* @LawJarp-A\r\n    * Add EfficientLoFTRImageProcessorFast for GPU-accelerated image processing (#40215)\r\n* @bozheng-hit\r\n    * Adding Support for Qwen3-Next (#40771)\r\n    * Align torch implementation of Gated DeltaNet in Qwen3-Next with fla library. (#40807)\r\n    * Fix the misalignment between the l2norm in GDN of Qwen3-Next and the implementation in the FLA library. (#40842)\r\n* @wangzhen0518\r\n    * Fix edge case for tokenize (#36277) (#36555)\r\n* @HyunZ118\r\n    * ğŸŒ [i18n-KO] Translated clipseg.md to Korean (#39903)\r\n    * ğŸŒ [i18n-KO] Translated smolvlm.md to Korean (#40414)\r\n    * ğŸŒ [i18n-KO] Translated `imageprocessor.md` to Korean (#39557)\r\n* @JJJYmmm\r\n    * Adding Support for Qwen3-VL Series (#40795)\r\n* @SamuelBarryCS\r\n    * Add Fast PromptDepthAnything Processor (#40602)\r\n* @2015aroras\r\n    * Add Olmo3 model (#40778)\r\n    * Add FlexOlmo model (#40921)",
      "publishedAt": "2025-10-03T17:04:51.000Z",
      "source": "Hugging Face GitHub",
      "sourceUrl": "https://github.com/huggingface/transformers/releases/tag/v4.57.0",
      "category": "tools",
      "company": "Hugging Face",
      "imageUrl": null,
      "tags": [
        "GPT",
        "Llama",
        "LLM",
        "AI",
        "ML"
      ],
      "featured": false
    },
    {
      "id": "mhlm92wis100zdoiipn",
      "title": "Show HN: Pantheon-CLI â€“ Open-Source Python Claude Code and Smart Notebook",
      "summary": "Hi, Weâ€™ve built Pantheon-CLI, an fully open-source project that aims to be the â€œPython Claude Code + Notebookâ€ â€” but designed for data analysis instead of just coding.<p>Unlike most AI coding assistants, Pantheon-CLI runs entirely on your machine (or server). No data upload required. It blends natur...",
      "content": "Hi, Weâ€™ve built Pantheon-CLI, an fully open-source project that aims to be the â€œPython Claude Code + Notebookâ€ â€” but designed for data analysis instead of just coding.<p>Unlike most AI coding assistants, Pantheon-CLI runs entirely on your machine (or server). No data upload required. It blends natural language and code in a single workflow, keeping variables in memory and letting you switch seamlessly between typing code and asking in plain English.<p>What it does:\n1. Chat with your data: Directly process CSV, Excel, AnnData, Pickle, Torch tensors, or any format supported by Python&#x2F;R&#x2F;Julia.\n2. Mixed programming: Variables persist across natural language and code; the CLI auto-generates and runs code for you.\n3. MCP-like agent integration: Read&#x2F;create files, run commands, fetch web pages, generate&#x2F;revise code.\n4. Human-like learning: Feed it a PDF paper or tutorialâ€”Pantheon-CLI reads it, plans steps, and replicates methods before analysis.\n5. Task planning: Builds scientific agents by learning from papers&#x2F;tutorials (not just fixed, human-predefined steps).\n6. Multi-model support: Works with OpenAI, Anthropic, Gemini, DeepSeek, Qwen, etc. + offline local LLMs (ollama, deepseek, gpt-oss).\n7. Multi-RAG support: Pre-learns from docs&#x2F;web into a local â€œbrainâ€ for more credible outputs without massive token costs.\n8. Built-in biology toolsets: For omics analysis (alignment, annotation, differential expression, full paper reproduction).\n9. Notebook mode: Brings the same agentic workflow into Jupyterâ€”automatically runs and revises code, operates on files, and learns from tutorials&#x2F;papers.<p>Pantheon-CLI is our attempt to push beyond â€œAI writes code for you.â€ Instead, itâ€™s an agentic operating system for data analysis, spanning both terminal and notebook.<p>Repo: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;aristoteleo&#x2F;pantheon-cli\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;aristoteleo&#x2F;pantheon-cli</a><p>Tutorial: <a href=\"https:&#x2F;&#x2F;pantheonos.stanford.edu&#x2F;cli&#x2F;docs&#x2F;intro&#x2F;getting-started\" rel=\"nofollow\">https:&#x2F;&#x2F;pantheonos.stanford.edu&#x2F;cli&#x2F;docs&#x2F;intro&#x2F;getting-start...</a><p>Home page: <a href=\"https:&#x2F;&#x2F;pantheonos.stanford.edu&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;pantheonos.stanford.edu&#x2F;</a><p>Would love to hear feedback from the HN communityâ€”what use cases would you try this for, and what features would make it more useful to you?",
      "publishedAt": "2025-08-26T16:58:33.000Z",
      "source": "Hacker News AI",
      "sourceUrl": "https://github.com/aristoteleo/pantheon-cli",
      "category": "industry",
      "company": "Community",
      "imageUrl": null,
      "tags": [
        "GPT",
        "Claude",
        "Gemini",
        "Llama",
        "LLM"
      ],
      "featured": false
    }
  ],
  "featuredCount": 14
}