{
  "lastUpdated": "2025-06-26T01:22:06.657Z",
  "totalArticles": 26,
  "articles": [
    {
      "id": "mccp712785o0083q9",
      "title": "AlphaGenome: AI for better understanding the genome",
      "summary": "Introducing a new, unifying DNA sequence model that advances regulatory variant-effect prediction and promises to shed new light on genome function ‚Äî now available via API.",
      "content": "Introducing a new, unifying DNA sequence model that advances regulatory variant-effect prediction and promises to shed new light on genome function ‚Äî now available via API.",
      "publishedAt": "2025-06-25T13:59:51.000Z",
      "source": "DeepMind Blog",
      "sourceUrl": "https://deepmind.google/discover/blog/alphagenome-ai-for-better-understanding-the-genome/",
      "category": "research",
      "company": "Google",
      "imageUrl": null,
      "tags": [
        "AI"
      ],
      "featured": false
    },
    {
      "id": "mccp771gyg7v2zcizdg",
      "title": "Hugging Face: Kyutai-STT (based on v4.52.4)",
      "summary": "A new model is added to transformers: Kyutai-STT\r\nIt is added on top of the v4.52.4 release, and can be installed from the following tag: `v4.52.4-Kyutai-STT-preview`.\r\n\r\nIn order to install this version, please install with the following command:\r\n\r\n```\r\npip install git+https://github.com/huggingfa...",
      "content": "A new model is added to transformers: Kyutai-STT\r\nIt is added on top of the v4.52.4 release, and can be installed from the following tag: `v4.52.4-Kyutai-STT-preview`.\r\n\r\nIn order to install this version, please install with the following command:\r\n\r\n```\r\npip install git+https://github.com/huggingface/transformers@v4.52.4-Kyutai-STT-preview\r\n```\r\n\r\nIf fixes are needed, they will be applied to this release; this installation may therefore be considered as stable and improving.\r\n\r\nAs the tag implies, this tag is a **_preview_** of the Kyutai-STT model. This tag is a tagged version of the `main` branch and does not follow semantic versioning. This model will be included in the next minor release: `v4.53.0`.\r\n\r\n## Kyutai-STT\r\n\r\n<img src=\"https://huggingface.co/datasets/eustlb/documentation-images/resolve/main/kyutai_stt.png\"/>\r\n\r\nKyutai STT is a speech-to-text model architecture based on the [Mimi codec](https://huggingface.co/docs/transformers/en/model_doc/mimi), which encodes audio into discrete tokens in a streaming fashion, and a [Moshi-like](https://huggingface.co/docs/transformers/en/model_doc/moshi) autoregressive decoder. Kyutai‚Äôs lab has released two model checkpoints:\r\n- [kyutai/stt-1b-en_fr](https://huggingface.co/kyutai/stt-1b-en_fr): a 1B-parameter model capable of transcribing both English and French\r\n- [kyutai/stt-2.6b-en](https://huggingface.co/kyutai/stt-2.6b-en): a 2.6B-parameter model focused solely on English, optimized for maximum transcription accuracy\r\n\r\n\r\n## Usage example\r\n\r\nKyutai-STT can be found on the [Huggingface Hub](https://huggingface.co/models?other=stt).\r\n\r\n### Inference\r\n\r\n```python\r\nimport torch\r\nfrom datasets import load_dataset, Audio\r\nfrom transformers import KyutaiSpeechToTextProcessor, KyutaiSpeechToTextForConditionalGeneration\r\n\r\n# 1. load the model and the processor\r\ntorch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\r\nmodel_id = \"kyutai/stt-2.6b-en\"\r\n\r\nprocessor = KyutaiSpeechToTextProcessor.from_pretrained(model_id)\r\nmodel = KyutaiSpeechToTextForConditionalGeneration.from_pretrained(model_id, device_map=torch_device)\r\n\r\n# 2. load audio samples\r\nds = load_dataset(\r\n    \"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\"\r\n)\r\nds = ds.cast_column(\"audio\", Audio(sampling_rate=24000))\r\n\r\n# 3. prepare the model inputs\r\ninputs = processor(\r\n    ds[0][\"audio\"][\"array\"],\r\n)\r\ninputs.to(torch_device)\r\n\r\n# 4. infer the model\r\noutput_tokens = model.generate(**inputs)\r\n\r\n# 5. decode the generated tokens\r\nprint(processor.batch_decode(output_tokens, skip_special_tokens=True))\r\n```\r\n\r\n### Batched Inference\r\n\r\n```python\r\nimport torch\r\nfrom datasets import load_dataset, Audio\r\nfrom transformers import KyutaiSpeechToTextProcessor, KyutaiSpeechToTextForConditionalGeneration\r\n\r\n# 1. load the model and the processor\r\ntorch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\r\nmodel_id = \"kyutai/stt-2.6b-en\"\r\n\r\nprocessor = KyutaiSpeechToTextProcessor.from_pretrained(model_id)\r\nmodel = KyutaiSpeechToTextForConditionalGeneration.from_pretrained(model_id, device_map=torch_device)\r\n\r\n# 2. load audio samples\r\nds = load_dataset(\r\n    \"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\"\r\n)\r\nds = ds.cast_column(\"audio\", Audio(sampling_rate=24000))\r\n\r\n# 3. prepare the model inputs\r\naudio_arrays = [ds[i][\"audio\"][\"array\"] for i in range(4)]\r\ninputs = processor(audio_arrays, return_tensors=\"pt\", padding=True)\r\ninputs = inputs.to(torch_device)\r\n\r\n# 4. infer the model\r\noutput_tokens = model.generate(**inputs)\r\n\r\n# 5. decode the generated tokens\r\ndecoded_outputs = processor.batch_decode(output_tokens, skip_special_tokens=True)\r\nfor output in decoded_outputs:\r\n    print(output)\r\n```",
      "publishedAt": "2025-06-24T16:05:00.000Z",
      "source": "Hugging Face GitHub",
      "sourceUrl": "https://github.com/huggingface/transformers/releases/tag/v4.52.4-Kyutai-STT-preview",
      "category": "tools",
      "company": "Hugging Face",
      "imageUrl": null,
      "tags": [
        "AI",
        "Transformer"
      ],
      "featured": false
    },
    {
      "id": "mccp712727oli9ab6bh",
      "title": "Gemini Robotics On-Device brings AI to local robotic devices",
      "summary": "We‚Äôre introducing an efficient, on-device robotics model with general-purpose dexterity and fast task adaptation.",
      "content": "We‚Äôre introducing an efficient, on-device robotics model with general-purpose dexterity and fast task adaptation.",
      "publishedAt": "2025-06-24T14:00:36.000Z",
      "source": "DeepMind Blog",
      "sourceUrl": "https://deepmind.google/discover/blog/gemini-robotics-on-device-brings-ai-to-local-robotic-devices/",
      "category": "research",
      "company": "Google",
      "imageUrl": null,
      "tags": [
        "Gemini",
        "AI"
      ],
      "featured": false
    },
    {
      "id": "mccp6vgth15wx2vj2qh",
      "title": "Driving scalable growth with OpenAI o3, GPT-4.1, and CUA",
      "summary": "Unify, an AI-powered GTM platform, uses OpenAI‚Äôs o3, GPT-4.1, and CUA to automate prospecting, research, and outreach. With hyper-personalized messaging and an always-on workflow, Unify helps teams generate pipeline at scale while focusing on high-impact customer interactions.",
      "content": "Unify, an AI-powered GTM platform, uses OpenAI‚Äôs o3, GPT-4.1, and CUA to automate prospecting, research, and outreach. With hyper-personalized messaging and an always-on workflow, Unify helps teams generate pipeline at scale while focusing on high-impact customer interactions.",
      "publishedAt": "2025-06-24T00:00:00.000Z",
      "source": "OpenAI Blog",
      "sourceUrl": "https://openai.com/index/unify",
      "category": "companies",
      "company": "OpenAI",
      "imageUrl": null,
      "tags": [
        "GPT",
        "AI",
        "OpenAI"
      ],
      "featured": true
    },
    {
      "id": "mccp721ha43ewdivpwv",
      "title": "Anthropic: v0.55.0",
      "summary": "## 0.55.0 (2025-06-23)\n\nFull Changelog: [v0.54.0...v0.55.0](https://github.com/anthropics/anthropic-sdk-python/compare/v0.54.0...v0.55.0)\n\n### Features\n\n* **api:** api update ([4b2134e](https://github.com/anthropics/anthropic-sdk-python/commit/4b2134e5ec3fecab7c56f483b8db87b403a08e05))\n* **api:** ap...",
      "content": "## 0.55.0 (2025-06-23)\n\nFull Changelog: [v0.54.0...v0.55.0](https://github.com/anthropics/anthropic-sdk-python/compare/v0.54.0...v0.55.0)\n\n### Features\n\n* **api:** api update ([4b2134e](https://github.com/anthropics/anthropic-sdk-python/commit/4b2134e5ec3fecab7c56f483b8db87b403a08e05))\n* **api:** api update ([2093bff](https://github.com/anthropics/anthropic-sdk-python/commit/2093bfff2a6c25573eaa2a4667f1e1d0e2d89e24))\n* **api:** manual updates ([c80fda8](https://github.com/anthropics/anthropic-sdk-python/commit/c80fda8cbd157fbbd23895d034cc7bb7a7614569))\n* **client:** add support for aiohttp ([3b03295](https://github.com/anthropics/anthropic-sdk-python/commit/3b03295f15a02ba629d1bdc77e330c2e6043b83e))\n\n\n### Bug Fixes\n\n* **client:** correctly parse binary response | stream ([d93817d](https://github.com/anthropics/anthropic-sdk-python/commit/d93817d9d761bd5e16b35f3c2973122a9c122240))\n* **internal:** revert unintentional changes ([bb3beab](https://github.com/anthropics/anthropic-sdk-python/commit/bb3beab10668be177d6bb573607ef6951a238b24))\n* **tests:** fix: tests which call HTTP endpoints directly with the example parameters ([ee69d74](https://github.com/anthropics/anthropic-sdk-python/commit/ee69d74cc40f749280a29afb12420c117d08ef34))\n* **tests:** suppress warnings in tests when running on the latest Python versions ([#982](https://github.com/anthropics/anthropic-sdk-python/issues/982)) ([740da21](https://github.com/anthropics/anthropic-sdk-python/commit/740da21b563c6ffe7618edf1dcd658bb894b2edf))\n\n\n### Chores\n\n* **ci:** enable for pull requests ([08f2dd2](https://github.com/anthropics/anthropic-sdk-python/commit/08f2dd2bd28958c08a3c82fcf00a0fc7d4e2807c))\n* **internal:** update conftest.py ([1174a62](https://github.com/anthropics/anthropic-sdk-python/commit/1174a6214624ff8cd64edb121d4ff09e9af6b717))\n* **internal:** version bump ([7241eaa](https://github.com/anthropics/anthropic-sdk-python/commit/7241eaa25b6f40bb55f61e766a996a3a18a53a02))\n* **readme:** update badges ([00661c2](https://github.com/anthropics/anthropic-sdk-python/commit/00661c275e120314f76bbd480c0267383e992638))\n* **tests:** add tests for httpx client instantiation & proxies ([b831d88](https://github.com/anthropics/anthropic-sdk-python/commit/b831d8833010c629143041b4b385929ca9c2198d))\n* **tests:** run tests in parallel ([4b24a79](https://github.com/anthropics/anthropic-sdk-python/commit/4b24a791b76c2176de1f35118901da533a10b991))\n\n\n### Documentation\n\n* **client:** fix httpx.Timeout documentation reference ([b0138b1](https://github.com/anthropics/anthropic-sdk-python/commit/b0138b1b2af3c73e568659c7e717fc955eb976b0))",
      "publishedAt": "2025-06-23T18:51:59.000Z",
      "source": "Anthropic GitHub",
      "sourceUrl": "https://github.com/anthropics/anthropic-sdk-python/releases/tag/v0.55.0",
      "category": "tools",
      "company": "Anthropic",
      "imageUrl": null,
      "tags": [
        "AI",
        "Anthropic"
      ],
      "featured": false
    },
    {
      "id": "mccp6wgcikcw85ux0k",
      "title": "Transformers backend integration in SGLang",
      "summary": "",
      "content": "",
      "publishedAt": "2025-06-23T00:00:00.000Z",
      "source": "Hugging Face Blog",
      "sourceUrl": "https://huggingface.co/blog/transformers-backend-sglang",
      "category": "tools",
      "company": "Hugging Face",
      "imageUrl": null,
      "tags": [
        "Transformer"
      ],
      "featured": false
    },
    {
      "id": "mccp6wgdeq44aa9hkgp",
      "title": "(LoRA) Fine-Tuning FLUX.1-dev on Consumer Hardware",
      "summary": "",
      "content": "",
      "publishedAt": "2025-06-19T00:00:00.000Z",
      "source": "Hugging Face Blog",
      "sourceUrl": "https://huggingface.co/blog/flux-qlora",
      "category": "tools",
      "company": "Hugging Face",
      "imageUrl": null,
      "tags": [],
      "featured": false
    },
    {
      "id": "mccp6vgtwtirqi3j57k",
      "title": "Preparing for future AI risks in biology",
      "summary": "Advanced AI can transform biology and medicine‚Äîbut also raises biosecurity risks. We‚Äôre proactively assessing capabilities and implementing safeguards to prevent misuse.",
      "content": "Advanced AI can transform biology and medicine‚Äîbut also raises biosecurity risks. We‚Äôre proactively assessing capabilities and implementing safeguards to prevent misuse.",
      "publishedAt": "2025-06-18T10:00:00.000Z",
      "source": "OpenAI Blog",
      "sourceUrl": "https://openai.com/index/preparing-for-future-ai-capabilities-in-biology",
      "category": "companies",
      "company": "OpenAI",
      "imageUrl": null,
      "tags": [
        "AI"
      ],
      "featured": true
    },
    {
      "id": "mccp6vgtwu0ixqr3d9g",
      "title": "Toward understanding and preventing misalignment generalization",
      "summary": "We study how training on incorrect responses can cause broader misalignment in language models and identify an internal feature driving this behavior‚Äîone that can be reversed with minimal fine-tuning.",
      "content": "We study how training on incorrect responses can cause broader misalignment in language models and identify an internal feature driving this behavior‚Äîone that can be reversed with minimal fine-tuning.",
      "publishedAt": "2025-06-18T10:00:00.000Z",
      "source": "OpenAI Blog",
      "sourceUrl": "https://openai.com/index/emergent-misalignment",
      "category": "companies",
      "company": "OpenAI",
      "imageUrl": null,
      "tags": [
        "AI"
      ],
      "featured": true
    },
    {
      "id": "mccp7127nbc9sugelc9",
      "title": "Gemini 2.5: Updates to our family of thinking models",
      "summary": "Explore the latest Gemini 2.5 model updates with enhanced performance and accuracy: Gemini 2.5 Pro now stable, Flash generally available, and the new Flash-Lite in preview.",
      "content": "Explore the latest Gemini 2.5 model updates with enhanced performance and accuracy: Gemini 2.5 Pro now stable, Flash generally available, and the new Flash-Lite in preview.",
      "publishedAt": "2025-06-17T16:03:39.000Z",
      "source": "DeepMind Blog",
      "sourceUrl": "https://deepmind.google/discover/blog/gemini-25-updates-to-our-family-of-thinking-models/",
      "category": "research",
      "company": "Google",
      "imageUrl": null,
      "tags": [
        "Gemini",
        "AI"
      ],
      "featured": false
    },
    {
      "id": "mccp7127xurbrkbee5s",
      "title": "We‚Äôre expanding our Gemini 2.5 family of models",
      "summary": "Gemini 2.5 Flash and Pro are now generally available, and we‚Äôre introducing 2.5 Flash-Lite, our most cost-efficient and fastest 2.5 model yet.",
      "content": "Gemini 2.5 Flash and Pro are now generally available, and we‚Äôre introducing 2.5 Flash-Lite, our most cost-efficient and fastest 2.5 model yet.",
      "publishedAt": "2025-06-17T16:01:00.000Z",
      "source": "DeepMind Blog",
      "sourceUrl": "https://deepmind.google/discover/blog/were-expanding-our-gemini-25-family-of-models/",
      "category": "research",
      "company": "Google",
      "imageUrl": null,
      "tags": [
        "Gemini",
        "AI"
      ],
      "featured": false
    },
    {
      "id": "mccp6vgtw2fzg2e0sv",
      "title": "Introducing OpenAI for Government",
      "summary": "We‚Äôre launching OpenAI for Government, a new initiative focused on bringing our most advanced AI tools to public servants across the United States. We're supporting the U.S. government's efforts in adopting best-in-class technology and deploying these tools in service of the public good.",
      "content": "We‚Äôre launching OpenAI for Government, a new initiative focused on bringing our most advanced AI tools to public servants across the United States. We're supporting the U.S. government's efforts in adopting best-in-class technology and deploying these tools in service of the public good.",
      "publishedAt": "2025-06-16T00:00:00.000Z",
      "source": "OpenAI Blog",
      "sourceUrl": "https://openai.com/global-affairs/introducing-openai-for-government",
      "category": "companies",
      "company": "OpenAI",
      "imageUrl": null,
      "tags": [
        "AI",
        "OpenAI"
      ],
      "featured": false
    },
    {
      "id": "mccp6wgdyfqms5aqbw",
      "title": "Groq on Hugging Face Inference Providers üî•",
      "summary": "",
      "content": "",
      "publishedAt": "2025-06-16T00:00:00.000Z",
      "source": "Hugging Face Blog",
      "sourceUrl": "https://huggingface.co/blog/inference-providers-groq",
      "category": "tools",
      "company": "Hugging Face",
      "imageUrl": null,
      "tags": [],
      "featured": false
    },
    {
      "id": "mccp7127rc36zl2522",
      "title": "Behind ‚ÄúANCESTRA‚Äù: combining Veo with live-action filmmaking",
      "summary": "We partnered with Darren Aronofsky, Eliza McNitt and a team of more than 200 people to make a film using Veo and live-action filmmaking.",
      "content": "We partnered with Darren Aronofsky, Eliza McNitt and a team of more than 200 people to make a film using Veo and live-action filmmaking.",
      "publishedAt": "2025-06-13T13:30:00.000Z",
      "source": "DeepMind Blog",
      "sourceUrl": "https://deepmind.google/discover/blog/behind-ancestra-combining-veo-with-live-action-filmmaking/",
      "category": "research",
      "company": "Google",
      "imageUrl": null,
      "tags": [],
      "featured": false
    },
    {
      "id": "mccp6vgt8hwxtys8oj",
      "title": "Bringing the Magic of AI to Mattel‚Äôs Iconic Brands",
      "summary": "OpenAI and Mattel are partnering to integrate AI into iconic brands such as Barbie and Hot Wheels, aiming to enhance creative development, streamline workflows, and create new ways for fans to engage.",
      "content": "OpenAI and Mattel are partnering to integrate AI into iconic brands such as Barbie and Hot Wheels, aiming to enhance creative development, streamline workflows, and create new ways for fans to engage.",
      "publishedAt": "2025-06-12T00:00:00.000Z",
      "source": "OpenAI Blog",
      "sourceUrl": "https://openai.com/index/mattels-iconic-brands",
      "category": "companies",
      "company": "OpenAI",
      "imageUrl": null,
      "tags": [
        "AI",
        "ML",
        "OpenAI"
      ],
      "featured": true
    },
    {
      "id": "mccp6wgdm588xlmxpw",
      "title": "Enhance Your Models in 5 Minutes with the Hugging Face Kernel Hub",
      "summary": "",
      "content": "",
      "publishedAt": "2025-06-12T00:00:00.000Z",
      "source": "Hugging Face Blog",
      "sourceUrl": "https://huggingface.co/blog/hello-hf-kernels",
      "category": "tools",
      "company": "Hugging Face",
      "imageUrl": null,
      "tags": [],
      "featured": false
    },
    {
      "id": "mccp6wgd451wyss1rp4",
      "title": "Featherless AI on Hugging Face Inference Providers üî•",
      "summary": "",
      "content": "",
      "publishedAt": "2025-06-12T00:00:00.000Z",
      "source": "Hugging Face Blog",
      "sourceUrl": "https://huggingface.co/blog/inference-providers-featherless",
      "category": "tools",
      "company": "Hugging Face",
      "imageUrl": null,
      "tags": [
        "AI"
      ],
      "featured": false
    },
    {
      "id": "mccp771guza7i06aoyi",
      "title": "Hugging Face: V-JEPA 2 (based on v4.52.4)",
      "summary": "A new model is added to transformers: V-JEPA 2\r\nIt is added on top of the v4.52.4 release, and can be installed from the following tag: `v4.52.4-VJEPA-2-preview`.\r\n\r\nIn order to install this version, please install with the following command:\r\n\r\n```\r\npip install git+https://github.com/huggingface/tr...",
      "content": "A new model is added to transformers: V-JEPA 2\r\nIt is added on top of the v4.52.4 release, and can be installed from the following tag: `v4.52.4-VJEPA-2-preview`.\r\n\r\nIn order to install this version, please install with the following command:\r\n\r\n```\r\npip install git+https://github.com/huggingface/transformers@v4.52.4-VJEPA-2-preview\r\n```\r\n\r\nIf fixes are needed, they will be applied to this release; this installation may therefore be considered as stable and improving.\r\n\r\nAs the tag implies, this tag is a **_preview_** of the VJEPA-2 model. This tag is a tagged version of the `main` branch and does not follow semantic versioning. This model will be included in the next minor release: `v4.53.0`.\r\n\r\n## VJEPA-2\r\n\r\n<div class=\"flex justify-center\">\r\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/vjepa.gif\" alt=\"drawing\" width=\"600\"/>\r\n</div>\r\n\r\nV-JEPA 2 is a self-supervised approach to training video encoders developed by FAIR, Meta. Using internet-scale video data, V-JEPA 2 attains state-of-the-art performance on motion understanding and human action anticipation tasks. V-JEPA 2-AC is a latent action-conditioned world model post-trained from V-JEPA 2 (using a small amount of robot trajectory interaction data) that solves robot manipulation tasks without environment-specific data collection or task-specific training or calibration.\r\n\r\nThe abstract from the technical report is the following:\r\n\r\n\r\n## Usage example\r\n\r\nVJEPA-2 can be found on the [Huggingface Hub](https://huggingface.co/models?other=vjepa2). V-JEPA 2 is intended to represent any video (and image) to perform video classification, retrieval, or as a video encoder for VLMs.\r\n\r\nThe snippet below shows how to load the V-JEPA 2 model using the `AutoModel` class.\r\n\r\n```py\r\nimport torch\r\nfrom torchcodec.decoders import VideoDecoder\r\nimport numpy as np\r\n\r\nprocessor = AutoVideoProcessor.from_pretrained(\"facebook/vjepa2-vitl-fpc64-256\")\r\nmodel = AutoModel.from_pretrained(\r\n    \"facebook/vjepa2-vitl-fpc64-256\",\r\n    torch_dtype=torch.float16,\r\n    device_map=\"auto\",\r\n    attn_implementation=\"sdpa\"\r\n)\r\n\r\nvideo_url = \"https://huggingface.co/datasets/nateraw/kinetics-mini/resolve/main/val/archery/-Qz25rXdMjE_000014_000024.mp4\"\r\n\r\nvr = VideoDecoder(video_url)\r\nframe_idx = np.arange(0, 64) # choosing some frames. here, you can define more complex sampling strategy\r\nvideo = vr.get_frames_at(indices=frame_idx).data  # T x C x H x W\r\nvideo = processor(video, return_tensors=\"pt\").to(model.device)\r\noutputs = model(**video)\r\n\r\n# V-JEPA 2 encoder outputs, same as calling `model.get_vision_features()`\r\nencoder_outputs = outputs.last_hidden_state\r\n\r\n# V-JEPA 2 predictor outputs\r\npredictor_outputs = outputs.predictor_output.last_hidden_state\r\n```\r\n",
      "publishedAt": "2025-06-11T14:59:35.000Z",
      "source": "Hugging Face GitHub",
      "sourceUrl": "https://github.com/huggingface/transformers/releases/tag/v4.52.4-VJEPA-2-preview",
      "category": "tools",
      "company": "Hugging Face",
      "imageUrl": null,
      "tags": [
        "AI",
        "Transformer",
        "Meta"
      ],
      "featured": false
    },
    {
      "id": "mccp721hvglq2ha9bl",
      "title": "Anthropic: v0.54.0",
      "summary": "## 0.54.0 (2025-06-10)\n\nFull Changelog: [v0.53.0...v0.54.0](https://github.com/anthropics/anthropic-sdk-python/compare/v0.53.0...v0.54.0)\n\n### Features\n\n* **client:** add support for fine-grained-tool-streaming-2025-05-14 ([07ec081](https://github.com/anthropics/anthropic-sdk-python/commit/07ec08119...",
      "content": "## 0.54.0 (2025-06-10)\n\nFull Changelog: [v0.53.0...v0.54.0](https://github.com/anthropics/anthropic-sdk-python/compare/v0.53.0...v0.54.0)\n\n### Features\n\n* **client:** add support for fine-grained-tool-streaming-2025-05-14 ([07ec081](https://github.com/anthropics/anthropic-sdk-python/commit/07ec08119dbc328934fea5ec6eacd00c8dbda089))\n\n\n### Bug Fixes\n\n* **httpx:** resolve conflict between default transport and proxy settings ([#969](https://github.com/anthropics/anthropic-sdk-python/issues/969)) ([a6efded](https://github.com/anthropics/anthropic-sdk-python/commit/a6efdedcfef881ae3466bb77d92d0338c8338e20))\n* **tests:** update test ([99c2433](https://github.com/anthropics/anthropic-sdk-python/commit/99c243363e94f5f3f627cb8b80e3f238503c89f5))\n\n\n### Chores\n\n* **internal:** version bump ([45029f4](https://github.com/anthropics/anthropic-sdk-python/commit/45029f41c96f62f26ead99a5989c9ad974fc21b9))\n\n\n### Documentation\n\n* **contributing:** fix uv script for bootstrapping ([d2bde52](https://github.com/anthropics/anthropic-sdk-python/commit/d2bde52286ee8fa65995e73c579a8962087c1da4))",
      "publishedAt": "2025-06-11T02:45:52.000Z",
      "source": "Anthropic GitHub",
      "sourceUrl": "https://github.com/anthropics/anthropic-sdk-python/releases/tag/v0.54.0",
      "category": "tools",
      "company": "Anthropic",
      "imageUrl": null,
      "tags": [
        "AI",
        "Anthropic"
      ],
      "featured": true
    },
    {
      "id": "mccp732d95r6h17nzru",
      "title": "Meta: v0.2.0",
      "summary": "Llama 4 Support ( https://www.llama.com ) \r\n\r\n...",
      "content": "Llama 4 Support ( https://www.llama.com ) \r\n\r\n",
      "publishedAt": "2025-04-05T19:02:56.000Z",
      "source": "Meta GitHub",
      "sourceUrl": "https://github.com/meta-llama/llama-models/releases/tag/v0.2.0",
      "category": "research",
      "company": "Meta",
      "imageUrl": null,
      "tags": [
        "Llama"
      ],
      "featured": false
    },
    {
      "id": "mccp732djrwi4h1kxrm",
      "title": "Meta: v0.1.4",
      "summary": "## What's Changed\r\n* fix: do not use python_tag when encoding non-code_interpreter tool_calls by @ehhuang in https://github.com/meta-llama/llama-models/pull/283\r\n* fix: tool_call was not encoded by @ehhuang in https://github.com/meta-llama/llama-models/pull/284\r\n\r\n\r\n**Full Changelog**: https://githu...",
      "content": "## What's Changed\r\n* fix: do not use python_tag when encoding non-code_interpreter tool_calls by @ehhuang in https://github.com/meta-llama/llama-models/pull/283\r\n* fix: tool_call was not encoded by @ehhuang in https://github.com/meta-llama/llama-models/pull/284\r\n\r\n\r\n**Full Changelog**: https://github.com/meta-llama/llama-models/compare/v0.1.3...v0.1.4",
      "publishedAt": "2025-02-25T00:04:44.000Z",
      "source": "Meta GitHub",
      "sourceUrl": "https://github.com/meta-llama/llama-models/releases/tag/v0.1.4",
      "category": "research",
      "company": "Meta",
      "imageUrl": null,
      "tags": [
        "Llama",
        "Meta"
      ],
      "featured": false
    },
    {
      "id": "mccp7aq1h5dm1u54tbm",
      "title": "Ask HN: Have LLM or generative AI made you more productive?",
      "summary": "I work in the gaming industry, and while there was a push to try to find integrations for AI, the most promising usages of machine learning algorithms are in areas like anti-cheat. One engineer reported success using a GPT product to generate some code which he could then correct, but the majority o...",
      "content": "I work in the gaming industry, and while there was a push to try to find integrations for AI, the most promising usages of machine learning algorithms are in areas like anti-cheat. One engineer reported success using a GPT product to generate some code which he could then correct, but the majority of his job doesn&#x27;t actually involve writing large amounts of new code so the utility is extremely limited. I don&#x27;t know of any other usages of LLM across our teams.<p>I am extremely interested in hearing of success stories from other industries, or conversely I&#x27;d love to hear about what other experiments others have been doing.",
      "publishedAt": "2024-10-04T15:28:28.000Z",
      "source": "Hacker News AI",
      "sourceUrl": "https://news.ycombinator.com/item?id=41742430",
      "category": "industry",
      "company": "Community",
      "imageUrl": null,
      "tags": [
        "GPT",
        "LLM",
        "AI"
      ],
      "featured": false
    },
    {
      "id": "mccp7aq1wsy8zc8y0nr",
      "title": "Ask HN: What's the role GCP/Google's LLM play in GenAI market",
      "summary": "This ask post is very opinionated.<p>I used be a die-hard Google&#x2F;GCP lover, from wanting to enter Google as an engineer when I was a student, to proposing to try out GCP for a new project at my first internship (and succeeded). Used Firebase for my on-campus goods trading platform, course proje...",
      "content": "This ask post is very opinionated.<p>I used be a die-hard Google&#x2F;GCP lover, from wanting to enter Google as an engineer when I was a student, to proposing to try out GCP for a new project at my first internship (and succeeded). Used Firebase for my on-campus goods trading platform, course projects, and contract projects, and got good experiences. I also used Colab as my go-to choice for my study of machine learning, running models for tasks including object detection, image segmentation, basic machine learning projects, and fine-tuning transformer models for my NLP course projects.<p>I still remembered the release Google&#x27;s LaMDA model in 2021, and the beautiful blog post site. They used to be the leading role in GenAI or AI serving in general with TensorFlow serve on GCP. However, ever since last year, I really felt that Google is severely lagging behind in the cloud service or GenAI in general.<p>For GCP, especially the AI related workspace, I see no meaningful improvement over the years. The file system still has a weird triangle that when you click it does nothing, the UI is extremely laggy when the data is huge just like in colab. I don&#x27;t even want to mention the accidental deletion of Unisuper&#x27;s data.<p>For Gemini, despite it has large context window (2M for 1.5 Pro, 1M for 1.5 Flash), I see no value at all to use this. I generally judge a service&#x27;s usability on three things: model competency &gt; API (design, price, speed) &gt; playground ease of use (because you can always use other playgrounds when you have the API). Gemini sucks on every aspect. It&#x27;s never in par with GPT-4o (opinionated, tested on all my graduate CS quiz questions). The API is such a pain in the ass (500, 400 all over the place when I was following the cookbook and Gemini&#x27;s team could not even solve my issue at a hackathon!) They playground is the most counter-intuitive thing I&#x27;ve ever used in my life.<p>What&#x27;s more, the executive team of the Google seems to believe that Gemini 1.5 Pro is as satisfying as GPT-4, and Gemini 1.5 Flash is better, faster, and cheaper than GPT-3.5. First statement is false (to me). Second statement is meaningless, because you should be comparing Gemini 1.5 Flash to Llama 3 or Mistral instead of GPT-3.5. Don&#x27;t even bring up that Gemini is not fine-tunable at this moment.<p>They have a slide number that over 60% GenAI startup is using GCP. I simply don&#x27;t understand where this number is from. Almost every startup I encountered is either using OpenAI or Azure Studio (if they have some scale). I used to be a MSFT hater, but Azure AI Studio just has far better DX and model catalog compared to Vertex AI.<p>Therefore, unless my company has already signed a contract with GCP, I really don&#x27;t see the point of a new startup starting to use GCP or Gemini in general.",
      "publishedAt": "2024-05-16T07:04:28.000Z",
      "source": "Hacker News AI",
      "sourceUrl": "https://news.ycombinator.com/item?id=40375903",
      "category": "industry",
      "company": "Community",
      "imageUrl": null,
      "tags": [
        "GPT",
        "Gemini",
        "Llama",
        "LLM",
        "AI"
      ],
      "featured": false
    },
    {
      "id": "mccp7aq1mfr894ps7b",
      "title": "Show HN: UpTrain (YC W23) ‚Äì open-source tool to evaluate LLM response quality",
      "summary": "Hello, we are Shikha and Sourabh, founders of UpTrain(YC W23) - an open-source tool to evaluate the performance of your LLM applications on aspects such as correctness, tonality, hallucination, fluency, etc.<p>The Problem: Unlike traditional Machine learning or Deep learning models where we always h...",
      "content": "Hello, we are Shikha and Sourabh, founders of UpTrain(YC W23) - an open-source tool to evaluate the performance of your LLM applications on aspects such as correctness, tonality, hallucination, fluency, etc.<p>The Problem: Unlike traditional Machine learning or Deep learning models where we always have a unique Ground Truth and can define metrics like Precision, Recall, accuracy, etc. to quantify the model‚Äôs performance, LLMs are trickier and it is very difficult to estimate if their response is correct or not. If you are using GPT-4 to write a recruitment email, there is no unique correct email to do a word-to-word comparison against.<p>As you build an LLM application, you want to compare it against different model providers, prompt configurations, etc., and figure out the best working combination. Instead of manually skimming through a couple of model responses, you want to run them through hundreds of test cases, aggregate their scores, and make an informed decision. Additionally, as your application generates responses for real user queries, you don‚Äôt want to wait for them to complain about the model inaccuracy, instead, you want to monitor the model‚Äôs performance over time and get alerted in case of any drifts.<p>Again, at the core of it, you want a tool to evaluate the quality of your LLM response and assign quantitative scores.<p>The Solution: To solve this, we are building UpTrain which has a set of evaluation metrics so that you can know when your application is going wrong. These metrics include traditional NLP metrics like Rogue, Bleu, etc., embeddings similarity metrics as well as model grading scores i.e. where we use LLMs to evaluate different aspects of your response. A few of these evaluation metrics include:<p>1. Response Relevancy: Measures if the response contains any irrelevant information\n2. Response Completeness: Measures if the response answers all aspects of the given question\n3. Factual Accuracy: Measures hallucinations i.e. if the response has any made-up information or not with respect to the provided context\n4. Retrieved Context Quality: Measures if the retrieved context has sufficient information to answer the given question\n5. Response Tonality: Measures if the response aligns with a specific persona or desired tone\netc.<p>We have designed workflows so that you can easily add your testing dataset, configure which checks you want to run (you can also define custom checks suitable for your use case) and conveniently access the results via Streamlit dashboards.<p>UpTrain also has experimentation capabilities where you can specify different prompt variations and models to test across and use these quantitative checks to find the best configuration for your application.<p>You can also use UpTrain to monitor your application‚Äôs performance and find avenues for improvement. We integrate directly with your databases (BigQuery, Postgres, MongoDB, etc.) and can run daily evaluations.<p>We‚Äôve launched the tool under an Apache 2.0 license to make it easy for everyone to integrate it into their LLM workflows. Additionally, we also provide managed service (with a free trial) where you can run LLM evaluations via an API request or through UpTrain testing console.<p>We would love for you to try it out and give your feedback.<p>Links:\nDemo: <a href=\"https:&#x2F;&#x2F;demo.uptrain.ai&#x2F;evals_demo&#x2F;\">https:&#x2F;&#x2F;demo.uptrain.ai&#x2F;evals_demo&#x2F;</a>\nGithub repo: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;uptrain-ai&#x2F;uptrain\">https:&#x2F;&#x2F;github.com&#x2F;uptrain-ai&#x2F;uptrain</a>\nCreate an account (free): <a href=\"https:&#x2F;&#x2F;uptrain.ai&#x2F;dashboard\">https:&#x2F;&#x2F;uptrain.ai&#x2F;dashboard</a>\nUpTrain testing console (need an account): <a href=\"https:&#x2F;&#x2F;demo.uptrain.ai&#x2F;dashboard\">https:&#x2F;&#x2F;demo.uptrain.ai&#x2F;dashboard</a>\nWebsite: <a href=\"https:&#x2F;&#x2F;uptrain.ai&#x2F;\">https:&#x2F;&#x2F;uptrain.ai&#x2F;</a>",
      "publishedAt": "2023-08-22T14:08:00.000Z",
      "source": "Hacker News AI",
      "sourceUrl": "https://demo.uptrain.ai/evals_demo/",
      "category": "industry",
      "company": "Community",
      "imageUrl": null,
      "tags": [
        "GPT",
        "LLM",
        "AI",
        "ML",
        "Deep Learning"
      ],
      "featured": false
    },
    {
      "id": "mccp7cnrd9v820nzvb",
      "title": "Social Norms Guide Reference Resolution",
      "summary": "Humans use natural language, vision, and context to resolve referents in their environment. While some situated reference resolution is trivial, ambiguous cases arise when the language is underspecified or there are multiple candidate referents. This study investigates howpragmatic modulators extern...",
      "content": "Humans use natural language, vision, and context to resolve referents in their environment. While some situated reference resolution is trivial, ambiguous cases arise when the language is underspecified or there are multiple candidate referents. This study investigates howpragmatic modulators external to the linguistic content are critical for the correct interpretation of referents in these scenarios. Inparticular, we demonstrate in a human subjects experiment how the social norms applicable in the given context influence theinterpretation of referring expressions. Additionally, we highlight how current coreference tools in natural language processing fail tohandle these ambiguous cases. We also briefly discuss the implications of this work for assistive robots which will routinely need to resolve referents in their environment.",
      "publishedAt": "1970-01-01T00:00:00.000Z",
      "source": "Papers with Code",
      "sourceUrl": "https://aclanthology.org/2022.naacl-main.1.pdf",
      "category": "research",
      "company": "Community",
      "imageUrl": null,
      "tags": [
        "AI"
      ],
      "featured": false
    },
    {
      "id": "mccp7cnrj9dknz2juw8",
      "title": "Modeling Content Importance for Summarization with Pre-trained Language Models",
      "summary": "Modeling content importance is an essential yet challenging task for summarization. Previous work is mostly based on statistical methods that estimate word-level salience, which does not consider semantics and larger context when quantifying importance. It is thus hard for these methods to generaliz...",
      "content": "Modeling content importance is an essential yet challenging task for summarization. Previous work is mostly based on statistical methods that estimate word-level salience, which does not consider semantics and larger context when quantifying importance. It is thus hard for these methods to generalize to semantic units of longer text spans. In this work, we apply information theory on top of pre-trained language models and define the concept of importance from the perspective of information amount. It considers both the semantics and context when evaluating the importance of each semantic unit. With the help of pre-trained language models, it can easily generalize to different kinds of semantic units n-grams or sentences. Experiments on CNN/Daily Mail and New York Times datasets demonstrate that our method can better model the importance of content than prior work based on F1 and ROUGE scores.",
      "publishedAt": "1970-01-01T00:00:00.000Z",
      "source": "Papers with Code",
      "sourceUrl": "https://aclanthology.org/2020.emnlp-main.293.pdf",
      "category": "research",
      "company": "Community",
      "imageUrl": null,
      "tags": [
        "AI"
      ],
      "featured": false
    }
  ]
}