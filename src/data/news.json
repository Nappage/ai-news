{
  "lastUpdated": "2025-07-04T18:24:41.011Z",
  "totalArticles": 34,
  "articles": [
    {
      "id": "mcp58kc4g4uy6pi3dge",
      "title": "Opening up ‘Zero-Knowledge Proof’ technology to promote privacy in age assurance",
      "summary": "Today, we open sourced our Zero-Knowledge Proof (ZKP) libraries, fulfilling a promise and building on our partnership with Sparkasse to support EU age assurance.",
      "content": "<img src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Screenshot_2025-07-03_12.59.53_.max-600x600.format-webp.webp\">Today, we open sourced our Zero-Knowledge Proof (ZKP) libraries, fulfilling a promise and building on our partnership with Sparkasse to support EU age assurance.",
      "publishedAt": "2025-07-03T12:00:00.000Z",
      "source": "Google Technology Blog",
      "sourceUrl": "https://blog.google/technology/safety-security/opening-up-zero-knowledge-proof-technology-to-promote-privacy-in-age-assurance/",
      "category": "companies",
      "company": "Google",
      "imageUrl": null,
      "tags": [],
      "featured": true
    },
    {
      "id": "mcp58jdz7rioy6tboix",
      "title": "Introducing Gemma 3n: The developer guide",
      "summary": "The Gemma 3n model has been fully released, building on the success of previous Gemma models and bringing advanced on-device multimodal capabilities to edge devices with unprecedented performance. Explore Gemma 3n's innovations, including its mobile-first architecture, MatFormer technology, Per-Laye...",
      "content": "The Gemma 3n model has been fully released, building on the success of previous Gemma models and bringing advanced on-device multimodal capabilities to edge devices with unprecedented performance. Explore Gemma 3n's innovations, including its mobile-first architecture, MatFormer technology, Per-Layer Embeddings, KV Cache Sharing, and new audio and MobileNet-V5 vision encoders, and how developers can start building with it today.",
      "publishedAt": "2025-07-01T18:24:08.999Z",
      "source": "Google Developers Blog",
      "sourceUrl": "https://developers.googleblog.com/en/introducing-gemma-3n-developer-guide/",
      "category": "tools",
      "company": "Google",
      "imageUrl": null,
      "tags": [],
      "featured": true
    },
    {
      "id": "mcp58je0ipiy1aif3j",
      "title": "Supercharge your notebooks: The new AI-first Google Colab is now available to everyone",
      "summary": "The new AI-first Google Colab enhances productivity with improvements powered by features like iterative querying for conversational coding, a next-generation Data Science Agent for autonomous workflows, and effortless code transformation. Early adopters report a dramatic productivity boost, acceler...",
      "content": "The new AI-first Google Colab enhances productivity with improvements powered by features like iterative querying for conversational coding, a next-generation Data Science Agent for autonomous workflows, and effortless code transformation. Early adopters report a dramatic productivity boost, accelerating ML projects, debugging code faster, and effortlessly creating high-quality visualizations.",
      "publishedAt": "2025-06-30T18:24:09.000Z",
      "source": "Google Developers Blog",
      "sourceUrl": "https://developers.googleblog.com/en/new-ai-first-google-colab-now-available-to-everyone/",
      "category": "tools",
      "company": "Google",
      "imageUrl": null,
      "tags": [
        "AI",
        "ML",
        "Google"
      ],
      "featured": true
    },
    {
      "id": "mcp58gkss6l0g2n3pna",
      "title": "AI in Australia—OpenAI’s Economic Blueprint",
      "summary": "Today, OpenAI, in partnership with Mandala Partners, is sharing the OpenAI AI Economic Blueprint for Australia. At a time when boosting productivity has emerged as a national priority for Australia, the Blueprint provides a clear, actionable plan for how Australia can unlock the full economic and so...",
      "content": "Today, OpenAI, in partnership with Mandala Partners, is sharing the OpenAI AI Economic Blueprint for Australia. At a time when boosting productivity has emerged as a national priority for Australia, the Blueprint provides a clear, actionable plan for how Australia can unlock the full economic and social potential of artificial intelligence.",
      "publishedAt": "2025-06-30T07:00:00.000Z",
      "source": "OpenAI Blog",
      "sourceUrl": "https://openai.com/global-affairs/openais-australia-economic-blueprint",
      "category": "companies",
      "company": "OpenAI",
      "imageUrl": null,
      "tags": [
        "AI",
        "OpenAI"
      ],
      "featured": true
    },
    {
      "id": "mcp58je0f8vt9za62ro",
      "title": "Google Cloud donates A2A to Linux Foundation",
      "summary": "Google, along with Amazon and Cisco, announces the formation of the Agent2Agent Foundation under the Linux Foundation, establishing A2A as an industry standard for AI agent interoperability, fostering a diverse ecosystem, ensuring neutral governance, and accelerating secure innovation in AI applicat...",
      "content": "Google, along with Amazon and Cisco, announces the formation of the Agent2Agent Foundation under the Linux Foundation, establishing A2A as an industry standard for AI agent interoperability, fostering a diverse ecosystem, ensuring neutral governance, and accelerating secure innovation in AI applications.",
      "publishedAt": "2025-06-28T18:24:09.000Z",
      "source": "Google Developers Blog",
      "sourceUrl": "https://developers.googleblog.com/en/google-cloud-donates-a2a-to-linux-foundation/",
      "category": "tools",
      "company": "Google",
      "imageUrl": null,
      "tags": [
        "AI",
        "Google"
      ],
      "featured": true
    },
    {
      "id": "mcp58kc4ir2woj4ld4d",
      "title": "We’re improving Ask Photos and bringing it to more Google Photos users.",
      "summary": "We love seeing how you’re using Ask Photos in early access, like asking \"suggest photos that'd make great phone backgrounds\" or \"what did I eat on my trip to Barcelona?\"…",
      "content": "<img src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/original_videos/wagtailvideo-mbcag8qi_thumb.jpg\">We love seeing how you’re using Ask Photos in early access, like asking \"suggest photos that'd make great phone backgrounds\" or \"what did I eat on my trip to Barcelona?\"…",
      "publishedAt": "2025-06-26T17:00:00.000Z",
      "source": "Google Technology Blog",
      "sourceUrl": "https://blog.google/products/photos/updates-ask-photos-search/",
      "category": "companies",
      "company": "Google",
      "imageUrl": null,
      "tags": [
        "Google"
      ],
      "featured": true
    },
    {
      "id": "mcp58gkthuy7p8iiitw",
      "title": "Preparing for future AI risks in biology",
      "summary": "Advanced AI can transform biology and medicine—but also raises biosecurity risks. We’re proactively assessing capabilities and implementing safeguards to prevent misuse.",
      "content": "Advanced AI can transform biology and medicine—but also raises biosecurity risks. We’re proactively assessing capabilities and implementing safeguards to prevent misuse.",
      "publishedAt": "2025-06-18T10:00:00.000Z",
      "source": "OpenAI Blog",
      "sourceUrl": "https://openai.com/index/preparing-for-future-ai-capabilities-in-biology",
      "category": "companies",
      "company": "OpenAI",
      "imageUrl": null,
      "tags": [
        "AI"
      ],
      "featured": true
    },
    {
      "id": "mcp58kc4d6sm508y3sf",
      "title": "The latest AI news we announced in June",
      "summary": "Here are Google’s latest AI updates from June 2025",
      "content": "<img src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/June_AI_Recap_social-share.max-600x600.format-webp.webp\">Here are Google’s latest AI updates from June 2025",
      "publishedAt": "2025-07-02T16:00:00.000Z",
      "source": "Google Technology Blog",
      "sourceUrl": "https://blog.google/technology/ai/google-ai-updates-june-2025/",
      "category": "companies",
      "company": "Google",
      "imageUrl": null,
      "tags": [
        "AI",
        "Google"
      ],
      "featured": false
    },
    {
      "id": "mcp58je0ymxn9oykhc",
      "title": "Using KerasHub for easy end-to-end machine learning workflows with Hugging Face",
      "summary": "KerasHub enables users to mix and match model architectures and weights across different machine learning frameworks, allowing checkpoints from sources like Hugging Face Hub (including those created with PyTorch) to be loaded into Keras models for use with JAX, PyTorch, or TensorFlow. This flexibili...",
      "content": "KerasHub enables users to mix and match model architectures and weights across different machine learning frameworks, allowing checkpoints from sources like Hugging Face Hub (including those created with PyTorch) to be loaded into Keras models for use with JAX, PyTorch, or TensorFlow. This flexibility means you can leverage a vast array of community fine-tuned models while maintaining full control over your chosen backend framework.",
      "publishedAt": "2025-07-01T18:24:09.000Z",
      "source": "Google Developers Blog",
      "sourceUrl": "https://developers.googleblog.com/en/load-model-weights-from-safetensors-into-kerashub-multi-framework-machine-learning/",
      "category": "tools",
      "company": "Google",
      "imageUrl": null,
      "tags": [
        "AI"
      ],
      "featured": false
    },
    {
      "id": "mcp58je03e9njujmftm",
      "title": "Imagen 4 is now available in the Gemini API and Google AI Studio",
      "summary": "Imagen 4, Google's advanced text-to-image model, is now available in paid preview via the Gemini API and Google AI Studio, offering significant quality improvements, especially for text generation within images. The Imagen 4 family includes Imagen 4 for general tasks and Imagen 4 Ultra for high-prec...",
      "content": "Imagen 4, Google's advanced text-to-image model, is now available in paid preview via the Gemini API and Google AI Studio, offering significant quality improvements, especially for text generation within images. The Imagen 4 family includes Imagen 4 for general tasks and Imagen 4 Ultra for high-precision prompt adherence, with all generated images featuring a non-visible SynthID watermark.",
      "publishedAt": "2025-07-01T18:24:09.000Z",
      "source": "Google Developers Blog",
      "sourceUrl": "https://developers.googleblog.com/en/imagen-4-now-available-in-the-gemini-api-and-google-ai-studio/",
      "category": "tools",
      "company": "Google",
      "imageUrl": null,
      "tags": [
        "Gemini",
        "AI",
        "Google"
      ],
      "featured": false
    },
    {
      "id": "mcp58kc475f0z0i5uxr",
      "title": "We used Veo to animate archive photography from the Harley-Davidson Museum",
      "summary": "In Moving Archives, we’re bringing the iconic Harley-Davidson Museum archives to life with the help of Veo and Gemini.",
      "content": "<img src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/MovingArchives_SS.max-600x600.format-webp.webp\">In Moving Archives, we’re bringing the iconic Harley-Davidson Museum archives to life with the help of Veo and Gemini.",
      "publishedAt": "2025-07-01T13:00:00.000Z",
      "source": "Google Technology Blog",
      "sourceUrl": "https://blog.google/outreach-initiatives/arts-culture/moving-archives/",
      "category": "companies",
      "company": "Google",
      "imageUrl": null,
      "tags": [
        "Gemini"
      ],
      "featured": false
    },
    {
      "id": "mcp58kc45vpear2hddp",
      "title": "A new award from Google for ML and systems pioneers in academia",
      "summary": "We’re announcing a new program, the Google ML and Systems Junior Faculty Awards.",
      "content": "<img src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/_A-new-award-from-Google-for-te.max-600x600.format-webp.webp\">We’re announcing a new program, the Google ML and Systems Junior Faculty Awards.",
      "publishedAt": "2025-07-01T12:38:00.000Z",
      "source": "Google Technology Blog",
      "sourceUrl": "https://blog.google/products/google-cloud/ml-systems-junior-faculty-awards/",
      "category": "companies",
      "company": "Google",
      "imageUrl": null,
      "tags": [
        "ML",
        "Google"
      ],
      "featured": false
    },
    {
      "id": "mcp58gkszrvtyuvkdsd",
      "title": "No-code personal agents, powered by GPT-4.1 and Realtime API",
      "summary": "Learn how Genspark built a $36M ARR AI product in 45 days—with no-code agents powered by GPT-4.1 and OpenAI Realtime API.",
      "content": "Learn how Genspark built a $36M ARR AI product in 45 days—with no-code agents powered by GPT-4.1 and OpenAI Realtime API.",
      "publishedAt": "2025-07-01T10:00:00.000Z",
      "source": "OpenAI Blog",
      "sourceUrl": "https://openai.com/index/genspark",
      "category": "companies",
      "company": "OpenAI",
      "imageUrl": null,
      "tags": [
        "GPT",
        "AI",
        "OpenAI"
      ],
      "featured": false
    },
    {
      "id": "mcp58kc446h5pcfy1x",
      "title": "We are announcing Sparkasse as our first national credential partner for EU age assurance.",
      "summary": "The lack of a reliable standard for safe, effective age checks across sites and apps online has long frustrated parents and companies. Google’s Credential Manager API — …",
      "content": "<img src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/original_videos/wagtailvideo-cf5278s1_thumb.jpg\">The lack of a reliable standard for safe, effective age checks across sites and apps online has long frustrated parents and companies. Google’s Credential Manager API — …",
      "publishedAt": "2025-07-01T07:50:00.000Z",
      "source": "Google Technology Blog",
      "sourceUrl": "https://blog.google/around-the-globe/google-europe/we-are-announcing-sparkasse-as-our-first-national-credential-partner-for-eu-age-assurance/",
      "category": "companies",
      "company": "Google",
      "imageUrl": null,
      "tags": [
        "Google"
      ],
      "featured": false
    },
    {
      "id": "mcp58hi30kf87h0odxa",
      "title": "Training and Finetuning Sparse Embedding Models with Sentence Transformers v5",
      "summary": "",
      "content": "",
      "publishedAt": "2025-07-01T00:00:00.000Z",
      "source": "Hugging Face Blog",
      "sourceUrl": "https://huggingface.co/blog/train-sparse-encoder",
      "category": "tools",
      "company": "Hugging Face",
      "imageUrl": null,
      "tags": [
        "AI",
        "Transformer"
      ],
      "featured": false
    },
    {
      "id": "mcp58je0d8wyctzlh88",
      "title": "Unlock deeper insights with the new Python client library for Data Commons",
      "summary": "Google has released a new Python client library for Data Commons – an open-source knowledge graph that unifies public statistical data, and enhances how data developers can leverage Data Commons by offering improved features, support for custom instances, and easier access to a vast array of statist...",
      "content": "Google has released a new Python client library for Data Commons – an open-source knowledge graph that unifies public statistical data, and enhances how data developers can leverage Data Commons by offering improved features, support for custom instances, and easier access to a vast array of statistical variables – developed with contributions from The ONE Campaign.",
      "publishedAt": "2025-06-30T18:24:08.999Z",
      "source": "Google Developers Blog",
      "sourceUrl": "https://developers.googleblog.com/en/pythondatacommons/",
      "category": "tools",
      "company": "Google",
      "imageUrl": null,
      "tags": [
        "AI",
        "Google"
      ],
      "featured": false
    },
    {
      "id": "mcp58kc4pv5uqjktam",
      "title": "Expanded access to Google Vids and no-cost AI tools in Classroom",
      "summary": "Learn more about expanded access to Google Vids for all education users, and Gemini in Classroom, a new suite of no-cost AI tools available for educators.",
      "content": "<img src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/027-ISTE-EDU-Keyword_blog-Googl.max-600x600.format-webp.webp\">Learn more about expanded access to Google Vids for all education users, and Gemini in Classroom, a new suite of no-cost AI tools available for educators.",
      "publishedAt": "2025-06-30T13:00:00.000Z",
      "source": "Google Technology Blog",
      "sourceUrl": "https://blog.google/outreach-initiatives/education/expanded-access-to-google-vids-and-no-cost-ai-tools-in-classroom/",
      "category": "companies",
      "company": "Google",
      "imageUrl": null,
      "tags": [
        "Gemini",
        "AI",
        "Google"
      ],
      "featured": false
    },
    {
      "id": "mcp58je00w9o4wmuth5e",
      "title": "Simulating a neural operating system with Gemini 2.5 Flash-Lite",
      "summary": "A research prototype simulating a neural operating system generates UI in real-time adapting to user interactions with Gemini 2.5 Flash-Lite, using interaction tracing for contextual awareness, streaming the UI for responsiveness, and achieving statefulness with an in-memory UI graph.",
      "content": "A research prototype simulating a neural operating system generates UI in real-time adapting to user interactions with Gemini 2.5 Flash-Lite, using interaction tracing for contextual awareness, streaming the UI for responsiveness, and achieving statefulness with an in-memory UI graph.",
      "publishedAt": "2025-06-29T18:24:09.000Z",
      "source": "Google Developers Blog",
      "sourceUrl": "https://developers.googleblog.com/en/simulating-a-neural-operating-system-with-gemini-2-5-flash-lite/",
      "category": "tools",
      "company": "Google",
      "imageUrl": null,
      "tags": [
        "Gemini"
      ],
      "featured": false
    },
    {
      "id": "mcp58je0o1d0ud51f2",
      "title": "Gemini 2.5 for robotics and embodied intelligence",
      "summary": "Gemini 2.5 Pro and Flash are transforming robotics by enhancing coding, reasoning, and multimodal capabilities, including spatial understanding. These models are used for semantic scene understanding, code generation for robot control, and building interactive applications with the Live API, with a ...",
      "content": "Gemini 2.5 Pro and Flash are transforming robotics by enhancing coding, reasoning, and multimodal capabilities, including spatial understanding. These models are used for semantic scene understanding, code generation for robot control, and building interactive applications with the Live API, with a strong emphasis on safety improvements and community applications.",
      "publishedAt": "2025-06-28T18:24:09.000Z",
      "source": "Google Developers Blog",
      "sourceUrl": "https://developers.googleblog.com/en/gemini-25-for-robotics-and-embodied-intelligence/",
      "category": "tools",
      "company": "Google",
      "imageUrl": null,
      "tags": [
        "Gemini"
      ],
      "featured": false
    },
    {
      "id": "mcp595l3o7lpviegiaq",
      "title": "Show HN: m(ctf)p – A semi-automated environment for solving CTF challenges",
      "summary": "Hi folks!<p>I built this over the past few weeks for a cross-company CTF I was participating in. It was mostly an experiment to learn about CTFs, MCP servers, Kali Linux, Claude Code, and really just how far LLMs can go in a given domain.<p>It&#x27;s basically just an MCP server (for integrating wit...",
      "content": "Hi folks!<p>I built this over the past few weeks for a cross-company CTF I was participating in. It was mostly an experiment to learn about CTFs, MCP servers, Kali Linux, Claude Code, and really just how far LLMs can go in a given domain.<p>It&#x27;s basically just an MCP server (for integrating with the CTF server APIs, providing notes, and a few other niceties) paired with a Kali Linux-based Docker image that has Claude Code installed, plus a custom slash command [1] to tie it all together.<p>It performed admirably during the CTF I tried it on, it was able to zero-shot solve maybe 10 or so of the simpler challenges, and provided substantial assistance on another 5 or 6 before getting stuck. It didn&#x27;t stand a chance against the hardest challenges.<p>This was the first CTF I&#x27;ve participated in, and it was an absolute blast. I can imagine some people feeling that LLMs take the fun out of CTFs, but I think the &quot;centaur&quot; [2] aspect of human-LLM interactions is both powerful and effective, given the right infrastructure and UX.<p>Happy to answer any questions people have about the project!<p>[1] <a href=\"https:&#x2F;&#x2F;docs.anthropic.com&#x2F;en&#x2F;docs&#x2F;claude-code&#x2F;slash-commands#custom-slash-commands\" rel=\"nofollow\">https:&#x2F;&#x2F;docs.anthropic.com&#x2F;en&#x2F;docs&#x2F;claude-code&#x2F;slash-command...</a><p>[2] <a href=\"https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Advanced_chess\" rel=\"nofollow\">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Advanced_chess</a>",
      "publishedAt": "2025-06-28T01:53:52.000Z",
      "source": "Hacker News Claude/Anthropic",
      "sourceUrl": "https://git.sr.ht/~bsprague/mctfp",
      "category": "companies",
      "company": "Anthropic",
      "imageUrl": null,
      "tags": [
        "Claude",
        "LLM",
        "AI",
        "Anthropic"
      ],
      "featured": false
    },
    {
      "id": "mcp58kc45ijjhkgci2e",
      "title": "Try on looks and discover your style with Doppl",
      "summary": "Doppl, a new Google Labs app, uses AI to create personalized outfit try-on images and videos.",
      "content": "<img src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Doppl_-_YouTube_Thumbnail_-_120.max-600x600.format-webp.webp\">Doppl, a new Google Labs app, uses AI to create personalized outfit try-on images and videos.",
      "publishedAt": "2025-06-26T16:16:00.000Z",
      "source": "Google Technology Blog",
      "sourceUrl": "https://blog.google/technology/google-labs/doppl/",
      "category": "companies",
      "company": "Google",
      "imageUrl": null,
      "tags": [
        "AI",
        "Google"
      ],
      "featured": false
    },
    {
      "id": "mcp58gksagkfikrn02l",
      "title": "Customizable, no-code voice agent automation with GPT-4o",
      "summary": "Retell AI is transforming the call center with AI voice automation powered by GPT-4o and GPT-4.1. Its no-code platform enables businesses to launch natural, real-time voice agents that cut call costs, boost CSAT, and automate customer conversations—without scripts or hold times.",
      "content": "Retell AI is transforming the call center with AI voice automation powered by GPT-4o and GPT-4.1. Its no-code platform enables businesses to launch natural, real-time voice agents that cut call costs, boost CSAT, and automate customer conversations—without scripts or hold times.",
      "publishedAt": "2025-06-26T10:00:00.000Z",
      "source": "OpenAI Blog",
      "sourceUrl": "https://openai.com/index/retell-ai",
      "category": "companies",
      "company": "OpenAI",
      "imageUrl": null,
      "tags": [
        "GPT",
        "AI"
      ],
      "featured": false
    },
    {
      "id": "mcp58hi37brmnzzwqcl",
      "title": "Gemma 3n fully available in the open-source ecosystem!",
      "summary": "",
      "content": "",
      "publishedAt": "2025-06-26T00:00:00.000Z",
      "source": "Hugging Face Blog",
      "sourceUrl": "https://huggingface.co/blog/gemma3n",
      "category": "tools",
      "company": "Hugging Face",
      "imageUrl": null,
      "tags": [
        "AI"
      ],
      "featured": false
    },
    {
      "id": "mcp58o2uzj54ibt66gp",
      "title": "AlphaGenome: AI for better understanding the genome",
      "summary": "Introducing a new, unifying DNA sequence model that advances regulatory variant-effect prediction and promises to shed new light on genome function — now available via API.",
      "content": "Introducing a new, unifying DNA sequence model that advances regulatory variant-effect prediction and promises to shed new light on genome function — now available via API.",
      "publishedAt": "2025-06-25T13:59:00.000Z",
      "source": "DeepMind Blog",
      "sourceUrl": "https://deepmind.google/discover/blog/alphagenome-ai-for-better-understanding-the-genome/",
      "category": "research",
      "company": "Google",
      "imageUrl": null,
      "tags": [
        "AI"
      ],
      "featured": false
    },
    {
      "id": "mcp58o2u6gk2ako5bcu",
      "title": "Gemini Robotics On-Device brings AI to local robotic devices",
      "summary": "We’re introducing an efficient, on-device robotics model with general-purpose dexterity and fast task adaptation.",
      "content": "We’re introducing an efficient, on-device robotics model with general-purpose dexterity and fast task adaptation.",
      "publishedAt": "2025-06-24T14:00:00.000Z",
      "source": "DeepMind Blog",
      "sourceUrl": "https://deepmind.google/discover/blog/gemini-robotics-on-device-brings-ai-to-local-robotic-devices/",
      "category": "research",
      "company": "Google",
      "imageUrl": null,
      "tags": [
        "Gemini",
        "AI"
      ],
      "featured": false
    },
    {
      "id": "mcp58gksk3gzwx2b0vn",
      "title": "Driving scalable growth with OpenAI o3, GPT-4.1, and CUA",
      "summary": "Unify, an AI-powered GTM platform, uses OpenAI’s o3, GPT-4.1, and CUA to automate prospecting, research, and outreach. With hyper-personalized messaging and an always-on workflow, Unify helps teams generate pipeline at scale while focusing on high-impact customer interactions.",
      "content": "Unify, an AI-powered GTM platform, uses OpenAI’s o3, GPT-4.1, and CUA to automate prospecting, research, and outreach. With hyper-personalized messaging and an always-on workflow, Unify helps teams generate pipeline at scale while focusing on high-impact customer interactions.",
      "publishedAt": "2025-06-24T00:00:00.000Z",
      "source": "OpenAI Blog",
      "sourceUrl": "https://openai.com/index/unify",
      "category": "companies",
      "company": "OpenAI",
      "imageUrl": null,
      "tags": [
        "GPT",
        "AI",
        "OpenAI"
      ],
      "featured": false
    },
    {
      "id": "mcp58hi3pr2sfvpf5xn",
      "title": "Transformers backend integration in SGLang",
      "summary": "",
      "content": "",
      "publishedAt": "2025-06-23T00:00:00.000Z",
      "source": "Hugging Face Blog",
      "sourceUrl": "https://huggingface.co/blog/transformers-backend-sglang",
      "category": "tools",
      "company": "Hugging Face",
      "imageUrl": null,
      "tags": [
        "Transformer"
      ],
      "featured": false
    },
    {
      "id": "mcp58hi3k1k9jfi31yo",
      "title": "(LoRA) Fine-Tuning FLUX.1-dev on Consumer Hardware",
      "summary": "",
      "content": "",
      "publishedAt": "2025-06-19T00:00:00.000Z",
      "source": "Hugging Face Blog",
      "sourceUrl": "https://huggingface.co/blog/flux-qlora",
      "category": "tools",
      "company": "Hugging Face",
      "imageUrl": null,
      "tags": [],
      "featured": false
    },
    {
      "id": "mcp58o2uif48ka68l",
      "title": "Gemini 2.5: Updates to our family of thinking models",
      "summary": "Explore the latest Gemini 2.5 model updates with enhanced performance and accuracy: Gemini 2.5 Pro now stable, Flash generally available, and the new Flash-Lite in preview.",
      "content": "Explore the latest Gemini 2.5 model updates with enhanced performance and accuracy: Gemini 2.5 Pro now stable, Flash generally available, and the new Flash-Lite in preview.",
      "publishedAt": "2025-06-17T16:03:39.000Z",
      "source": "DeepMind Blog",
      "sourceUrl": "https://deepmind.google/discover/blog/gemini-25-updates-to-our-family-of-thinking-models/",
      "category": "research",
      "company": "Google",
      "imageUrl": null,
      "tags": [
        "Gemini",
        "AI"
      ],
      "featured": false
    },
    {
      "id": "mcp58o2v1qsvbrwrpj7",
      "title": "We’re expanding our Gemini 2.5 family of models",
      "summary": "Gemini 2.5 Flash and Pro are now generally available, and we’re introducing 2.5 Flash-Lite, our most cost-efficient and fastest 2.5 model yet.",
      "content": "Gemini 2.5 Flash and Pro are now generally available, and we’re introducing 2.5 Flash-Lite, our most cost-efficient and fastest 2.5 model yet.",
      "publishedAt": "2025-06-17T16:01:00.000Z",
      "source": "DeepMind Blog",
      "sourceUrl": "https://deepmind.google/discover/blog/were-expanding-our-gemini-25-family-of-models/",
      "category": "research",
      "company": "Google",
      "imageUrl": null,
      "tags": [
        "Gemini",
        "AI"
      ],
      "featured": false
    },
    {
      "id": "mcp58hi3pnlbyc6h9jh",
      "title": "Groq on Hugging Face Inference Providers 🔥",
      "summary": "",
      "content": "",
      "publishedAt": "2025-06-16T00:00:00.000Z",
      "source": "Hugging Face Blog",
      "sourceUrl": "https://huggingface.co/blog/inference-providers-groq",
      "category": "tools",
      "company": "Hugging Face",
      "imageUrl": null,
      "tags": [],
      "featured": false
    },
    {
      "id": "mcp58o2vvogoqgz2be8",
      "title": "Behind “ANCESTRA”: combining Veo with live-action filmmaking",
      "summary": "We partnered with Darren Aronofsky, Eliza McNitt and a team of more than 200 people to make a film using Veo and live-action filmmaking.",
      "content": "We partnered with Darren Aronofsky, Eliza McNitt and a team of more than 200 people to make a film using Veo and live-action filmmaking.",
      "publishedAt": "2025-06-13T13:30:00.000Z",
      "source": "DeepMind Blog",
      "sourceUrl": "https://deepmind.google/discover/blog/behind-ancestra-combining-veo-with-live-action-filmmaking/",
      "category": "research",
      "company": "Google",
      "imageUrl": null,
      "tags": [],
      "featured": false
    },
    {
      "id": "mcp597bd5757erq8i5e",
      "title": "The AFRL IWSLT 2018 Systems: What Worked, What Didn’t",
      "summary": "This report summarizes the Air Force Research Laboratory (AFRL) machine translation (MT) and automatic speech recognition (ASR) systems submitted to the spoken language translation (SLT) and low-resource MT tasks as part of the IWSLT18 evaluation campaign....",
      "content": "This report summarizes the Air Force Research Laboratory (AFRL) machine translation (MT) and automatic speech recognition (ASR) systems submitted to the spoken language translation (SLT) and low-resource MT tasks as part of the IWSLT18 evaluation campaign.",
      "publishedAt": "1970-01-01T00:00:00.000Z",
      "source": "Papers with Code",
      "sourceUrl": "https://aclanthology.org/2018.iwslt-1.18.pdf",
      "category": "research",
      "company": "Community",
      "imageUrl": null,
      "tags": [
        "AI"
      ],
      "featured": false
    },
    {
      "id": "mcp597bdjnhf5gye2x",
      "title": "How to leverage the multimodal EHR data for better medical prediction?",
      "summary": "Healthcare is becoming a more and more important research topic recently. With the growing data in the healthcare domain, it offers a great opportunity for deep learning to improve the quality of service and reduce costs. However, the complexity of electronic health records (EHR) data is a challenge...",
      "content": "Healthcare is becoming a more and more important research topic recently. With the growing data in the healthcare domain, it offers a great opportunity for deep learning to improve the quality of service and reduce costs. However, the complexity of electronic health records (EHR) data is a challenge for the application of deep learning. Specifically, the data produced in the hospital admissions are monitored by the EHR system, which includes structured data like daily body temperature and unstructured data like free text and laboratory measurements. Although there are some preprocessing frameworks proposed for specific EHR data, the clinical notes that contain significant clinical value are beyond the realm of their consideration. Besides, whether these different data from various views are all beneficial to the medical tasks and how to best utilize these data remain unclear. Therefore, in this paper, we first extract the accompanying clinical notes from EHR and propose a method to integrate these data, we also comprehensively study the different models and the data leverage methods for better medical task prediction performance. The results on two prediction tasks show that our fused model with different data outperforms the state-of-the-art method without clinical notes, which illustrates the importance of our fusion method and the clinical note features.",
      "publishedAt": "1970-01-01T00:00:00.000Z",
      "source": "Papers with Code",
      "sourceUrl": "https://aclanthology.org/2021.emnlp-main.329.pdf",
      "category": "research",
      "company": "Community",
      "imageUrl": null,
      "tags": [
        "AI",
        "Deep Learning"
      ],
      "featured": false
    }
  ],
  "communityArticles": [
    {
      "id": "mcp58yaquufxk7tj5vs",
      "title": "発見: lab1702/ubuntu-ansible - Ansible config for Debian or Ubuntu",
      "summary": "新しいプロジェクトを発見: Ansible config for Debian or Ubuntu (⭐0 | 🍴0)",
      "content": "Ansible config for Debian or Ubuntu\n\n言語: N/A\nスター数: 0\nフォーク数: 0",
      "publishedAt": "2025-07-04T18:24:27.000Z",
      "source": "GitHub Search",
      "sourceUrl": "https://github.com/lab1702/ubuntu-ansible",
      "category": "community",
      "company": "Community",
      "imageUrl": null,
      "tags": [],
      "featured": false,
      "showOnTopPage": false
    },
    {
      "id": "mcp58z7qgl1wmphss7e",
      "title": "発見: dsikar/llm-benchmark - Benchmarking private vs public LLMs",
      "summary": "新しいプロジェクトを発見: Benchmarking private vs public LLMs (⭐0 | 🍴0)",
      "content": "Benchmarking private vs public LLMs\n\n言語: Python\nスター数: 0\nフォーク数: 0",
      "publishedAt": "2025-07-04T18:24:25.000Z",
      "source": "GitHub Search",
      "sourceUrl": "https://github.com/dsikar/llm-benchmark",
      "category": "community",
      "company": "Community",
      "imageUrl": null,
      "tags": [
        "LLM"
      ],
      "featured": false,
      "showOnTopPage": false
    },
    {
      "id": "mcp58xd8xcximr1ekmp",
      "title": "発見: Hussein1147/volatility-trading-bot - Automated volatility trading bot using Alpaca API and Claude AI for credit sprea",
      "summary": "新しいプロジェクトを発見: Automated volatility trading bot using Alpaca API and Claude AI for credit spread strategies (⭐0 | 🍴0)",
      "content": "Automated volatility trading bot using Alpaca API and Claude AI for credit spread strategies\n\n言語: Python\nスター数: 0\nフォーク数: 0",
      "publishedAt": "2025-07-04T18:24:24.000Z",
      "source": "GitHub Search",
      "sourceUrl": "https://github.com/Hussein1147/volatility-trading-bot",
      "category": "community",
      "company": "Community",
      "imageUrl": null,
      "tags": [
        "Claude",
        "AI"
      ],
      "featured": false,
      "showOnTopPage": false
    },
    {
      "id": "mcp58xd8hjzcvrre8vk",
      "title": "発見: MeltedMindz/Trickster - GitHub新プロジェクト",
      "summary": "新しいプロジェクトを発見: GitHub上の注目プロジェクト (⭐1 | 🍴1)",
      "content": "\n\n言語: Python\nスター数: 1\nフォーク数: 1",
      "publishedAt": "2025-07-04T18:24:21.000Z",
      "source": "GitHub Search",
      "sourceUrl": "https://github.com/MeltedMindz/Trickster",
      "category": "community",
      "company": "Community",
      "imageUrl": null,
      "tags": [],
      "featured": false,
      "showOnTopPage": false
    },
    {
      "id": "mcp58xd8j5pjzlbo7j9",
      "title": "発見: extremedonkey/castbot - Discord bot for managing casting related affairs.",
      "summary": "新しいプロジェクトを発見: Discord bot for managing casting related affairs. (⭐0 | 🍴0)",
      "content": "Discord bot for managing casting related affairs.\n\n言語: JavaScript\nスター数: 0\nフォーク数: 0",
      "publishedAt": "2025-07-04T18:24:18.000Z",
      "source": "GitHub Search",
      "sourceUrl": "https://github.com/extremedonkey/castbot",
      "category": "community",
      "company": "Community",
      "imageUrl": null,
      "tags": [
        "AI"
      ],
      "featured": false,
      "showOnTopPage": false
    },
    {
      "id": "mcp58z7qe8uzdsnhjlv",
      "title": "発見: johnbean393/SVGBench - SVGBench: A challenging contamination-free LLM benchmark that tests knowledge, c",
      "summary": "新しいプロジェクトを発見: SVGBench: A challenging contamination-free LLM benchmark that tests knowledge, coding, physical reasoning capabilities of LLMs. (⭐0 | 🍴0)",
      "content": "SVGBench: A challenging contamination-free LLM benchmark that tests knowledge, coding, physical reasoning capabilities of LLMs.\n\n言語: N/A\nスター数: 0\nフォーク数: 0",
      "publishedAt": "2025-07-04T18:24:07.000Z",
      "source": "GitHub Search",
      "sourceUrl": "https://github.com/johnbean393/SVGBench",
      "category": "community",
      "company": "Community",
      "imageUrl": null,
      "tags": [
        "LLM"
      ],
      "featured": false,
      "showOnTopPage": false
    },
    {
      "id": "mcp58xd8e8vlegfnvni",
      "title": "発見: zkjiang12/second_brainv0 - building second brain from data recorded by AI glasses",
      "summary": "新しいプロジェクトを発見: building second brain from data recorded by AI glasses (⭐0 | 🍴0)",
      "content": "building second brain from data recorded by AI glasses\n\n言語: Python\nスター数: 0\nフォーク数: 0",
      "publishedAt": "2025-07-04T18:24:06.000Z",
      "source": "GitHub Search",
      "sourceUrl": "https://github.com/zkjiang12/second_brainv0",
      "category": "community",
      "company": "Community",
      "imageUrl": null,
      "tags": [
        "AI"
      ],
      "featured": false,
      "showOnTopPage": false
    },
    {
      "id": "mcp58xd89pbqcn47dmg",
      "title": "発見: akitectio/aki-ui - A modern React UI component library built with TypeScript and Tailwind CS",
      "summary": "新しいプロジェクトを発見: A modern React UI component library built with TypeScript and Tailwind CS (⭐3 | 🍴1)",
      "content": "A modern React UI component library built with TypeScript and Tailwind CS\n\n言語: TypeScript\nスター数: 3\nフォーク数: 1",
      "publishedAt": "2025-07-04T18:24:05.000Z",
      "source": "GitHub Search",
      "sourceUrl": "https://github.com/akitectio/aki-ui",
      "category": "community",
      "company": "Community",
      "imageUrl": null,
      "tags": [
        "AI"
      ],
      "featured": false,
      "showOnTopPage": false
    },
    {
      "id": "mcp58z7qk81epx5wgg",
      "title": "発見: shubhs1908/ai-travel-planner - A Streamlit-based AI travel planner that generates personalized itineraries, rec",
      "summary": "新しいプロジェクトを発見: A Streamlit-based AI travel planner that generates personalized itineraries, recommends attractions, and provides travel insights using APIs and large language models. (⭐0 | 🍴0)",
      "content": "A Streamlit-based AI travel planner that generates personalized itineraries, recommends attractions, and provides travel insights using APIs and large language models.\n\n言語: N/A\nスター数: 0\nフォーク数: 0",
      "publishedAt": "2025-07-04T18:24:05.000Z",
      "source": "GitHub Search",
      "sourceUrl": "https://github.com/shubhs1908/ai-travel-planner",
      "category": "community",
      "company": "Community",
      "imageUrl": null,
      "tags": [
        "AI",
        "ML"
      ],
      "featured": false,
      "showOnTopPage": false
    },
    {
      "id": "mcp58yaq9vdyagsz6qe",
      "title": "発見: hlsitechio/noteai-horizon-suite - GitHub新プロジェクト",
      "summary": "新しいプロジェクトを発見: GitHub上の注目プロジェクト (⭐0 | 🍴0)",
      "content": "\n\n言語: TypeScript\nスター数: 0\nフォーク数: 0",
      "publishedAt": "2025-07-04T18:24:00.000Z",
      "source": "GitHub Search",
      "sourceUrl": "https://github.com/hlsitechio/noteai-horizon-suite",
      "category": "community",
      "company": "Community",
      "imageUrl": null,
      "tags": [
        "AI"
      ],
      "featured": false,
      "showOnTopPage": false
    },
    {
      "id": "mcp58yaqhmcm6jz2nff",
      "title": "発見: Varadpensalwar/Varadpensalwar - GitHub新プロジェクト",
      "summary": "新しいプロジェクトを発見: GitHub上の注目プロジェクト (⭐3 | 🍴0)",
      "content": "\n\n言語: N/A\nスター数: 3\nフォーク数: 0",
      "publishedAt": "2025-07-04T18:23:51.000Z",
      "source": "GitHub Search",
      "sourceUrl": "https://github.com/Varadpensalwar/Varadpensalwar",
      "category": "community",
      "company": "Community",
      "imageUrl": null,
      "tags": [],
      "featured": false,
      "showOnTopPage": false
    },
    {
      "id": "mcp58z7q5ytgcu3nb6f",
      "title": "発見: joshuabenedict-665/isro-geo-llm - GitHub新プロジェクト",
      "summary": "新しいプロジェクトを発見: GitHub上の注目プロジェクト (⭐0 | 🍴0)",
      "content": "\n\n言語: Python\nスター数: 0\nフォーク数: 0",
      "publishedAt": "2025-07-04T18:23:43.000Z",
      "source": "GitHub Search",
      "sourceUrl": "https://github.com/joshuabenedict-665/isro-geo-llm",
      "category": "community",
      "company": "Community",
      "imageUrl": null,
      "tags": [
        "LLM"
      ],
      "featured": false,
      "showOnTopPage": false
    },
    {
      "id": "mcp58yaqjb6639um1v",
      "title": "発見: kortix-ai/suna - Suna - Open Source Generalist AI Agent",
      "summary": "新しいプロジェクトを発見: Suna - Open Source Generalist AI Agent (⭐16341 | 🍴2545)",
      "content": "Suna - Open Source Generalist AI Agent\n\n言語: TypeScript\nスター数: 16341\nフォーク数: 2545",
      "publishedAt": "2025-07-04T18:23:36.000Z",
      "source": "GitHub Search",
      "sourceUrl": "https://github.com/kortix-ai/suna",
      "category": "community",
      "company": "Community",
      "imageUrl": null,
      "tags": [
        "AI"
      ],
      "featured": false,
      "showOnTopPage": false
    },
    {
      "id": "mcp58z7ql1cuorjnuck",
      "title": "発見: vivek-devCodes/AI-code-review - 📌 AI Code Review – Automated Code Quality Feedback using AI AI Code Review is a",
      "summary": "新しいプロジェクトを発見: 📌 AI Code Review – Automated Code Quality Feedback using AI AI Code Review is an intelligent GitHub-integrated tool that automatically reviews pull requests, detects code smells, provides suggestions, and ensures best practices using advanced AI/LLM-based models. (⭐0 | 🍴0)",
      "content": "📌 AI Code Review – Automated Code Quality Feedback using AI AI Code Review is an intelligent GitHub-integrated tool that automatically reviews pull requests, detects code smells, provides suggestions, and ensures best practices using advanced AI/LLM-based models.\n\n言語: PHP\nスター数: 0\nフォーク数: 0",
      "publishedAt": "2025-07-04T18:23:02.000Z",
      "source": "GitHub Search",
      "sourceUrl": "https://github.com/vivek-devCodes/AI-code-review",
      "category": "community",
      "company": "Community",
      "imageUrl": null,
      "tags": [
        "LLM",
        "AI"
      ],
      "featured": false,
      "showOnTopPage": false
    }
  ],
  "githubArticles": [
    {
      "id": "mcp58yaquufxk7tj5vs",
      "title": "発見: lab1702/ubuntu-ansible - Ansible config for Debian or Ubuntu",
      "summary": "新しいプロジェクトを発見: Ansible config for Debian or Ubuntu (⭐0 | 🍴0)",
      "content": "Ansible config for Debian or Ubuntu\n\n言語: N/A\nスター数: 0\nフォーク数: 0",
      "publishedAt": "2025-07-04T18:24:27.000Z",
      "source": "GitHub Search",
      "sourceUrl": "https://github.com/lab1702/ubuntu-ansible",
      "category": "community",
      "company": "Community",
      "imageUrl": null,
      "tags": [],
      "featured": false,
      "showOnTopPage": false
    },
    {
      "id": "mcp58z7qgl1wmphss7e",
      "title": "発見: dsikar/llm-benchmark - Benchmarking private vs public LLMs",
      "summary": "新しいプロジェクトを発見: Benchmarking private vs public LLMs (⭐0 | 🍴0)",
      "content": "Benchmarking private vs public LLMs\n\n言語: Python\nスター数: 0\nフォーク数: 0",
      "publishedAt": "2025-07-04T18:24:25.000Z",
      "source": "GitHub Search",
      "sourceUrl": "https://github.com/dsikar/llm-benchmark",
      "category": "community",
      "company": "Community",
      "imageUrl": null,
      "tags": [
        "LLM"
      ],
      "featured": false,
      "showOnTopPage": false
    },
    {
      "id": "mcp5908uhuoyldqcyn7",
      "title": "Anthropic: claude-code-actionの最新アップデート",
      "summary": "claude-code-actionが更新されました (⭐1501)",
      "content": "\n\n最終更新: 7/4/2025\nスター数: 1501",
      "publishedAt": "2025-07-04T18:24:25.000Z",
      "source": "Anthropic GitHub Org",
      "sourceUrl": "https://github.com/anthropics/claude-code-action",
      "category": "tools",
      "company": "Anthropic",
      "imageUrl": null,
      "tags": [
        "Claude"
      ],
      "featured": true
    },
    {
      "id": "mcp58xd8xcximr1ekmp",
      "title": "発見: Hussein1147/volatility-trading-bot - Automated volatility trading bot using Alpaca API and Claude AI for credit sprea",
      "summary": "新しいプロジェクトを発見: Automated volatility trading bot using Alpaca API and Claude AI for credit spread strategies (⭐0 | 🍴0)",
      "content": "Automated volatility trading bot using Alpaca API and Claude AI for credit spread strategies\n\n言語: Python\nスター数: 0\nフォーク数: 0",
      "publishedAt": "2025-07-04T18:24:24.000Z",
      "source": "GitHub Search",
      "sourceUrl": "https://github.com/Hussein1147/volatility-trading-bot",
      "category": "community",
      "company": "Community",
      "imageUrl": null,
      "tags": [
        "Claude",
        "AI"
      ],
      "featured": false,
      "showOnTopPage": false
    },
    {
      "id": "mcp58xd8hjzcvrre8vk",
      "title": "発見: MeltedMindz/Trickster - GitHub新プロジェクト",
      "summary": "新しいプロジェクトを発見: GitHub上の注目プロジェクト (⭐1 | 🍴1)",
      "content": "\n\n言語: Python\nスター数: 1\nフォーク数: 1",
      "publishedAt": "2025-07-04T18:24:21.000Z",
      "source": "GitHub Search",
      "sourceUrl": "https://github.com/MeltedMindz/Trickster",
      "category": "community",
      "company": "Community",
      "imageUrl": null,
      "tags": [],
      "featured": false,
      "showOnTopPage": false
    },
    {
      "id": "mcp58xd8j5pjzlbo7j9",
      "title": "発見: extremedonkey/castbot - Discord bot for managing casting related affairs.",
      "summary": "新しいプロジェクトを発見: Discord bot for managing casting related affairs. (⭐0 | 🍴0)",
      "content": "Discord bot for managing casting related affairs.\n\n言語: JavaScript\nスター数: 0\nフォーク数: 0",
      "publishedAt": "2025-07-04T18:24:18.000Z",
      "source": "GitHub Search",
      "sourceUrl": "https://github.com/extremedonkey/castbot",
      "category": "community",
      "company": "Community",
      "imageUrl": null,
      "tags": [
        "AI"
      ],
      "featured": false,
      "showOnTopPage": false
    },
    {
      "id": "mcp58z7qe8uzdsnhjlv",
      "title": "発見: johnbean393/SVGBench - SVGBench: A challenging contamination-free LLM benchmark that tests knowledge, c",
      "summary": "新しいプロジェクトを発見: SVGBench: A challenging contamination-free LLM benchmark that tests knowledge, coding, physical reasoning capabilities of LLMs. (⭐0 | 🍴0)",
      "content": "SVGBench: A challenging contamination-free LLM benchmark that tests knowledge, coding, physical reasoning capabilities of LLMs.\n\n言語: N/A\nスター数: 0\nフォーク数: 0",
      "publishedAt": "2025-07-04T18:24:07.000Z",
      "source": "GitHub Search",
      "sourceUrl": "https://github.com/johnbean393/SVGBench",
      "category": "community",
      "company": "Community",
      "imageUrl": null,
      "tags": [
        "LLM"
      ],
      "featured": false,
      "showOnTopPage": false
    },
    {
      "id": "mcp58xd8e8vlegfnvni",
      "title": "発見: zkjiang12/second_brainv0 - building second brain from data recorded by AI glasses",
      "summary": "新しいプロジェクトを発見: building second brain from data recorded by AI glasses (⭐0 | 🍴0)",
      "content": "building second brain from data recorded by AI glasses\n\n言語: Python\nスター数: 0\nフォーク数: 0",
      "publishedAt": "2025-07-04T18:24:06.000Z",
      "source": "GitHub Search",
      "sourceUrl": "https://github.com/zkjiang12/second_brainv0",
      "category": "community",
      "company": "Community",
      "imageUrl": null,
      "tags": [
        "AI"
      ],
      "featured": false,
      "showOnTopPage": false
    },
    {
      "id": "mcp58xd89pbqcn47dmg",
      "title": "発見: akitectio/aki-ui - A modern React UI component library built with TypeScript and Tailwind CS",
      "summary": "新しいプロジェクトを発見: A modern React UI component library built with TypeScript and Tailwind CS (⭐3 | 🍴1)",
      "content": "A modern React UI component library built with TypeScript and Tailwind CS\n\n言語: TypeScript\nスター数: 3\nフォーク数: 1",
      "publishedAt": "2025-07-04T18:24:05.000Z",
      "source": "GitHub Search",
      "sourceUrl": "https://github.com/akitectio/aki-ui",
      "category": "community",
      "company": "Community",
      "imageUrl": null,
      "tags": [
        "AI"
      ],
      "featured": false,
      "showOnTopPage": false
    },
    {
      "id": "mcp58z7qk81epx5wgg",
      "title": "発見: shubhs1908/ai-travel-planner - A Streamlit-based AI travel planner that generates personalized itineraries, rec",
      "summary": "新しいプロジェクトを発見: A Streamlit-based AI travel planner that generates personalized itineraries, recommends attractions, and provides travel insights using APIs and large language models. (⭐0 | 🍴0)",
      "content": "A Streamlit-based AI travel planner that generates personalized itineraries, recommends attractions, and provides travel insights using APIs and large language models.\n\n言語: N/A\nスター数: 0\nフォーク数: 0",
      "publishedAt": "2025-07-04T18:24:05.000Z",
      "source": "GitHub Search",
      "sourceUrl": "https://github.com/shubhs1908/ai-travel-planner",
      "category": "community",
      "company": "Community",
      "imageUrl": null,
      "tags": [
        "AI",
        "ML"
      ],
      "featured": false,
      "showOnTopPage": false
    },
    {
      "id": "mcp58yaq9vdyagsz6qe",
      "title": "発見: hlsitechio/noteai-horizon-suite - GitHub新プロジェクト",
      "summary": "新しいプロジェクトを発見: GitHub上の注目プロジェクト (⭐0 | 🍴0)",
      "content": "\n\n言語: TypeScript\nスター数: 0\nフォーク数: 0",
      "publishedAt": "2025-07-04T18:24:00.000Z",
      "source": "GitHub Search",
      "sourceUrl": "https://github.com/hlsitechio/noteai-horizon-suite",
      "category": "community",
      "company": "Community",
      "imageUrl": null,
      "tags": [
        "AI"
      ],
      "featured": false,
      "showOnTopPage": false
    },
    {
      "id": "mcp58yaqhmcm6jz2nff",
      "title": "発見: Varadpensalwar/Varadpensalwar - GitHub新プロジェクト",
      "summary": "新しいプロジェクトを発見: GitHub上の注目プロジェクト (⭐3 | 🍴0)",
      "content": "\n\n言語: N/A\nスター数: 3\nフォーク数: 0",
      "publishedAt": "2025-07-04T18:23:51.000Z",
      "source": "GitHub Search",
      "sourceUrl": "https://github.com/Varadpensalwar/Varadpensalwar",
      "category": "community",
      "company": "Community",
      "imageUrl": null,
      "tags": [],
      "featured": false,
      "showOnTopPage": false
    },
    {
      "id": "mcp58z7q5ytgcu3nb6f",
      "title": "発見: joshuabenedict-665/isro-geo-llm - GitHub新プロジェクト",
      "summary": "新しいプロジェクトを発見: GitHub上の注目プロジェクト (⭐0 | 🍴0)",
      "content": "\n\n言語: Python\nスター数: 0\nフォーク数: 0",
      "publishedAt": "2025-07-04T18:23:43.000Z",
      "source": "GitHub Search",
      "sourceUrl": "https://github.com/joshuabenedict-665/isro-geo-llm",
      "category": "community",
      "company": "Community",
      "imageUrl": null,
      "tags": [
        "LLM"
      ],
      "featured": false,
      "showOnTopPage": false
    },
    {
      "id": "mcp58yaqjb6639um1v",
      "title": "発見: kortix-ai/suna - Suna - Open Source Generalist AI Agent",
      "summary": "新しいプロジェクトを発見: Suna - Open Source Generalist AI Agent (⭐16341 | 🍴2545)",
      "content": "Suna - Open Source Generalist AI Agent\n\n言語: TypeScript\nスター数: 16341\nフォーク数: 2545",
      "publishedAt": "2025-07-04T18:23:36.000Z",
      "source": "GitHub Search",
      "sourceUrl": "https://github.com/kortix-ai/suna",
      "category": "community",
      "company": "Community",
      "imageUrl": null,
      "tags": [
        "AI"
      ],
      "featured": false,
      "showOnTopPage": false
    },
    {
      "id": "mcp58z7ql1cuorjnuck",
      "title": "発見: vivek-devCodes/AI-code-review - 📌 AI Code Review – Automated Code Quality Feedback using AI AI Code Review is a",
      "summary": "新しいプロジェクトを発見: 📌 AI Code Review – Automated Code Quality Feedback using AI AI Code Review is an intelligent GitHub-integrated tool that automatically reviews pull requests, detects code smells, provides suggestions, and ensures best practices using advanced AI/LLM-based models. (⭐0 | 🍴0)",
      "content": "📌 AI Code Review – Automated Code Quality Feedback using AI AI Code Review is an intelligent GitHub-integrated tool that automatically reviews pull requests, detects code smells, provides suggestions, and ensures best practices using advanced AI/LLM-based models.\n\n言語: PHP\nスター数: 0\nフォーク数: 0",
      "publishedAt": "2025-07-04T18:23:02.000Z",
      "source": "GitHub Search",
      "sourceUrl": "https://github.com/vivek-devCodes/AI-code-review",
      "category": "community",
      "company": "Community",
      "imageUrl": null,
      "tags": [
        "LLM",
        "AI"
      ],
      "featured": false,
      "showOnTopPage": false
    },
    {
      "id": "mcp5909848u57x7trpy",
      "title": "Anthropic: prompt-eng-interactive-tutorialの最新アップデート",
      "summary": "Anthropic's Interactive Prompt Engineering Tutorialが更新されました (⭐14205)",
      "content": "Anthropic's Interactive Prompt Engineering Tutorial\n\n最終更新: 7/4/2025\nスター数: 14205",
      "publishedAt": "2025-07-04T17:50:10.000Z",
      "source": "Anthropic GitHub Org",
      "sourceUrl": "https://github.com/anthropics/prompt-eng-interactive-tutorial",
      "category": "tools",
      "company": "Anthropic",
      "imageUrl": null,
      "tags": [
        "Anthropic"
      ],
      "featured": true
    },
    {
      "id": "mcp590983ind0oy9x5q",
      "title": "Anthropic: dxtの最新アップデート",
      "summary": "Desktop Extensions: One-click local MCP server installation in desktop appsが更新されました (⭐748)",
      "content": "Desktop Extensions: One-click local MCP server installation in desktop apps\n\n最終更新: 7/4/2025\nスター数: 748",
      "publishedAt": "2025-07-04T17:20:36.000Z",
      "source": "Anthropic GitHub Org",
      "sourceUrl": "https://github.com/anthropics/dxt",
      "category": "tools",
      "company": "Anthropic",
      "imageUrl": null,
      "tags": [],
      "featured": true
    },
    {
      "id": "mcp59098otgjql6kls",
      "title": "Anthropic: anthropic-cookbookの最新アップデート",
      "summary": "A collection of notebooks/recipes showcasing some fun and effective ways of using Claude.が更新されました (⭐17558)",
      "content": "A collection of notebooks/recipes showcasing some fun and effective ways of using Claude.\n\n最終更新: 7/4/2025\nスター数: 17558",
      "publishedAt": "2025-07-04T17:13:18.000Z",
      "source": "Anthropic GitHub Org",
      "sourceUrl": "https://github.com/anthropics/anthropic-cookbook",
      "category": "tools",
      "company": "Anthropic",
      "imageUrl": null,
      "tags": [
        "Claude",
        "Anthropic"
      ],
      "featured": true
    },
    {
      "id": "mcp5909891xk7fonabp",
      "title": "Anthropic: coursesの最新アップデート",
      "summary": "Anthropic's educational coursesが更新されました (⭐16268)",
      "content": "Anthropic's educational courses\n\n最終更新: 7/4/2025\nスター数: 16268",
      "publishedAt": "2025-07-04T16:26:08.000Z",
      "source": "Anthropic GitHub Org",
      "sourceUrl": "https://github.com/anthropics/courses",
      "category": "tools",
      "company": "Anthropic",
      "imageUrl": null,
      "tags": [
        "Anthropic"
      ],
      "featured": true
    },
    {
      "id": "mcp58toegdw18e3zzjs",
      "title": "Google: cookbookの最新アップデート",
      "summary": "cookbookリポジトリに新しい更新: Getting started with adk (#710)\n\nAdds a first ADK example notebook \n* Introduces basic ADK usage for stateful workflows involving Gemini.\n* Demonstrates core components (Agent, Tool, State, Runner) in...",
      "content": "Getting started with adk (#710)\n\nAdds a first ADK example notebook \n* Introduces basic ADK usage for stateful workflows involving Gemini.\n* Demonstrates core components (Agent, Tool, State, Runner) in action.\n* Serves as the initial example for the google-adk section.",
      "publishedAt": "2025-07-04T12:36:21.000Z",
      "source": "Google GitHub",
      "sourceUrl": "https://github.com/google-gemini/cookbook",
      "category": "tools",
      "company": "Google",
      "imageUrl": null,
      "tags": [
        "Gemini",
        "Google"
      ],
      "featured": false
    },
    {
      "id": "mcp58wf1zfq6q8lfac",
      "title": "Hugging Face: Patch Release v4.53.1",
      "summary": "This patch contains several bug fixes. The following commits are included:\r\n\r\n- Fix: unprotected import of tp plugin (#39083)\r\n- Fix key mapping for VLMs (#39029)\r\n- Several fixes for Gemma3n(#39135)\r\n- [qwen2-vl] fix FA2 inference (#39121)\r\n- [smolvlm] fix video inference (#39147)\r\n- Fix multimodal...",
      "content": "This patch contains several bug fixes. The following commits are included:\r\n\r\n- Fix: unprotected import of tp plugin (#39083)\r\n- Fix key mapping for VLMs (#39029)\r\n- Several fixes for Gemma3n(#39135)\r\n- [qwen2-vl] fix FA2 inference (#39121)\r\n- [smolvlm] fix video inference (#39147)\r\n- Fix multimodal processor get duplicate arguments when receive kwargs for initialization (#39125)\r\n- when delaying optimizer creation only prepare the model (#39152)\r\n- Add packed tensor format support for flex/sdpa/eager through the mask! (#39194)",
      "publishedAt": "2025-07-04T08:29:22.000Z",
      "source": "Hugging Face GitHub",
      "sourceUrl": "https://github.com/huggingface/transformers/releases/tag/v4.53.1",
      "category": "tools",
      "company": "Hugging Face",
      "imageUrl": null,
      "tags": [
        "AI"
      ],
      "featured": false
    },
    {
      "id": "mcp58pyo0y8bw8nm9fxc",
      "title": "Anthropic: claude-codeの最新アップデート",
      "summary": "claude-codeリポジトリに新しい更新: chore: Update CHANGELOG.md...",
      "content": "chore: Update CHANGELOG.md",
      "publishedAt": "2025-07-03T21:09:58.000Z",
      "source": "Anthropic GitHub",
      "sourceUrl": "https://github.com/anthropics/claude-code",
      "category": "tools",
      "company": "Anthropic",
      "imageUrl": null,
      "tags": [
        "Claude"
      ],
      "featured": false
    },
    {
      "id": "mcp58oze3ici2se69uc",
      "title": "Anthropic: v0.57.1",
      "summary": "## 0.57.1 (2025-07-03)\n\nFull Changelog: [v0.57.0...v0.57.1](https://github.com/anthropics/anthropic-sdk-python/compare/v0.57.0...v0.57.1)\n\n### Chores\n\n* **api:** update BetaCitationSearchResultLocation ([e0735b4](https://github.com/anthropics/anthropic-sdk-python/commit/e0735b45216fc97866492bf2fff50...",
      "content": "## 0.57.1 (2025-07-03)\n\nFull Changelog: [v0.57.0...v0.57.1](https://github.com/anthropics/anthropic-sdk-python/compare/v0.57.0...v0.57.1)\n\n### Chores\n\n* **api:** update BetaCitationSearchResultLocation ([e0735b4](https://github.com/anthropics/anthropic-sdk-python/commit/e0735b45216fc97866492bf2fff50ea7bc9768ef))\n* **internal:** version bump ([d368831](https://github.com/anthropics/anthropic-sdk-python/commit/d3688311d7b175986cff8e87ccc6e4d3159e43f4))",
      "publishedAt": "2025-07-03T16:57:02.000Z",
      "source": "Anthropic GitHub",
      "sourceUrl": "https://github.com/anthropics/anthropic-sdk-python/releases/tag/v0.57.1",
      "category": "tools",
      "company": "Anthropic",
      "imageUrl": null,
      "tags": [
        "Anthropic"
      ],
      "featured": true
    },
    {
      "id": "mcp58ozel2pbb4s0z3",
      "title": "Anthropic: v0.57.0",
      "summary": "## 0.57.0 (2025-07-03)\n\nFull Changelog: [v0.56.0...v0.57.0](https://github.com/anthropics/anthropic-sdk-python/compare/v0.56.0...v0.57.0)\n\n### Features\n\n* **api:** add support for Search Result Content Blocks ([4896178](https://github.com/anthropics/anthropic-sdk-python/commit/4896178d23832e4c847755...",
      "content": "## 0.57.0 (2025-07-03)\n\nFull Changelog: [v0.56.0...v0.57.0](https://github.com/anthropics/anthropic-sdk-python/compare/v0.56.0...v0.57.0)\n\n### Features\n\n* **api:** add support for Search Result Content Blocks ([4896178](https://github.com/anthropics/anthropic-sdk-python/commit/4896178d23832e4c84775571e8919c690ff998a1))\n\n\n### Bug Fixes\n\n* improve timeout/network error message to be more helpful ([347fb57](https://github.com/anthropics/anthropic-sdk-python/commit/347fb57c49129ff1fdac19859eb4c80808ed0711))\n\n\n### Chores\n\n* **ci:** change upload type ([4dc4178](https://github.com/anthropics/anthropic-sdk-python/commit/4dc4178d0a1eaeafc248deac4e08cc782f778600))\n* **internal:** version bump ([363629c](https://github.com/anthropics/anthropic-sdk-python/commit/363629cbc85d1e81d1e503d224dc8c7a3d1fa113))\n* **stream:** improve get_final_text() error message ([#979](https://github.com/anthropics/anthropic-sdk-python/issues/979)) ([5ae0a33](https://github.com/anthropics/anthropic-sdk-python/commit/5ae0a3303f8369575d9ebefe5b2c45cc435facdb))\n\n\n### Documentation\n\n* fix vertex id ([f7392c7](https://github.com/anthropics/anthropic-sdk-python/commit/f7392c7789fc2d329ab63c4d2ed7ba0d1dc0c7c0))\n* fix vertex id ([92fe132](https://github.com/anthropics/anthropic-sdk-python/commit/92fe1329a9a8a31de2fe71b40c4fdd84fb033dae))\n* update model in readme ([1a4df78](https://github.com/anthropics/anthropic-sdk-python/commit/1a4df783a75589dce9826a5c0564692ed0d7d7fb))\n* update models and non-beta ([a54e65c](https://github.com/anthropics/anthropic-sdk-python/commit/a54e65c5bc9dd1ac188ea9c166943548cc6f7c08))\n* update more models ([9e3dd6a](https://github.com/anthropics/anthropic-sdk-python/commit/9e3dd6afc565a6777f96ab05a28dcf2b4b9591da))",
      "publishedAt": "2025-07-03T16:24:59.000Z",
      "source": "Anthropic GitHub",
      "sourceUrl": "https://github.com/anthropics/anthropic-sdk-python/releases/tag/v0.57.0",
      "category": "tools",
      "company": "Anthropic",
      "imageUrl": null,
      "tags": [
        "Anthropic"
      ],
      "featured": true
    },
    {
      "id": "mcp58ukzijamgs5xtb9",
      "title": "Google: v0.1.9",
      "summary": "Fast follow bug fixes from 0.1.8\r\n\r\n## Packages\r\n\r\n- https://www.npmjs.com/package/@google/gemini-cli/v/0.1.9\r\n- https://www.npmjs.com/package/@google/gemini-cli-core/v/0.1.9\r\n\r\n## What's Changed\r\n\r\n* Improve slashCommand autoCompletion logic by @sethtroisi in https://github.com/google-gemini/gemini...",
      "content": "Fast follow bug fixes from 0.1.8\r\n\r\n## Packages\r\n\r\n- https://www.npmjs.com/package/@google/gemini-cli/v/0.1.9\r\n- https://www.npmjs.com/package/@google/gemini-cli-core/v/0.1.9\r\n\r\n## What's Changed\r\n\r\n* Improve slashCommand autoCompletion logic by @sethtroisi in https://github.com/google-gemini/gemini-cli/pull/2776\r\n* Special case mime type for ts file. by @scidomino in https://github.com/google-gemini/gemini-cli/pull/2902\r\n* Fix characters being dropped in text-buffer by @bbiggs in https://github.com/google-gemini/gemini-cli/pull/2504\r\n* feat(core): Add infinite loop protection to client by @allenhutchison in https://github.com/google-gemini/gemini-cli/pull/2793\r\n* Add excludedTools to extensions. by @scidomino in https://github.com/google-gemini/gemini-cli/pull/2853\r\n* Added support for session_id in API calls by @bdmorgan in https://github.com/google-gemini/gemini-cli/pull/2886\r\n* chore: bump version to 0.1.9 by @KeijiBranshi in https://github.com/google-gemini/gemini-cli/pull/2906\r\n\r\n\r\n**Full Changelog**: https://github.com/google-gemini/gemini-cli/compare/v0.1.8...v0.1.9",
      "publishedAt": "2025-07-02T00:29:03.000Z",
      "source": "Google GitHub",
      "sourceUrl": "https://github.com/google-gemini/gemini-cli/releases/tag/v0.1.9",
      "category": "tools",
      "company": "Google",
      "imageUrl": null,
      "tags": [
        "Gemini",
        "Google"
      ],
      "featured": false
    },
    {
      "id": "mcp58ukzux7vpb8086",
      "title": "Google: v0.1.8",
      "summary": "## What's Changed\r\n* Upgrade to Ink 6 and React 19 by @SandyTao520 in https://github.com/google-gemini/gemini-cli/pull/2096\r\n* Updates error handling in case of incorrect tool calling. by @vachan-shetty in https://github.com/google-gemini/gemini-cli/pull/2304\r\n* chore: bump to 0.1.8 by @KeijiBranshi...",
      "content": "## What's Changed\r\n* Upgrade to Ink 6 and React 19 by @SandyTao520 in https://github.com/google-gemini/gemini-cli/pull/2096\r\n* Updates error handling in case of incorrect tool calling. by @vachan-shetty in https://github.com/google-gemini/gemini-cli/pull/2304\r\n* chore: bump to 0.1.8 by @KeijiBranshi in https://github.com/google-gemini/gemini-cli/pull/2308\r\n* add issue triage using gemini cli by @jerop in https://github.com/google-gemini/gemini-cli/pull/2310\r\n* improve triage prompt by @jerop in https://github.com/google-gemini/gemini-cli/pull/2314\r\n* chore: fix typo in mcp-client by @noritaka1166 in https://github.com/google-gemini/gemini-cli/pull/1555\r\n* Fix typos by @sadikkuzu in https://github.com/google-gemini/gemini-cli/pull/1629\r\n* quiet dotenv log message by @motdotla in https://github.com/google-gemini/gemini-cli/pull/2239\r\n* Inline the description and schema of the shell tool in the source by @bbiggs in https://github.com/google-gemini/gemini-cli/pull/1709\r\n* chore: add proper pluralization handling for match in grep tool by @kahlstrm in https://github.com/google-gemini/gemini-cli/pull/2344\r\n* fix: typo by @reidliu41 in https://github.com/google-gemini/gemini-cli/pull/2415\r\n* Add troubleshooting note about CI env variables by @chrstnb in https://github.com/google-gemini/gemini-cli/pull/2229\r\n* feat: add VSCodium editor support by @psinha40898 in https://github.com/google-gemini/gemini-cli/pull/2299\r\n* fix edit retrigger by @ngleo in https://github.com/google-gemini/gemini-cli/pull/2306\r\n* fix file extension in \"modify flow\" temp files by @ngleo in https://github.com/google-gemini/gemini-cli/pull/2478\r\n* Re-enable Gemini Code Assist PR review bot by @umairidris in https://github.com/google-gemini/gemini-cli/pull/2254\r\n* chore(gha): pin issue triage workflow to a specific commit by @jerop in https://github.com/google-gemini/gemini-cli/pull/2496\r\n* docs: remove duplicate tool descriptions in file-system.md by @StarkOne in https://github.com/google-gemini/gemini-cli/pull/1790\r\n* fix: Correct start script reference in create_alias.sh by @jimmyliao in https://github.com/google-gemini/gemini-cli/pull/1487\r\n* docs: fix typos and grammatical errors by @krushna-sharma in https://github.com/google-gemini/gemini-cli/pull/2459\r\n* Remove debug logs that are not actionable but numerous by @anj-s in https://github.com/google-gemini/gemini-cli/pull/2030\r\n* 📦 NEW: Theme Shades of Purple  by @ahmadawais in https://github.com/google-gemini/gemini-cli/pull/2114\r\n* Esc to exit privacy screen in error state by @scidomino in https://github.com/google-gemini/gemini-cli/pull/2527\r\n* refactor: use for...of loop instead of traditional for loop by @noritaka1166 in https://github.com/google-gemini/gemini-cli/pull/1840\r\n* refactor: remove imported multiple times by @noritaka1166 in https://github.com/google-gemini/gemini-cli/pull/1846\r\n* docs: Add uninstallation instructions to README by @Zircoz in https://github.com/google-gemini/gemini-cli/pull/1985\r\n* feat: add Neovim editor support by @yuki-yano in https://github.com/google-gemini/gemini-cli/pull/1448\r\n* Fix a broken link by @doggy8088 in https://github.com/google-gemini/gemini-cli/pull/2598\r\n* Fix a heading issue for Authentication Setup doc by @doggy8088 in https://github.com/google-gemini/gemini-cli/pull/2592\r\n* Clarify .gemini/config.yaml is for the PR review bot (not CLI). by @umairidris in https://github.com/google-gemini/gemini-cli/pull/2495\r\n* refactor: remove unnecessary assertion by @noritaka1166 in https://github.com/google-gemini/gemini-cli/pull/2579\r\n* refactor: remove unnecessary \"await\" by @noritaka1166 in https://github.com/google-gemini/gemini-cli/pull/2574\r\n* feat: allow command-specific restrictions for ShellTool by @jerop in https://github.com/google-gemini/gemini-cli/pull/2605\r\n* chore: add .editorconfig by @aspiers in https://github.com/google-gemini/gemini-cli/pull/2572\r\n* fix: Correct pluralization of the number of occurrences in `EditTool` tool errors by @timrogers in https://github.com/google-gemini/gemini-cli/pull/2463\r\n* Added obfuscated google account ID to clearcut log messages by @bdmorgan in https://github.com/google-gemini/gemini-cli/pull/2593\r\n* Fix clearcut-logger.ts to event name GEMINI_CLI_API_RESPONSE_TOOL_TOK… by @uttamkanodia14 in https://github.com/google-gemini/gemini-cli/pull/1875\r\n* feat: modular GEMINI.md imports with @file.md syntax (#1585) by @bniladridas in https://github.com/google-gemini/gemini-cli/pull/2230\r\n* fix:Update /help to show correct newline key combo for different OS #… by @devpool007 in https://github.com/google-gemini/gemini-cli/pull/2043\r\n* Highlight previous user input by @AlphaDaze in https://github.com/google-gemini/gemini-cli/pull/2507\r\n* feat(cli): Add hideTips setting by @marcinjahn in https://github.com/google-gemini/gemini-cli/pull/1524\r\n* feat: add support to remote MCP servers for custom HTTP headers by @aspiers in https://github.com/google-gemini/gemini-cli/pull/2477\r\n* feat: Change /stats to include more detailed breakdowns by @abhipatel12 in https://github.com/google-gemini/gemini-cli/pull/2615\r\n* feat(shell): Enable prefix matching for flexible command validation by @jerop in https://github.com/google-gemini/gemini-cli/pull/2653\r\n* Fix oauth credential caching. by @scidomino in https://github.com/google-gemini/gemini-cli/pull/2709\r\n* refactor(workflows): separate issue triage into two workflows by @jerop in https://github.com/google-gemini/gemini-cli/pull/2746\r\n* feat(workflows): add issues list command to automated triage workflow by @jerop in https://github.com/google-gemini/gemini-cli/pull/2749\r\n* fix(workflows): use preview release gemini-cli in triage workflows by @jerop in https://github.com/google-gemini/gemini-cli/pull/2759\r\n* Fix CODE_ASSIST_ENDPOINT env var. by @scidomino in https://github.com/google-gemini/gemini-cli/pull/2712\r\n* Removed fallback logic for gaia id logging by @owenofbrien in https://github.com/google-gemini/gemini-cli/pull/2761\r\n* Remove unused method by @scidomino in https://github.com/google-gemini/gemini-cli/pull/2721\r\n* Use structured prompt for compression. by @scidomino in https://github.com/google-gemini/gemini-cli/pull/2747\r\n* Rename AuthType LOGIN_WITH_GOOGLE_PERSONAL -> LOGIN_WITH_GOOGLE by @scidomino in https://github.com/google-gemini/gemini-cli/pull/2769\r\n* refactor(ui): revamp exit stats display by @abhipatel12 in https://github.com/google-gemini/gemini-cli/pull/2771\r\n* Fix: Use HTTPS in docs and correct formatting typo in troubleshooting guide by @Jvr2022 in https://github.com/google-gemini/gemini-cli/pull/2762\r\n* feat(triage): improve automated issue triage workflows by @jerop in https://github.com/google-gemini/gemini-cli/pull/2778\r\n* update check + tests by @eddie-santos in https://github.com/google-gemini/gemini-cli/pull/2772\r\n* docs(auth): clarify env-file discovery & recommend by @jarvisphere in https://github.com/google-gemini/gemini-cli/pull/2402\r\n* feat: Add markdown table rendering support by @heartyguy in https://github.com/google-gemini/gemini-cli/pull/1955\r\n* Make clean script cross-platform by @Mirza-Samad-Ahmed-Baig in https://github.com/google-gemini/gemini-cli/pull/1990\r\n* Fix spurious logs about invalid MaxSizedBox children due to Ink6 + React19 migration by @jacob314 in https://github.com/google-gemini/gemini-cli/pull/2794\r\n* feat: add weekly community report workflow by @jerop in https://github.com/google-gemini/gemini-cli/pull/2855\r\n* Use the constant placeholders for \".gemini/settings.json\" in gemini.tsx by @scidomino in https://github.com/google-gemini/gemini-cli/pull/2860\r\n* Update README.md by @logankilpatrick in https://github.com/google-gemini/gemini-cli/pull/2729\r\n* feat: add audio and video support to read_file by @santhoshkumarCodes in https://github.com/google-gemini/gemini-cli/pull/2556\r\n* Docs: Add a page detailing quota and cost information by @ptone in https://github.com/google-gemini/gemini-cli/pull/2894\r\n\r\n## New Contributors\r\n* @vachan-shetty made their first contribution in https://github.com/google-gemini/gemini-cli/pull/2304\r\n* @sadikkuzu made their first contribution in https://github.com/google-gemini/gemini-cli/pull/1629\r\n* @motdotla made their first contribution in https://github.com/google-gemini/gemini-cli/pull/2239\r\n* @kahlstrm made their first contribution in https://github.com/google-gemini/gemini-cli/pull/2344\r\n* @reidliu41 made their first contribution in https://github.com/google-gemini/gemini-cli/pull/2415\r\n* @chrstnb made their first contribution in https://github.com/google-gemini/gemini-cli/pull/2229\r\n* @psinha40898 made their first contribution in https://github.com/google-gemini/gemini-cli/pull/2299\r\n* @umairidris made their first contribution in https://github.com/google-gemini/gemini-cli/pull/2254\r\n* @krushna-sharma made their first contribution in https://github.com/google-gemini/gemini-cli/pull/2459\r\n* @ahmadawais made their first contribution in https://github.com/google-gemini/gemini-cli/pull/2114\r\n* @Zircoz made their first contribution in https://github.com/google-gemini/gemini-cli/pull/1985\r\n* @yuki-yano made their first contribution in https://github.com/google-gemini/gemini-cli/pull/1448\r\n* @doggy8088 made their first contribution in https://github.com/google-gemini/gemini-cli/pull/2598\r\n* @aspiers made their first contribution in https://github.com/google-gemini/gemini-cli/pull/2572\r\n* @timrogers made their first contribution in https://github.com/google-gemini/gemini-cli/pull/2463\r\n* @uttamkanodia14 made their first contribution in https://github.com/google-gemini/gemini-cli/pull/1875\r\n* @bniladridas made their first contribution in https://github.com/google-gemini/gemini-cli/pull/2230\r\n* @devpool007 made their first contribution in https://github.com/google-gemini/gemini-cli/pull/2043\r\n* @AlphaDaze made their first contribution in https://github.com/google-gemini/gemini-cli/pull/2507\r\n* @marcinjahn made their first contribution in https://github.com/google-gemini/gemini-cli/pull/1524\r\n* @Jvr2022 made their first contribution in https://github.com/google-gemini/gemini-cli/pull/2762\r\n* @jarvisphere made their first contribution in https://github.com/google-gemini/gemini-cli/pull/2402\r\n* @heartyguy made their first contribution in https://github.com/google-gemini/gemini-cli/pull/1955\r\n* @Mirza-Samad-Ahmed-Baig made their first contribution in https://github.com/google-gemini/gemini-cli/pull/1990\r\n* @santhoshkumarCodes made their first contribution in https://github.com/google-gemini/gemini-cli/pull/2556\r\n\r\n**Full Changelog**: https://github.com/google-gemini/gemini-cli/compare/v0.1.7...v0.1.8",
      "publishedAt": "2025-07-01T22:52:01.000Z",
      "source": "Google GitHub",
      "sourceUrl": "https://github.com/google-gemini/gemini-cli/releases/tag/v0.1.8",
      "category": "tools",
      "company": "Google",
      "imageUrl": null,
      "tags": [
        "Gemini",
        "AI",
        "ML",
        "Google"
      ],
      "featured": true
    },
    {
      "id": "mcp58qu24r3bnxsu9hp",
      "title": "Anthropic: v0.1.0 (Initial release)",
      "summary": "**Full Changelog**: https://github.com/anthropics/dxt/commits/v0.1.0...",
      "content": "**Full Changelog**: https://github.com/anthropics/dxt/commits/v0.1.0",
      "publishedAt": "2025-06-28T23:43:01.000Z",
      "source": "Anthropic GitHub",
      "sourceUrl": "https://github.com/anthropics/dxt/releases/tag/v0.1.0",
      "category": "tools",
      "company": "Anthropic",
      "imageUrl": null,
      "tags": [
        "Anthropic"
      ],
      "featured": false
    },
    {
      "id": "mcp58solpvlvrp1z9f",
      "title": "OpenAI: openai-cookbookの最新アップデート",
      "summary": "openai-cookbookリポジトリに新しい更新: fix images display issue for codex <> jira cookbook (#1925)...",
      "content": "fix images display issue for codex <> jira cookbook (#1925)",
      "publishedAt": "2025-06-27T13:28:01.000Z",
      "source": "OpenAI GitHub",
      "sourceUrl": "https://github.com/openai/openai-cookbook",
      "category": "tools",
      "company": "OpenAI",
      "imageUrl": null,
      "tags": [
        "AI",
        "OpenAI"
      ],
      "featured": false
    },
    {
      "id": "mcp58wf1x0wvgsqdvei",
      "title": "Hugging Face: Release v4.53.0",
      "summary": "## Release v4.53.0\r\n\r\n### Gemma3n\r\n\r\nGemma 3n models are designed for efficient execution on low-resource devices. They are capable of multimodal input, handling text, image, video, and audio input, and generating text outputs, with open weights for pre-trained and instruction-tuned variants. These ...",
      "content": "## Release v4.53.0\r\n\r\n### Gemma3n\r\n\r\nGemma 3n models are designed for efficient execution on low-resource devices. They are capable of multimodal input, handling text, image, video, and audio input, and generating text outputs, with open weights for pre-trained and instruction-tuned variants. These models were trained with data in over 140 spoken languages.\r\n\r\nGemma 3n models use selective parameter activation technology to reduce resource requirements. This technique allows the models to operate at an effective size of 2B and 4B parameters, which is lower than the total number of parameters they contain. For more information on Gemma 3n's efficient parameter management technology, see the [Gemma 3n](https://ai.google.dev/gemma/docs/gemma-3n#parameters) page.\r\n\r\n![image](https://github.com/user-attachments/assets/858cb034-364d-4eb6-8de8-4a0b5eaff3d7)\r\n\r\n```python\r\nfrom transformers import pipeline\r\nimport torch\r\n\r\npipe = pipeline(\r\n    \"image-text-to-text\",\r\n    torch_dtype=torch.bfloat16,\r\n    model=\"google/gemma-3n-e4b\",\r\n    device=\"cuda\",\r\n)\r\noutput = pipe(\r\n    \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg\",\r\n    text=\"<image_soft_token> in this image, there is\"\r\n)\r\n\r\nprint(output)\r\n```\r\n\r\n### Dia\r\n\r\n![image](https://github.com/user-attachments/assets/bf86e887-e4f4-4222-993d-f5eac58f8040)\r\n\r\nDia is an opensource text-to-speech (TTS) model (1.6B parameters) developed by [Nari Labs](https://huggingface.co/nari-labs).\r\nIt can generate highly realistic dialogue from transcript including nonverbal communications such as laughter and coughing.\r\nFurthermore, emotion and tone control is also possible via audio conditioning (voice cloning).\r\n\r\n**Model Architecture:**\r\nDia is an encoder-decoder transformer based on the original transformer architecture. However, some more modern features such as\r\nrotational positional embeddings (RoPE) are also included. For its text portion (encoder), a byte tokenizer is utilized while\r\nfor the audio portion (decoder), a pretrained codec model [DAC](./dac.md) is used - DAC encodes speech into discrete codebook\r\ntokens and decodes them back into audio.\r\n\r\n* Add Dia model  by @buttercrab in #38405\r\n\r\n### Kyutai Speech-to-Text\r\n\r\n<img src=\"https://huggingface.co/datasets/eustlb/documentation-images/resolve/main/kyutai_stt.png\"/>\r\n\r\nKyutai STT is a speech-to-text model architecture based on the [Mimi codec](https://huggingface.co/docs/transformers/en/model_doc/mimi), which encodes audio into discrete tokens in a streaming fashion, and a [Moshi-like](https://huggingface.co/docs/transformers/en/model_doc/moshi) autoregressive decoder. Kyutai’s lab has released two model checkpoints:\r\n- [kyutai/stt-1b-en_fr](https://huggingface.co/kyutai/stt-1b-en_fr): a 1B-parameter model capable of transcribing both English and French\r\n- [kyutai/stt-2.6b-en](https://huggingface.co/kyutai/stt-2.6b-en): a 2.6B-parameter model focused solely on English, optimized for maximum transcription accuracy\r\n\r\n* Add kyutai stt  by @eustlb in #38909\r\n\r\nRead more about the model in the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/stt)\r\n\r\n### V-JEPA 2\r\n\r\n<div class=\"flex justify-center\">\r\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/vjepa.gif\" alt=\"drawing\" width=\"600\"/>\r\n</div>\r\n\r\nV-JEPA 2 is a self-supervised approach to training video encoders developed by FAIR, Meta. Using internet-scale video data, V-JEPA 2 attains state-of-the-art performance on motion understanding and human action anticipation tasks. V-JEPA 2-AC is a latent action-conditioned world model post-trained from V-JEPA 2 (using a small amount of robot trajectory interaction data) that solves robot manipulation tasks without environment-specific data collection or task-specific training or calibration.\r\n\r\n* Add V-JEPA 2  by @qubvel in #38746\r\n\r\nRead more about the model in the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/vjepa2).\r\n\r\n### Arcee\r\n\r\n![image](https://github.com/user-attachments/assets/1e7b594b-9973-4a07-b30a-cea0968b081d)\r\n\r\nArcee is a decoder-only transformer model based on the Llama architecture with a key modification: it uses ReLU² (ReLU-squared) activation in the MLP blocks instead of SiLU, following recent research showing improved training efficiency with squared activations. This architecture is designed for efficient training and inference while maintaining the proven stability of the Llama design.\r\n\r\nThe Arcee model is architecturally similar to Llama but uses x * relu(x) in MLP layers for improved gradient flow and is optimized for efficiency in both training and inference scenarios.\r\n\r\n* Add Arcee model support  by @Crystalcareai in #38621\r\n\r\nRead more about the model in the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/arcee#arcee).\r\n\r\n###  ColQwen2\r\n\r\n[ColQwen2](https://doi.org/10.48550/arXiv.2407.01449) is a variant of the [ColPali](./colpali) model designed to retrieve documents by analyzing their visual features. Unlike traditional systems that rely heavily on text extraction and OCR, ColQwen2 treats each page as an image. It uses the [Qwen2-VL](./qwen2_vl) backbone to capture not only text, but also the layout, tables, charts, and other visual elements to create detailed multi-vector embeddings that can be used for retrieval by computing pairwise late interaction similarity scores. This offers a more comprehensive understanding of documents and enables more efficient and accurate retrieval.\r\n\r\n![image](https://github.com/user-attachments/assets/eb833323-675a-4858-9aa9-834d49bcff93)\r\n\r\n* Add ColQwen2 to 🤗 transformers  by @tonywu71 in #35778\r\n\r\nRead more about the model in the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/colqwen2).\r\n\r\n### MiniMax\r\n\r\n![image](https://github.com/user-attachments/assets/0e05053b-bae9-4504-b0b3-f0d7988fe995)\r\n\r\nMiniMax is a powerful language model with 456 billion total parameters, of which 45.9 billion are activated per token. To better unlock the long context capabilities of the model, MiniMax adopts a hybrid architecture that combines Lightning Attention, Softmax Attention and Mixture-of-Experts (MoE). Leveraging advanced parallel strategies and innovative compute-communication overlap methods—such as Linear Attention Sequence Parallelism Plus (LASP+), varlen ring attention, Expert Tensor Parallel (ETP), etc., MiniMax's training context length is extended to 1 million tokens, and it can handle a context of up to 4 million tokens during the inference. On various academic benchmarks, MiniMax also demonstrates the performance of a top-tier model.\r\n\r\nThe architecture of MiniMax is briefly described as follows:\r\n\r\n- Total Parameters: 456B\r\n- Activated Parameters per Token: 45.9B\r\n- Number Layers: 80\r\n- Hybrid Attention: a softmax attention is positioned after every 7 lightning attention.\r\n    - Number of attention heads: 64\r\n    - Attention head dimension: 128\r\n- Mixture of Experts:\r\n    - Number of experts: 32\r\n    - Expert hidden dimension: 9216\r\n    - Top-2 routing strategy\r\n- Positional Encoding: Rotary Position Embedding (RoPE) applied to half of the attention head dimension with a base frequency of 10,000,000\r\n- Hidden Size: 6144\r\n- Vocab Size: 200,064\r\n\r\nFor more details refer to the [release blog post](https://www.minimaxi.com/en/news/minimax-01-series-2).\r\n\r\n* Add support for MiniMax's MiniMax-Text-01  by @geetu040 in #35831\r\n\r\nRead more about the model in the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/minimax).\r\n\r\n### Encoder-Decoder Gemma\r\n\r\n![image](https://github.com/user-attachments/assets/1780e426-435c-47e3-b872-d8b0016648ce)\r\n\r\nT5Gemma (aka encoder-decoder Gemma) was proposed in a [research paper](https://arxiv.org/abs/2504.06225) by Google. It is a family of encoder-decoder large langauge models, developed by adapting pretrained decoder-only models into encoder-decoder. T5Gemma includes pretrained and instruction-tuned variants. The architecture is based on transformer encoder-decoder design following T5, with improvements from Gemma 2: GQA, RoPE, GeGLU activation, RMSNorm, and interleaved local/global attention.\r\n\r\nT5Gemma has two groups of model sizes: 1) [Gemma 2](https://ai.google.dev/gemma/docs/core/model_card_2) sizes (2B-2B, 9B-2B, and 9B-9B), which are based on the offical Gemma 2 models (2B and 9B); and 2) [T5](https://arxiv.org/abs/1910.10683) sizes (Small, Base, Large, and XL), where are pretrained under the Gemma 2 framework following T5 configuration. In addition, we also provide a model at ML size (medium large, ~2B in total), which is in-between T5 Large and T5 XL.\r\n\r\nThe pretrained varaints are trained with two objectives: prefix language modeling with knowledge distillation (PrefixLM) and UL2, separately. We release both variants for each model size. The instruction-turned varaints was post-trained with supervised fine-tuning and reinforcement learning.\r\n\r\n* Encoder-Decoder Gemma  by @bzhangGo in #38332\r\n\r\nRead more about the model in the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/t5gemma).\r\n\r\n### GLM-4.1V\r\n\r\nThe GLM-4.1V model architecture is added to transformers; no models have yet been released with that architecture. Stay tuned for the GLM team upcoming releases!\r\n\r\n* GLM-4.1V Model support  by @zRzRzRzRzRzRzR in #38431\r\n\r\nRead more about the model in the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/glm4v).\r\n\r\n### Falcon H1\r\n\r\n![image](https://github.com/user-attachments/assets/873dd344-2566-408f-8eaf-aab149acabc1)\r\n\r\nThe FalconH1 model was developed by the TII Pretraining team. A comprehensive research paper covering the architecture, pretraining dynamics, experimental results, and conclusions is forthcoming. You can read more about this series in [this website](https://github.com/tiiuae/Falcon-H1).\r\n\r\n* [MODEL] Add Falcon H1  by @younesbelkada in #38249\r\n\r\nRead more about the model in the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/falcon_h1).\r\n\r\n### LightGlue\r\n\r\n![image](https://github.com/user-attachments/assets/45ceebaa-2216-4fcd-9fcc-f05299019c6a)\r\n\r\nThe LightGlue model was proposed in [LightGlue: Local Feature Matching at Light Speed](https://arxiv.org/abs/2306.13643)\r\nby Philipp Lindenberger, Paul-Edouard Sarlin and Marc Pollefeys.\r\n\r\nSimilar to [SuperGlue](https://huggingface.co/magic-leap-community/superglue_outdoor), this model consists of matching\r\ntwo sets of local features extracted from two images, its goal is to be faster than SuperGlue. Paired with the \r\n[SuperPoint model](https://huggingface.co/magic-leap-community/superpoint), it can be used to match two images and \r\nestimate the pose between them. This model is useful for tasks such as image matching, homography estimation, etc.\r\n\r\nThe abstract from the paper is the following:\r\n\r\n*We introduce LightGlue, a deep neural network that learns to match local features across images. We revisit multiple\r\ndesign decisions of SuperGlue, the state of the art in sparse matching, and derive simple but effective improvements. \r\nCumulatively, they make LightGlue more efficient - in terms of both memory and computation, more accurate, and much\r\neasier to train. One key property is that LightGlue is adaptive to the difficulty of the problem: the inference is much\r\nfaster on image pairs that are intuitively easy to match, for example because of a larger visual overlap or limited\r\nappearance change. This opens up exciting prospects for deploying deep matchers in latency-sensitive applications like\r\n3D reconstruction. The code and trained models are publicly available at this [https URL](https://github.com/cvg/LightGlue)*\r\n\r\n* Add LightGlue model  by @sbucaille in #31718\r\n\r\nRead more about the model in the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/lightglue).\r\n\r\n### dots.llm1\r\n\r\nThe abstract from the report is the following:\r\n\r\n*Mixture of Experts (MoE) models have emerged as a promising paradigm for scaling language models efficiently by activating only a subset of parameters for each input token. In this report, we present dots.llm1, a large-scale MoE model that activates 14B parameters out of a total of 142B parameters, delivering performance on par with state-of-the-art models while reducing training and inference costs. Leveraging our meticulously crafted and efficient data processing pipeline, dots.llm1 achieves performance comparable to Qwen2.5-72B after pretraining on high-quality corpus and post-training to fully unlock its capabilities. Notably, no synthetic data is used during pretraining. To foster further research, we open-source intermediate training checkpoints spanning the entire training process, providing valuable insights into the learning dynamics of large language models.*\r\n\r\n* [Model] add dots1  by @redmoe-moutain in #38143\r\n\r\nRead more about the model in the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/dots1).\r\n\r\n### SmolLM3\r\n\r\nSmolLM3 is a fully open, compact language model designed for efficient deployment while maintaining strong performance. It uses a Transformer decoder architecture with Grouped Query Attention (GQA) to reduce the kv cache, and no RoPE, enabling improved performance on long-context tasks. It is trained using a multi-stage training approach on high-quality public datasets across web, code, and math domains. The model is multilingual and supports very large context lengths. The instruct variant is optimized for reasoning and tool use.\r\n\r\n* Add SmolLM3  by @anton-l in #38755\r\n\r\nRead more about the model in the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/smollm3).\r\n\r\n## Performance optimizations\r\n\r\n### Kernels\r\n\r\nIn previous versions, installing the `kernels` library would **automatically activate the custom kernels** added to `transformers`, because the `@use_kernel_forward_from_the_hub` decorator directly swapped out the model’s forward method. This implicit behavior caused several issues for users — including problems with `torch.compile`, non-determinism, and inconsistent outputs.\r\n\r\nTo address this, we've introduced a new **opt-in mechanism** called `kernelize`. You can now enable kernel usage explicitly by passing `use_kernels=True` to `from_pretrained`. The `use_kernel_forward_from_the_hub` decorator now simply stores the kernel name that the user wants to use — and `kernelize` handles the rest under the hood.\r\n\r\n#### Example\r\n\r\n```python\r\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\r\nimport torch\r\n\r\nmodel = AutoModelForCausalLM.from_pretrained(\r\n    \"meta-llama/Llama-3.2-1B-Instruct\",\r\n    torch_dtype=torch.bfloat16,\r\n    device_map=\"cuda\",\r\n    use_kernels=True\r\n)\r\ntokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")\r\n\r\ninput = \"Hello\"\r\ninput_ids = tokenizer(input, return_tensors=\"pt\").to(model.device).input_ids\r\noutput = model.generate(input_ids, max_new_tokens=100)\r\n\r\nprint(tokenizer.decode(output[0], skip_special_tokens=True))\r\n```\r\nMore kernels will be added over time — this will be a collaborative, community-driven effort to make transformers lighter and faster 🤗\r\n\r\n* Add kernelize to transformers  by @MekkCyber in #38205\r\n\r\n### Flash Attention 3\r\n\r\nSupport for Flash Attention 3 is added across the most popular models.\r\n\r\n* Support for Flash Attention 3  by @EduardDurech in #38972\r\n\r\n## Notable repository maintenance & refactors\r\n\r\nSeveral efforts refactoring the repository are happening in parallel. The direction is to greatly simplify the library, removing unnecessary codepaths. Whilst the efforts are spread across the library, they're particularly visible in each individual models; where non-modeling-specific code will be simplified and eventually removed.\r\n\r\nWe take the assumption that model-agnostic utilities shouldn't be in the modeling code. Things like the output of attentions, hidden states, router logits, are important for end-users but don't need to be explicitely displayed in the modeling code. \r\n\r\n* Apply GradientCheckpointingLayer to the whole repo  by @qubvel in #38913\r\n* No more Tuple, List, Dict  by @Rocketknight1 in #38797\r\n* Deprecate TF + JAX  by @Rocketknight1 in #38758\r\n\r\n## Breaking changes\r\n\r\nSeveral minimal breaking changes aiming to bring clearer defaults while greatly simplifying the library have been merged.\r\n\r\n* 🔴 Update default `dtype` for pipelines to `auto`  by @Vaibhavs10 in #38882\r\n* 🚨🚨 Fix initialization of Mask2Former  by @Cyrilvallez in #38864\r\n* :rotating_light: :rotating_light: Inherited CausalLM Tests  by @Rocketknight1 in #37590\r\n* 🚨Early-error🚨 config will error out if `output_attentions=True` and the attn implementation is wrong  by @ArthurZucker in #38288\r\n* 🔴 [VLM] modeling updates  by @zucchini-nlp in #38317\r\n* :rotating_light: :rotating_light: Fix custom code saving  by @Rocketknight1 in #37716\r\n* 🚨🚨[core] Completely rewrite the masking logic for all attentions  by @Cyrilvallez in #37866\r\n* 🔴🔴🔴 [`Attention`] Refactor Attention Interface for Bart-based Models  by @vasqu in #38108\r\n* 🔴[`Attention`] Attention refactor for Whisper-based models  by @vasqu in #38235\r\n* Add CB  by @ArthurZucker in #38085\r\n\r\n## Bugfixes and improvements\r\n\r\n* CI reporting improvements  by @ydshieh in #38230\r\n* Revert parallelism temporarily  by @LysandreJik in #38240\r\n* tp plan should not be NONE  by @ArthurZucker in #38255\r\n* [Falcon H1] Fix Typo in Integration Test  by @dhiaEddineRhaiem in #38256\r\n* [`compile`] re-enable for Qwen-VL models  by @zucchini-nlp in #38127\r\n* fix multi-image case for llava-onevision  by @cyr0930 in #38084\r\n* Add tearDown method to Quark to solve OOM issues  by @MekkCyber in #38234\r\n* Clearer error on import failure  by @LysandreJik in #38257\r\n* [whisper] small changes for faster tests  by @gante in #38236\r\n* Simplify DTensor Check for modeling_utils.py  by @amd-xiaoyu12 in #38245\r\n* Improve typing in TrainingArgument  by @cyyever in #36944\r\n* Fix: missing else branch to handle \"--load_best_model_at_end\" in training_args.py  by @danielyxyang in #38217\r\n* assign the correct torchao data layout for xpu  by @jiqing-feng in #37781\r\n* Remove Japanese sequence_classification doc and update references  by @ritsumei-aoi in #38246\r\n* Protect ParallelInterface  by @ArthurZucker in #38262\r\n* Update Model Card for Mamba  by @ParagEkbote in #37863\r\n* docs(swin): Update Swin model card to standard format  by @BryanBradfo in #37628\r\n* add XPU info print in print_env  by @yao-matrix in #38282\r\n* [whisper] move processor test into processor test file 🧹   by @gante in #38266\r\n* [Whisper] handle deprecation of `forced_decoder_ids`  by @gante in #38232\r\n* add `liger-kernel` to docker file  by @ydshieh in #38292\r\n* Fix tp error when torch distributed is already initialized  by @SunMarc in #38294\r\n* More typing in src/transformers/training_args.py  by @cyyever in #38106\r\n* refine `transformers env` output  by @yao-matrix in #38274\r\n* Update CI Docker base image for AMD tests  by @ahadnagy in #38261\r\n* Fix HybridChunedCache & Llama4  by @Cyrilvallez in #38299\r\n* Oups typo for HybridChunkedCache  by @Cyrilvallez in #38303\r\n* [Tests] Cleanup Janus Testcase  by @yaswanth19 in #38311\r\n* [emu3] fix conversion script  by @zucchini-nlp in #38297\r\n* Fix run_slow  by @cyyever in #38314\r\n* Fix typo: change 'env' to 'environment' in .circleci/config.yml  by @AbdessamadEnabih in #38273\r\n* Adds use_repr to model_addition_debugger_context  by @RyanMullins in #37984\r\n* [tf/flax] handle `forced_decoder_ids` deletion  by @gante in #38316\r\n* [Whisper + beam search] fix usage of `beam_indices`  by @gante in #38259\r\n* Expose AutoModelForTimeSeriesPrediction for import  by @jinan-zhou in #38307\r\n* [custom_generate] don't forward `custom_generate` and `trust_remote_code`  by @gante in #38304\r\n* add `vasqu` to `self-comment-ci.yml`  by @ydshieh in #38324\r\n* Fix some tests (especially compile with fullgraph=True on Python<3.11)  by @Cyrilvallez in #38319\r\n* [performance_optim] reduce frequency of declaring attention_mask in Ascend NPU flash attention  by @FightingZhen in #38278\r\n* refactor can_save_slow_tokenizer  by @itazap in #37722\r\n* [`FlexAttention`] Reenable flex for encoder-decoder and make the test more robust  by @vasqu in #38321\r\n* Enhance Model Loading By Providing Parallelism, Uses Optional Env Flag  by @inf3rnus in #36835\r\n* Use Gradient Checkpointing Layer in Jamba & Blip Related Models  by @alex-jw-brooks in #38310\r\n* Never fallback to eager implicitly  by @Cyrilvallez in #38327\r\n* Remove duplicate docstring: resample  by @qqii in #38305\r\n* Update BioGPT model card  by @Aguedoom in #38214\r\n* docs(swinv2): Update SwinV2 model card to new standard format  by @BryanBradfo in #37942\r\n* [docs]: update roformer.md model card  by @KsuParkhamchuk in #37946\r\n* new failure CI reports for all jobs   by @ydshieh in #38298\r\n* Hot fix for AMD CI workflow  by @ydshieh in #38349\r\n* Uninstall `kernels` for AMD docker images  by @ydshieh in #38354\r\n* [VLMs] add helpers for get/set embedding  by @zucchini-nlp in #38144\r\n* switch to device agnostic device calling for test cases  by @yao-matrix in #38247\r\n* [`OPT`] Fix attention scaling  by @vasqu in #38290\r\n* Fix all import errors based on older torch versions  by @Cyrilvallez in #38370\r\n* Fix incorrect batching audio index calculation for Phi-4-Multimodal   by @Isotr0py in #38103\r\n* Protect `get_default_device` for torch<2.3  by @Cyrilvallez in #38376\r\n* [Falcon H1] Fix slow path forward pass  by @dhiaEddineRhaiem in #38320\r\n* Improved cache docs  by @manueldeprada in #38060\r\n* for now disable compile  by @ArthurZucker in #38383\r\n* Use one `utils/notification_service.py`  by @ydshieh in #38379\r\n* Better check in `initialize_weights`  by @Cyrilvallez in #38382\r\n* fix typos  by @DeVikingMark in #38336\r\n* fix typo: `tokenizer` -> `tokenize`  by @foldl in #38357\r\n* Stop TF weight rename reDOS  by @Rocketknight1 in #38325\r\n* [cli] cli usable without torch  by @gante in #38386\r\n* update gemma tests  by @ydshieh in #38384\r\n* Stop autoconverting custom code checkpoints  by @Rocketknight1 in #37751\r\n* Add AMD MI300 CI caller leveraging self-hosted runner scale set workflow in hf-workflows  by @jitesh-gupta in #38132\r\n* Fix image token mask in Gemma3  by @Cyrilvallez in #38295\r\n* [transformers x vLLM] standardize processors  by @zucchini-nlp in #37915\r\n* [paligemma] fix processor with suffix  by @zucchini-nlp in #38365\r\n* [video utils] group and reorder by number of frames  by @zucchini-nlp in #38374\r\n* [aya vision] fix processor for vLLM  by @zucchini-nlp in #38371\r\n* guard size mismatch check to only quantized models  by @SunMarc in #38397\r\n* [chat] improvements for thinking models and reduce default verbosity  by @gante in #38322\r\n* Fix convert to original state dict for VLMs  by @hiyouga in #38385\r\n* [chat] use the checkpoint's `generation_config.json` as base parameterization  by @gante in #38330\r\n* Fix Qwen2.5-VL Video Processor  by @yeliudev in #38366\r\n* [CSM] infer codec model with no_grad + audio eos label  by @eustlb in #38215\r\n* Add report_repo_id to mi300 workflow  by @ivarflakstad in #38401\r\n* [CSM] update model id  by @eustlb in #38211\r\n* [cleanup] delete deprecated kwargs in qwen2_audio 🧹   by @gante in #38404\r\n* [tests] remove overload for deleted test (`test_offloaded_cache_implementation`)  by @gante in #37896\r\n* [mllama] Allow `pixel_values` with `inputs_embeds`  by @dxoigmn in #38334\r\n* Update Model Card for Mamba-2  by @ParagEkbote in #37951\r\n* Updated Zoedepth model card  by @miniMaddy in #37898\r\n* Updated BigBird Model card as per #36979.  by @RogerSinghChugh in #37959\r\n* Updated BERTweet model card.  by @RogerSinghChugh in #37981\r\n* New bart model card  by @RogerSinghChugh in #37858\r\n* Update granite.md  by @Tanuj-rai in #37791\r\n* Falcon-H1 - Fix auto_docstring and add can_return_tuple decorator  by @yonigozlan in #38260\r\n* Updated model card for OLMo2  by @andyvu923 in #38394\r\n* Add mi300 to amd daily ci workflows definition  by @ivarflakstad in #38415\r\n* Change slack channel for mi250 CI  by @ivarflakstad in #38410\r\n* Fix an error in verify_tp_plan for keys without '.'  by @liwii in #38420\r\n* [qwen-vl] Look for vocab size in text config  by @zucchini-nlp in #38372\r\n* Update `CsmForConditionalGenerationIntegrationTest`  by @ydshieh in #38424\r\n* enable large_gpu and torchao cases on XPU  by @yao-matrix in #38355\r\n* Disable mi210 scheduled CI  by @ivarflakstad in #38411\r\n* Update error when using additional and/or masks  by @Cyrilvallez in #38429\r\n* Fix CircleCI not triggered when PR is opened from a branch of `huggingface/transformers`  by @ydshieh in #38413\r\n* make Llama4TextMoe forward more readable  by @JJJYmmm in #37529\r\n* [core] support tensor-valued _extra_state values in `from_pretrained`  by @pstjohn in #38155\r\n* Fix typo in tokenization_utils_base.py docstring  by @cwngan in #38418\r\n* Fix convert weights for InternVL  by @yonigozlan in #38233\r\n* Trigger doc-builder job after style bot  by @ydshieh in #38398\r\n* Remove redundant test_sdpa_equivalence test  by @Rocketknight1 in #38436\r\n* Fix MoE gradient test  by @Rocketknight1 in #38438\r\n* Fix `from_args_and_dict` ProcessorMixin  by @yonigozlan in #38296\r\n* Fix handling of slow/fast image processors in image_processing_auto.py  by @yonigozlan in #38161\r\n* Updated the Model docs - for the ALIGN model  by @1himan in #38072\r\n* Updated the model card for ViTMAE  by @mreraser in #38302\r\n* Model card for mobilenet v1 and v2  by @yuanjua in #37948\r\n* Merge type hints from `microsoft/python-type-stubs` (post dropping support for Python 3.8)  by @Avasam in #38335\r\n* Fix GLM4 checkpoints  by @ydshieh in #38412\r\n* feat: add cache retention for requests  by @McPatate in #38446\r\n* [Tests] Clean up test cases for few models  by @yaswanth19 in #38315\r\n* Fix TypeError in save_pretrained error handling (fixes #38422)  by @rahulrshetty45 in #38449\r\n* Cleanup `BatchFeature` and `BatchEncoding`  by @lgeiger in #38459\r\n* Fix `Gemma3IntegrationTest`  by @ydshieh in #38471\r\n* [Qwen2.5-Omni] Fix dtype of cos,sin when used with flash attention  by @HarryHsing in #38453\r\n* fix: handle no scheduler passed by user  by @McPatate in #38407\r\n* make it go brrrr  by @ArthurZucker in #38409\r\n* Fix convert_internvl_weights_to_hf.py to support local paths  by @xvyv99 in #38264\r\n* Fix incorrect bbox_embed initialization when decoder_bbox_embed_share=False in GroundingDINO  by @islemyakoubi in #38238\r\n* [Tests] Reduced model size for albert-test model  by @saqlain2204 in #38480\r\n* Align TP check  by @SunMarc in #38328\r\n* protect dtensor import   by @SunMarc in #38496\r\n* [docs] add xpu environment variable for gpu selection  by @faaany in #38194\r\n* Remove deprecated use_flash_attention_2 parameter  by @cyyever in #37131\r\n* Fix setting FLASH_ATTENTION_DETERMINISTIC after importing  by @HollowMan6 in #37185\r\n* [seamless_m4t] Skip some tests when speech is not available  by @remi-or in #38430\r\n* Update Loss Functions to Accept Tensor num_items_in_batch  by @NEREUScode in #38029\r\n* [generate] add soft deprecations on custom generation methods  by @gante in #38406\r\n* [generate] move `SinkCache` to a `custom_generate` repo  by @gante in #38399\r\n* remove unhandled parameter  by @itazap in #38145\r\n* Fix amp deprecation issue  by @SunMarc in #38100\r\n* [flax/mistral] support sliding_window: null in config  by @yiding in #37402\r\n* Num parameters in model.safetensors.index.json  by @LysandreJik in #38531\r\n* Remove type annotation in Siglip Attention Module  by @yaswanth19 in #38503\r\n* Fix `Gemma2IntegrationTest`  by @ydshieh in #38492\r\n* Fix blip2 tests  by @ydshieh in #38510\r\n* [tests] expand flex-attn test for vision models  by @zucchini-nlp in #38434\r\n* Don't use default attn if pre-set in sub-config  by @zucchini-nlp in #38526\r\n* update emu3 test  by @jiqing-feng in #38543\r\n* Update docker image to use `av`  by @ydshieh in #38548\r\n* [bugfix] [WIP] fix apply_rotary_emb error on Ascend NPU  by @FightingZhen in #38491\r\n* [TP] Change command in tests to `python3`  by @S1ro1 in #38555\r\n* Explicitly setting encoding in tokenization_utils_base.py  by @Muqi1029 in #38553\r\n* Fix `utils/notification_service.py`  by @ydshieh in #38556\r\n* Name change AOPermod -> ModuleFqn  by @drisspg in #38456\r\n* Fix hqq issue  by @SunMarc in #38551\r\n* [docs] Format fix  by @stevhliu in #38414\r\n* [janus] Fix failing tests on mi3XX  by @remi-or in #38426\r\n* Fix `chameleon` tests  by @ydshieh in #38565\r\n* update `utils/notification_service.py` for AMD vs Nvidia  by @ydshieh in #38563\r\n* Fix `deepseekv3`  by @ydshieh in #38562\r\n* [`FlexAttn`] Fix models with unique characteristics  by @vasqu in #38433\r\n* fix(attention_visualizer): add default value for image_seq_length  by @IceGiraffe in #38577\r\n* allow custom head_dim for qwen2_moe  by @bzantium in #37188\r\n* Docs: fix code formatting in torchao docs  by @Manalelaidouni in #38504\r\n* feat: add `repository` field to benchmarks table  by @McPatate in #38582\r\n* [Dinov2] Enable device_map=\"auto\" support  by @aryanchauhan31 in #38487\r\n* tests/roformer: fix couple roformer tests on gpus  by @dvrogozh in #38570\r\n* New gpt neo model card  by @RogerSinghChugh in #38505\r\n* Updated deprecated typing imports with equivalents for Python 3.9+  by @Sai-Suraj-27 in #38546\r\n* added fast image processor for ZoeDepth and expanded tests accordingly  by @henrikm11 in #38515\r\n* [qwen-omni] fix sliding window  by @zucchini-nlp in #38525\r\n* Remove custom pytest and pluggy  by @ydshieh in #38589\r\n* pin pandas  by @ydshieh in #38605\r\n* Allow `mlm_probability` to be set to `None` when `mlm=False` in DataCollatorForLanguageModeling  by @KameniAlexNea in #38522) \r\n* Avoid overwrite existing local implementation when loading remote custom model  by @Isotr0py in #38474\r\n* fix spelling errors   by @davidjsonn in #38608\r\n* Remove `isort` from dependencies  by @Sai-Suraj-27 in #38616\r\n* Fix `return_dict=False` giving errors in a few VLM models  by @ydshieh in #38519\r\n* docs: fix dark mode logo display.  by @johncaged in #38586\r\n* Fix typo in LLaVa documentation  by @mynameismon in #38618\r\n* [Nit] Add Note on SigOpt being in Public Archive Mode  by @ParagEkbote in #38610\r\n* Updated Aria model card  by @1himan in #38472\r\n* Fix `MiniMax` (docs and integration tests checkpoint)  by @geetu040 in #38575\r\n* enable more test cases on xpu  by @yao-matrix in #38572\r\n* Improve `test_initialization`  by @ydshieh in #38607\r\n* Use torch 2.7.1 on CircleCI jobs  by @ydshieh in #37856\r\n* [generation] bring back tests on vision models  by @zucchini-nlp in #38603\r\n* update `ColQwen2ModelIntegrationTest`  by @ydshieh in #38583\r\n* Improve `test_initialization` for `SwiftFormer`  by @ydshieh in #38636\r\n* fix: support grad clipping for TP through replicating non-sharded modules  by @kmehant in #36132\r\n* Don't run `AriaForConditionalGenerationModelTest` on CircleCI  by @ydshieh in #38615\r\n* fix total batch size calculation in trainer  by @inkcherry in #38286\r\n* fix torch_dtype on awq  by @jiqing-feng in #38463\r\n* Better CI  by @ydshieh in #38552\r\n* remove ipex_optimize_model usage  by @yao-matrix in #38632\r\n* Skip torchscript tests for 2 models  by @ydshieh in #38643\r\n* Fix `InternVL` integration test  by @ydshieh in #38612\r\n* Use torch 2.7.1 on daily CI  by @ydshieh in #38620\r\n* Fix qwen2-audio chat template audio placeholder insertion  by @Isotr0py in #38640\r\n* Fixed modeling_auto.py MODEL_FOR_MASK_GENERATION_MAPPING_NAMES variable  by @sbucaille in #38664\r\n* fix: \"check out\" as verb  by @DePasqualeOrg in #38678\r\n* Fix attention mask expansion when converting to executorch  by @pweglik in #38637\r\n* Fix some models import  by @nicelulu in #38694\r\n* Fix retrieve function signature and remove faiss requirement  by @Fiona-Waters in #38624\r\n* Fix TypeError: 'NoneType' object is not iterable for esm  by @dbleyl in #38667) \r\n* Docs: update bitsandbytes torch.compile compatibility  by @matthewdouglas in #38651\r\n* Drop as_target_processor from the _call_ and pad methods  by @marcndo in #38642\r\n* Created model card for XLM model  by @AshAnand34 in #38595\r\n* Update XLM-RoBERTa model documentation with enhanced usage examples and improved layout  by @AshAnand34 in #38596\r\n* Created model card for xlm-roberta-xl  by @AshAnand34 in #38597\r\n* Fix `aya_vision` test  by @ydshieh in #38674\r\n* Standardize ByT5 model card format  by @yanamis in #38699\r\n* Fix smart resize  by @rdonggroq in #38706\r\n* Update some tests for torch 2.7.1  by @ydshieh in #38701\r\n* Logging message for ``` is_bitsandbytes_available() ```   by @ved1beta in #38528\r\n* Fix `llava` tests  by @ydshieh in #38722\r\n* Use OSError  by @cyyever in #38712\r\n* [add-new-model-like] Robust search & proper outer '),' in tokenizer mapping  by @alexzms in #38703\r\n* Fix typo in Language Modeling example scripts and update TPU type  by @framoncg in #38652\r\n* Add AGENTS.md  by @Rocketknight1 in #38734\r\n* New canine model card  by @RogerSinghChugh in #38631\r\n* Fixed a multiple-devices issue in SmolVLM model  by @remi-or in #38736\r\n* [llava] fix integration tests with Siglip  by @zucchini-nlp in #38732\r\n* fix: Add method to get image features in PaliGemmaForConditionalGeneration  by @YushunXiang in #38730\r\n* from 1.11.0, torchao.prototype.low_bit_optim is promoted to torchao.optim  by @yao-matrix in #38689\r\n* fix: bf16 with TPU is allowed in configuration  by @yevvonlim in #38670\r\n* [DeepSeek-V3] implement when q_lora_rank is None  by @bzantium in #38743\r\n* Revert \"Trigger doc-builder job after style bot\"  by @ydshieh in #38735\r\n* Add z-loss to Bamba for v2  by @daviswer in #37842\r\n* Better typing for num_items_in_batch  by @SunMarc in #38728\r\n* Prepare for TF+Jax deprecation  by @Rocketknight1 in #38760\r\n* Remove IPEX requirement for bitsandbytes on CPU  by @matthewdouglas in #38594\r\n* Update repo consistency check  by @Rocketknight1 in #38763\r\n* fix(qwen3_moe): pass kwargs to self_attn  by @llllvvuu in #38691\r\n* Update pegasus model card  by @dross20 in #38675\r\n* Make style bot trigger CI after push  by @ydshieh in #38754\r\n* chore(pixtral): emit block attention mask when using flash attention  by @starcatmeow in #38741\r\n* Update altCLIP model card  by @EmileAydar in #38306\r\n* Add Qwen2 MoE model card  by @rileyafox in #38649\r\n* [masking utils] check `None` instead of try/except  by @zucchini-nlp in #38561\r\n* [Hotfix] Fix style bot   by @ydshieh in #38779\r\n* Fix masking utils  by @Cyrilvallez in #38783\r\n* [video processors] support frame sampling within processors  by @zucchini-nlp in #38105\r\n* Skip some export tests on torch 2.7  by @ydshieh in #38677\r\n* Reduce verbosity for `average_tokens_across_devices=True` and `world size = 1`  by @qgallouedec in #38785\r\n* Update PULL_REQUEST_TEMPLATE.md  by @qgallouedec in #38770\r\n* [docs] Add int4wo + 2:4 sparsity example to TorchAO README  by @jcaip in #38592\r\n* Fix `qwen_2_5 omni`  by @ydshieh in #38658\r\n* Fix `llava_onevision` tests  by @ydshieh in #38791\r\n* Reword README in light of model definitions  by @LysandreJik in #38762\r\n* Fix Typos in Comments: \"quantitation\" → \"quantization\", \"averege\" → \"average\"  by @leopardracer in #38766\r\n* Initialize flash attn flag  by @farnasirim in #38768\r\n* Fix `mllama`  by @ydshieh in #38704\r\n* build: :pushpin: Remove upper bound on PyTorch  by @KyleMylonakisProtopia in #38789\r\n* Remove all traces of `low_cpu_mem_usage`  by @Cyrilvallez in #38792\r\n* [Docs] New DiT model card  by @yushi2006 in #38721\r\n* Add missing div in Pegasus model card  by @dross20 in #38773\r\n* Updated moonshine modelcard  by @SohamPrabhu in #38711\r\n* refactor create_token_type_ids_from_sequences  by @itazap in #37681\r\n* [docs] update cache docs with new info  by @zucchini-nlp in #38775\r\n* Fix erroneous docstring for the ordering of SWA layers  by @norpadon in #38794\r\n* Fix configs and doc for the Qwens  by @Cyrilvallez in #38808\r\n* Unbreak optimum-executorch  by @guangy10 in #38646\r\n* Disable custom MRA kernels for ROCm  by @ahadnagy in #38738\r\n* Use HF papers  by @qgallouedec in #38184\r\n* Simplify and update trl examples  by @qgallouedec in #38772\r\n* Better pipeline type hints ✨  by @qubvel in #38049\r\n* Fix `llava_next` tests  by @ydshieh in #38813\r\n* Expectation fixes and added AMD expectations  by @remi-or in #38729\r\n* Use `wandb.run.url` instead of `wandb.run.get_url()` (deprecated)  by @qgallouedec in #38817\r\n* Refactor DBRX tests to use CausalLMModelTest base classes  by @Rocketknight1 in #38475\r\n* change fsdp_strategy to fsdp in TrainingArguments in accelerate doc  by @PT-10 in #38807\r\n* Fix a minor security issue  by @ydshieh in #38815\r\n* Fix trainer.py not showing signature columns  by @nenesekai in #38465\r\n* Add V-JEPA for video classification model  by @qubvel in #38788\r\n* fixed docstring in modular_qwen2_5_vl.py  by @lawrencefeng17 in #38798\r\n* [docs] Update docs moved to the course  by @stevhliu in #38800\r\n* [docs] updated roberta model card  by @allmight05 in #38777\r\n* Updated Albert model Card  by @souvikchand in #37753\r\n* [internvl] fix video inference  by @zucchini-nlp in #38811\r\n* Fix redundant code in Janus  by @yaswanth19 in #38826\r\n* bugfix: propage weight key_mapping to peft to fix 3.52 VLM renaming   by @ManuelFay in #38627\r\n* Fix peft integration  by @Cyrilvallez in #38841\r\n* Fix broken notebooks link in Italian training docs  by @VolodymyrBg in #38834\r\n* Fix broken tag in Longformer model card  by @dross20 in #38828\r\n* [BugFix] QA pipeline edge case: `align_to_words=True` in `QuestionAnsweringPipeline` can lead to duplicate answers  by @yushi2006 in #38761\r\n* GraniteMoeHybrid: Allow for only shared expert case.  by @shawntan in #38801\r\n* Updated aya_vision.md  by @1himan in #38749\r\n* Remove merge conflict artifacts in Albert model doc  by @druvdub in #38849\r\n* [video processor] fix BC when no video config if found  by @zucchini-nlp in #38840\r\n* Fix incorrect width ratio calculation in Llama4 image processor  by @Jingxiang-Zhang in #38842\r\n* Allow customization of sdpa in executorch.py  by @kimishpatel in #38827\r\n* Fix `qwen2_5_vl` tests  by @ydshieh in #38845\r\n* Improve `auxiliary_in_channels` default behavior in UperNet  by @simonreise in #37540\r\n* Fix `qwen3` tests  by @ydshieh in #38862\r\n* Update CvT documentation with improved usage examples and additional …  by @sezan92 in #38731\r\n* Update roc bert docs  by @SohamPrabhu in #38835\r\n* Post-PR fixes!  by @Rocketknight1 in #38868\r\n* enable misc test cases on XPU  by @yao-matrix in #38852\r\n* Fix `phi4_multimodal` tests  by @ydshieh in #38816\r\n* Fix `qwen3_moe` tests  by @ydshieh in #38865\r\n* Fix HQQ model param device transfer issue  by @HighCWu in #38466\r\n* Fixed markdown for BertTokenizer's '[CLS]' token.  by @eu90h in #38506\r\n* null deepspeed_plugin in args for wandb callback fake trainer  by @winglian in #38867\r\n* More PYUP fixes  by @cyyever in #38883\r\n* Fix loop var naming  by @Rocketknight1 in #38885\r\n* [bugfix] fix ATTN_MASK_NPU device mismatch error on multi-device NPU …  by @qykong in #38876\r\n* log: Add logging when using split_batches and per_device_train_batch_size  by @KeshavSingh29 in #38633\r\n* Docs: Add custom fine-tuning tutorial to TrOCR model page  by @Ashutosh-4485 in #38847\r\n* 36978 | Fast image processor for DPT model  by @samrae7 in #37481\r\n* [video processor] fix slow tests  by @zucchini-nlp in #38881\r\n* Update bamba model card  by @druvdub in #38853\r\n* Add support for specifying revisions when pushing to Hub via internal Trainer call  by @IsaacBreen in #36852\r\n* Use `raise from e` in `hub.py` utility  by @Wauplin in #37241\r\n* [phi-4] use mel filters from audio utils  by @eustlb in #36966\r\n* Fix `fsmt` tests  by @ydshieh in #38904\r\n* Fix unnecessary super calls  by @cyyever in #38897\r\n* align xpu's autocast behavior w/ cuda by using device agnostic torch APIs  by @yao-matrix in #38284\r\n* Fix `FalconMambaIntegrationTests`  by @ydshieh in #38566\r\n* Skip sdpa tests if submodule does not support sdpa  by @ivarflakstad in #38907\r\n* Fix ReDOS in tokenizer digit substitution  by @Rocketknight1 in #38844\r\n* feat: Add granite architectures to auto tokenizer name mappings  by @gabe-l-hart in #38802\r\n* Allow make-fixup on main branch, albeit slowly  by @Rocketknight1 in #38892\r\n* feat: add flexible Liger Kernel configuration to TrainingArguments  by @hamza-hcompany in #38911\r\n* Remove deprecated classes in modeling_utils.py  by @Cyrilvallez in #38919\r\n* Skip some tests for now  by @ydshieh in #38931\r\n* Modernbert fixes  by @remi-or in #38912\r\n* add pytorch-xpu Dockerfile  by @yao-matrix in #38875\r\n* Remove `ALL_LAYERNORM_LAYERS`  by @Cyrilvallez in #38922\r\n* [static cache] fix device map per layer in VLMs  by @zucchini-nlp in #38488\r\n* Add kwargs for timm.create_model in TimmWrapper  by @qubvel in #38860\r\n* Pin PyTorch extras for AMD containers  by @ahadnagy in #38941\r\n* Correctly raise error for awq quantization  by @Cyrilvallez in #38945\r\n* Fix more flaky `test_initialization`  by @ydshieh in #38932\r\n* Switch to use A10 progressively  by @ydshieh in #38936\r\n* Fix custom generate from local directory  by @manueldeprada in #38916\r\n* Update blip model card  by @devkade in #38513\r\n* Gaudi3 CI  by @IlyasMoutawwakil in #38790\r\n* Fix DTensor import compatibility for PyTorch < 2.5  by @Benoqtr in #38836\r\n* Fix(informer): Correct tensor shape for input_size=1  by @Flink-ddd in #38856\r\n* [modular] CLI allows positional arguments, and more defaults names for the optional arg  by @Cyrilvallez in #38979\r\n* Remove dead protected imports  by @Cyrilvallez in #38980\r\n* Break tie in Expectations and gemma3 fixes  by @remi-or in #38943\r\n* Add Idefics2/3 and SmolVLM Fast image processors + improvements for fast image processors  by @yonigozlan in #38157\r\n* fix: add __bool__ operator to tokenizer to avoid bloated asserts  by @kallewoof in #38899\r\n* Add support for auto_docstring with model outputs  by @yonigozlan in #38242\r\n* fix `mistral` and `mistral3` tests  by @ydshieh in #38978\r\n* [Feature] Support `is_split_into_words` in the `TokenClassificationPipeline`.  by @yushi2006 in #38818\r\n* Fix `rag`  by @ydshieh in #38585\r\n* [docs] Typos - Single GPU efficient training features  by @casinca in #38964\r\n* [qwen] refactor attentions for vision/audio  by @zucchini-nlp in #38930\r\n* Removing extra space in large command for speech-pretraining example  by @dggaytan in #38705\r\n* [`Attention`] Small fix on output attentions  by @vasqu in #38948\r\n* Fixes for Arcee model  by @Cyrilvallez in #39001\r\n* Added scikit-learn to the example image-classification requirements.txt  by @mylonjones in #37506\r\n* Update attention_visualizer.py  by @Tanuj-rai in #37860\r\n* Skip non-selected experts for qwen3_moe  by @seven-mile in #38133\r\n* Fix undeterministic order in modular dependencies  by @Cyrilvallez in #39005\r\n* Granite speech - minor fixes to support training with the HF trainer  by @avihu111 in #38833\r\n* Fix bugs in DynamicCache  by @tugsbayasgalan in #37880\r\n* Update self-comment-ci.yml user list  by @ivarflakstad in #39014\r\n* Skip sdpa dispatch on flash test due to unsupported head dims  by @ivarflakstad in #39010\r\n* [HPU][Critical Issue Fix] ThreadPool instead of Pool for parallel pre-processing  by @dsmertin in #39002\r\n* Add Hugging Face authentication procedure for IDEs (PyCharm, VS Code,…  by @marcndo in #38954\r\n* [LightGlue] Fixed attribute usage from descriptor_dim to keypoint_detector_descriptor_dim  by @sbucaille in #39021\r\n* Add zero dim tensor check when using flash_attention  by @ranzhejiang in #38280\r\n* Fix graph break in torch.compile when using FA2 with attention_mask=None and batch size > 1  by @efsotr in #37332\r\n* [AutoModelForMaskGeneration] Remove duplicate code  by @NielsRogge in #38622\r\n* [video processor] support torchcodec and decrease cuda memory usage  by @zucchini-nlp in #38880\r\n* Drop unnecessary tokens in GPT2Model generation  by @null-pointer-access in #39016\r\n* Fix the seamless_m4t cannot work on Gaudi  by @yuanwu2017 in #38363\r\n* fix: astronomical loss with ModernBERT when using gradient checkpointing  by @umarbutler in #38982) \r\n* fix gemma3 grad acc  by @SunMarc in #37208\r\n* Remove script datasets in tests  by @lhoestq in #38940\r\n* Fix grammatical error in models documentation  by @marcndo in #39019\r\n* refactor: remove custom BarkLayerNorm  by @eginhard in #39003\r\n* [Kyutai-STT] correct model type + model id  by @eustlb in #39035\r\n* Two ReDOS fixes  by @Rocketknight1 in #39013\r\n* [tests] remove TF tests (uses of `require_tf`)  by @gante in #38944\r\n* Granite speech speedup + model saving bugfix  by @avihu111 in #39028\r\n* Fix Bad Outputs in Fast Path for GraniteMoeHybrid  by @alex-jw-brooks in #39033\r\n\r\n## Significant community contributions\r\n\r\nThe following contributors have made significant changes to the library over the last release:\r\n\r\n* @ydshieh\r\n    * CI reporting improvements (#38230)\r\n    * add `liger-kernel` to docker file (#38292)\r\n    * add `vasqu` to `self-comment-ci.yml` (#38324)\r\n    * new failure CI reports for all jobs  (#38298)\r\n    * Hot fix for AMD CI workflow (#38349)\r\n    * Uninstall `kernels` for AMD docker images (#38354)\r\n    * Use one `utils/notification_service.py` (#38379)\r\n    * update gemma tests (#38384)\r\n    * Update `CsmForConditionalGenerationIntegrationTest` (#38424)\r\n    * Fix CircleCI not triggered when PR is opened from a branch of `huggingface/transformers` (#38413)\r\n    * Trigger doc-builder job after style bot (#38398)\r\n    * Fix GLM4 checkpoints (#38412)\r\n    * Fix `Gemma3IntegrationTest` (#38471)\r\n    * Fix `Gemma2IntegrationTest` (#38492)\r\n    * Fix blip2 tests (#38510)\r\n    * Update docker image to use `av` (#38548)\r\n    * Fix `utils/notification_service.py` (#38556)\r\n    * Fix `chameleon` tests (#38565)\r\n    * update `utils/notification_service.py` for AMD vs Nvidia (#38563)\r\n    * Fix `deepseekv3` (#38562)\r\n    * Remove custom pytest and pluggy (#38589)\r\n    * pin pandas (#38605)\r\n    * Fix `return_dict=False` giving errors in a few VLM models (#38519)\r\n    * Improve `test_initialization` (#38607)\r\n    * Use torch 2.7.1 on CircleCI jobs (#37856)\r\n    * update `ColQwen2ModelIntegrationTest` (#38583)\r\n    * Improve `test_initialization` for `SwiftFormer` (#38636)\r\n    * Don't run `AriaForConditionalGenerationModelTest` on CircleCI (#38615)\r\n    * Better CI (#38552)\r\n    * Skip torchscript tests for 2 models (#38643)\r\n    * Fix `InternVL` integration test (#38612)\r\n    * Use torch 2.7.1 on daily CI (#38620)\r\n    * Fix `aya_vision` test (#38674)\r\n    * Update some tests for torch 2.7.1 (#38701)\r\n    * Fix `llava` tests (#38722)\r\n    * Revert \"Trigger doc-builder job after style bot\" (#38735)\r\n    * Make style bot trigger CI after push (#38754)\r\n    * [Hotfix] Fix style bot  (#38779)\r\n    * Skip some export tests on torch 2.7 (#38677)\r\n    * Fix `qwen_2_5 omni` (#38658)\r\n    * Fix `llava_onevision` tests (#38791)\r\n    * Fix `mllama` (#38704)\r\n    * Fix `llava_next` tests (#38813)\r\n    * Fix a minor security issue (#38815)\r\n    * Fix `qwen2_5_vl` tests (#38845)\r\n    * Fix `qwen3` tests (#38862)\r\n    * Fix `phi4_multimodal` tests (#38816)\r\n    * Fix `qwen3_moe` tests (#38865)\r\n    * Fix `fsmt` tests (#38904)\r\n    * Fix `FalconMambaIntegrationTests` (#38566)\r\n    * Skip some tests for now (#38931)\r\n    * Fix more flaky `test_initialization` (#38932)\r\n    * Switch to use A10 progressively (#38936)\r\n    * fix `mistral` and `mistral3` tests (#38978)\r\n    * Fix `rag` (#38585)\r\n* @ArthurZucker\r\n    * tp plan should not be NONE (#38255)\r\n    * Protect ParallelInterface (#38262)\r\n    * Add CB (#38085)\r\n    * 🚨Early-error🚨 config will error out if `output_attentions=True` and the attn implementation is wrong (#38288)\r\n    * for now disable compile (#38383)\r\n    * make it go brrrr (#38409)\r\n* @younesbelkada\r\n    * [MODEL] Add Falcon H1 (#38249)\r\n* @cyr0930\r\n    * fix multi-image case for llava-onevision (#38084)\r\n* @cyyever\r\n    * Improve typing in TrainingArgument (#36944)\r\n    * More typing in src/transformers/training_args.py (#38106)\r\n    * Fix run_slow (#38314)\r\n    * Remove deprecated use_flash_attention_2 parameter (#37131)\r\n    * Use OSError (#38712)\r\n    * More PYUP fixes (#38883)\r\n    * Fix unnecessary super calls (#38897)\r\n* @ritsumei-aoi\r\n    * Remove Japanese sequence_classification doc and update references (#38246)\r\n* @yao-matrix\r\n    * add XPU info print in print_env (#38282)\r\n    * refine `transformers env` output (#38274)\r\n    * switch to device agnostic device calling for test cases (#38247)\r\n    * enable large_gpu and torchao cases on XPU (#38355)\r\n    * enable more test cases on xpu (#38572)\r\n    * remove ipex_optimize_model usage (#38632)\r\n    * from 1.11.0, torchao.prototype.low_bit_optim is promoted to torchao.optim (#38689)\r\n    * enable misc test cases on XPU (#38852)\r\n    * align xpu's autocast behavior w/ cuda by using device agnostic torch APIs (#38284)\r\n    * add pytorch-xpu Dockerfile (#38875)\r\n* @vasqu\r\n    * 🔴🔴🔴 [`Attention`] Refactor Attention Interface for Bart-based Models (#38108)\r\n    * [`FlexAttention`] Reenable flex for encoder-decoder and make the test more robust (#38321)\r\n    * [`OPT`] Fix attention scaling (#38290)\r\n    * 🔴[`Attention`] Attention refactor for Whisper-based models (#38235)\r\n    * [`FlexAttn`] Fix models with unique characteristics (#38433)\r\n    * [`Attention`] Small fix on output attentions (#38948)\r\n* @itazap\r\n    * refactor can_save_slow_tokenizer (#37722)\r\n    * remove unhandled parameter (#38145)\r\n    * refactor create_token_type_ids_from_sequences (#37681)\r\n* @eustlb\r\n    * [CSM] infer codec model with no_grad + audio eos label (#38215)\r\n    * [CSM] update model id (#38211)\r\n    * [phi-4] use mel filters from audio utils (#36966)\r\n    * Add kyutai stt (#38909)\r\n    * [Kyutai-STT] correct model type + model id (#39035)\r\n* @RogerSinghChugh\r\n    * Updated BigBird Model card as per #36979. (#37959)\r\n    * Updated BERTweet model card. (#37981)\r\n    * New bart model card (#37858)\r\n    * New gpt neo model card (#38505)\r\n    * New canine model card (#38631)\r\n* @1himan\r\n    * Updated the Model docs - for the ALIGN model (#38072)\r\n    * Updated Aria model card (#38472)\r\n    * Updated aya_vision.md (#38749)\r\n* @Avasam\r\n    * Merge type hints from `microsoft/python-type-stubs` (post dropping support for Python 3.8) (#38335)\r\n* @remi-or\r\n    * [seamless_m4t] Skip some tests when speech is not available (#38430)\r\n    * [janus] Fix failing tests on mi3XX (#38426)\r\n    * Fixed a multiple-devices issue in SmolVLM model (#38736)\r\n    * Expectation fixes and added AMD expectations (#38729)\r\n    * Modernbert fixes (#38912)\r\n    * Break tie in Expectations and gemma3 fixes (#38943)\r\n* @tonywu71\r\n    * Add ColQwen2 to 🤗 transformers (#35778)\r\n* @geetu040\r\n    * Add support for MiniMax's MiniMax-Text-01 (#35831)\r\n    * Fix `MiniMax` (docs and integration tests checkpoint) (#38575)\r\n* @sbucaille\r\n    * Fixed modeling_auto.py MODEL_FOR_MASK_GENERATION_MAPPING_NAMES variable (#38664)\r\n    * Add LightGlue model (#31718)\r\n    * [LightGlue] Fixed attribute usage from descriptor_dim to keypoint_detector_descriptor_dim (#39021)\r\n* @samrae7\r\n    * 36978 | Fast image processor for DPT model (#37481)\r\n* @Crystalcareai\r\n    * Add Arcee model support (#38621)\r\n* @zRzRzRzRzRzRzR\r\n    * GLM-4.1V Model support (#38431)\r\n* @bzhangGo\r\n    * Encoder-Decoder Gemma (#38332)\r\n* @redmoe-moutain\r\n    * [Model] add dots1 (#38143)\r\n* @EduardDurech\r\n    * Support for Flash Attention 3 (#38972)\r\n",
      "publishedAt": "2025-06-26T16:07:22.000Z",
      "source": "Hugging Face GitHub",
      "sourceUrl": "https://github.com/huggingface/transformers/releases/tag/v4.53.0",
      "category": "tools",
      "company": "Hugging Face",
      "imageUrl": null,
      "tags": [
        "GPT",
        "Llama",
        "LLM",
        "AI",
        "ML"
      ],
      "featured": false
    },
    {
      "id": "mcp595l3t6xy1jfgbhq",
      "title": "Show HN: Claude Slash Command Suite inspired by Anthropics best practices guide",
      "summary": "I built a collection of professional slash commands for Anthropic&#x27;s Claude Code that provide structured workflows for common software development tasks.<p><pre><code>  These commands are directly inspired by and adapted from Anthropic&#x27;s own claude-code-best-practices (https:&#x2F;&#x2F;www...",
      "content": "I built a collection of professional slash commands for Anthropic&#x27;s Claude Code that provide structured workflows for common software development tasks.<p><pre><code>  These commands are directly inspired by and adapted from Anthropic&#x27;s own claude-code-best-practices (https:&#x2F;&#x2F;www.anthropic.com&#x2F;engineering&#x2F;claude-code-best-practices) documentation, translating their recommendations into executable workflows.\n\n  The suite includes commands for:\n  • Comprehensive code reviews with security and performance analysis\n  • End-to-end feature development with planning, implementation, and testing\n  • Architectural analysis and design pattern assessment\n • Security audits and vulnerability scanning\n  • GitHub issue resolution with root cause analysis\n  • Performance optimization and build improvements\n\n  Each command follows a systematic approach based on Anthropic&#x27;s\n  best practices, breaking complex tasks into manageable steps.\n  Instead of ad-hoc AI interactions, you get consistent, thorough\n   workflows that adapt to any codebase.\n\n  The commands work through Claude Code&#x27;s slash command system -\n  just type `&#x2F;project:code-review` or `&#x2F;project:create-feature\n  user-authentication` and Claude follows the predefined\n  workflow.\n\n  Installation is straightforward with an interactive script that\n   can install project-specific or globally. The commands are\n  fully customizable markdown files, so you can adapt them to\n  your team&#x27;s specific requirements.\n\n</code></pre>\nI hope others find it useful!",
      "publishedAt": "2025-06-13T02:05:24.000Z",
      "source": "Hacker News Claude/Anthropic",
      "sourceUrl": "https://github.com/qdhenry/Claude-Command-Suite",
      "category": "companies",
      "company": "Anthropic",
      "imageUrl": null,
      "tags": [
        "Claude",
        "AI",
        "Anthropic"
      ],
      "featured": true
    }
  ],
  "featuredCount": 7
}