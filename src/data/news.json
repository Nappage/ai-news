{
  "lastUpdated": "2025-06-24T14:27:37.800Z",
  "totalArticles": 26,
  "articles": [
    {
      "id": "mcamdfmev650wabttym",
      "title": "Gemini Robotics On-Device brings AI to local robotic devices",
      "summary": "We’re introducing an efficient, on-device robotics model with general-purpose dexterity and fast task adaptation.",
      "content": "We’re introducing an efficient, on-device robotics model with general-purpose dexterity and fast task adaptation.",
      "publishedAt": "2025-06-24T14:00:36.000Z",
      "source": "DeepMind Blog",
      "sourceUrl": "https://deepmind.google/discover/blog/gemini-robotics-on-device-brings-ai-to-local-robotic-devices/",
      "category": "research",
      "company": "Google",
      "imageUrl": null,
      "tags": [
        "Gemini",
        "AI"
      ],
      "featured": false
    },
    {
      "id": "mcamdpmve670xv6oh27",
      "title": "GPT-4o in thinking mode?",
      "summary": "Is anyone else consistently seeing GPT-4o use \"thinking\" mode? I thought that this was a non-reasoning model.\n\nIs this being noticed by everyone or am I in a weird A/B test by OpenAI?...",
      "content": "Is anyone else consistently seeing GPT-4o use \"thinking\" mode? I thought that this was a non-reasoning model.\n\nIs this being noticed by everyone or am I in a weird A/B test by OpenAI?",
      "publishedAt": "2025-06-24T13:44:09.000Z",
      "source": "Reddit r/OpenAI",
      "sourceUrl": "https://i.redd.it/fr70vwfrpv8f1.png",
      "category": "companies",
      "company": "OpenAI",
      "imageUrl": null,
      "tags": [
        "GPT",
        "AI",
        "OpenAI"
      ],
      "featured": false
    },
    {
      "id": "mcamdgparxqb3sbdelg",
      "title": "Anthropic: v0.55.0",
      "summary": "## 0.55.0 (2025-06-23)\n\nFull Changelog: [v0.54.0...v0.55.0](https://github.com/anthropics/anthropic-sdk-python/compare/v0.54.0...v0.55.0)\n\n### Features\n\n* **api:** api update ([4b2134e](https://github.com/anthropics/anthropic-sdk-python/commit/4b2134e5ec3fecab7c56f483b8db87b403a08e05))\n* **api:** ap...",
      "content": "## 0.55.0 (2025-06-23)\n\nFull Changelog: [v0.54.0...v0.55.0](https://github.com/anthropics/anthropic-sdk-python/compare/v0.54.0...v0.55.0)\n\n### Features\n\n* **api:** api update ([4b2134e](https://github.com/anthropics/anthropic-sdk-python/commit/4b2134e5ec3fecab7c56f483b8db87b403a08e05))\n* **api:** api update ([2093bff](https://github.com/anthropics/anthropic-sdk-python/commit/2093bfff2a6c25573eaa2a4667f1e1d0e2d89e24))\n* **api:** manual updates ([c80fda8](https://github.com/anthropics/anthropic-sdk-python/commit/c80fda8cbd157fbbd23895d034cc7bb7a7614569))\n* **client:** add support for aiohttp ([3b03295](https://github.com/anthropics/anthropic-sdk-python/commit/3b03295f15a02ba629d1bdc77e330c2e6043b83e))\n\n\n### Bug Fixes\n\n* **client:** correctly parse binary response | stream ([d93817d](https://github.com/anthropics/anthropic-sdk-python/commit/d93817d9d761bd5e16b35f3c2973122a9c122240))\n* **internal:** revert unintentional changes ([bb3beab](https://github.com/anthropics/anthropic-sdk-python/commit/bb3beab10668be177d6bb573607ef6951a238b24))\n* **tests:** fix: tests which call HTTP endpoints directly with the example parameters ([ee69d74](https://github.com/anthropics/anthropic-sdk-python/commit/ee69d74cc40f749280a29afb12420c117d08ef34))\n* **tests:** suppress warnings in tests when running on the latest Python versions ([#982](https://github.com/anthropics/anthropic-sdk-python/issues/982)) ([740da21](https://github.com/anthropics/anthropic-sdk-python/commit/740da21b563c6ffe7618edf1dcd658bb894b2edf))\n\n\n### Chores\n\n* **ci:** enable for pull requests ([08f2dd2](https://github.com/anthropics/anthropic-sdk-python/commit/08f2dd2bd28958c08a3c82fcf00a0fc7d4e2807c))\n* **internal:** update conftest.py ([1174a62](https://github.com/anthropics/anthropic-sdk-python/commit/1174a6214624ff8cd64edb121d4ff09e9af6b717))\n* **internal:** version bump ([7241eaa](https://github.com/anthropics/anthropic-sdk-python/commit/7241eaa25b6f40bb55f61e766a996a3a18a53a02))\n* **readme:** update badges ([00661c2](https://github.com/anthropics/anthropic-sdk-python/commit/00661c275e120314f76bbd480c0267383e992638))\n* **tests:** add tests for httpx client instantiation & proxies ([b831d88](https://github.com/anthropics/anthropic-sdk-python/commit/b831d8833010c629143041b4b385929ca9c2198d))\n* **tests:** run tests in parallel ([4b24a79](https://github.com/anthropics/anthropic-sdk-python/commit/4b24a791b76c2176de1f35118901da533a10b991))\n\n\n### Documentation\n\n* **client:** fix httpx.Timeout documentation reference ([b0138b1](https://github.com/anthropics/anthropic-sdk-python/commit/b0138b1b2af3c73e568659c7e717fc955eb976b0))",
      "publishedAt": "2025-06-23T18:51:59.000Z",
      "source": "Anthropic GitHub",
      "sourceUrl": "https://github.com/anthropics/anthropic-sdk-python/releases/tag/v0.55.0",
      "category": "tools",
      "company": "Anthropic",
      "imageUrl": null,
      "tags": [
        "AI",
        "Anthropic"
      ],
      "featured": false
    },
    {
      "id": "mcamdpmvqdgonq12wkg",
      "title": "Learn to use AI or... uh...",
      "summary": "Reddit投稿",
      "content": "",
      "publishedAt": "2025-06-23T16:12:26.000Z",
      "source": "Reddit r/OpenAI",
      "sourceUrl": "https://i.redd.it/eele5c4fbp8f1.png",
      "category": "companies",
      "company": "OpenAI",
      "imageUrl": null,
      "tags": [
        "AI"
      ],
      "featured": false
    },
    {
      "id": "mcamd9v1w84enas6u3",
      "title": "Transformers backend integration in SGLang",
      "summary": "",
      "content": "",
      "publishedAt": "2025-06-23T00:00:00.000Z",
      "source": "Hugging Face Blog",
      "sourceUrl": "https://huggingface.co/blog/transformers-backend-sglang",
      "category": "tools",
      "company": "Hugging Face",
      "imageUrl": null,
      "tags": [
        "Transformer"
      ],
      "featured": false
    },
    {
      "id": "mcamd9v1i646pdbur8a",
      "title": "(LoRA) Fine-Tuning FLUX.1-dev on Consumer Hardware",
      "summary": "",
      "content": "",
      "publishedAt": "2025-06-19T00:00:00.000Z",
      "source": "Hugging Face Blog",
      "sourceUrl": "https://huggingface.co/blog/flux-qlora",
      "category": "tools",
      "company": "Hugging Face",
      "imageUrl": null,
      "tags": [],
      "featured": false
    },
    {
      "id": "mcamd8otrzs5w5tpohl",
      "title": "Preparing for future AI risks in biology",
      "summary": "Advanced AI can transform biology and medicine—but also raises biosecurity risks. We’re proactively assessing capabilities and implementing safeguards to prevent misuse.",
      "content": "Advanced AI can transform biology and medicine—but also raises biosecurity risks. We’re proactively assessing capabilities and implementing safeguards to prevent misuse.",
      "publishedAt": "2025-06-18T10:00:00.000Z",
      "source": "OpenAI Blog",
      "sourceUrl": "https://openai.com/index/preparing-for-future-ai-capabilities-in-biology",
      "category": "companies",
      "company": "OpenAI",
      "imageUrl": null,
      "tags": [
        "AI"
      ],
      "featured": true
    },
    {
      "id": "mcamd8otoj1b3u8wu1",
      "title": "Toward understanding and preventing misalignment generalization",
      "summary": "We study how training on incorrect responses can cause broader misalignment in language models and identify an internal feature driving this behavior—one that can be reversed with minimal fine-tuning.",
      "content": "We study how training on incorrect responses can cause broader misalignment in language models and identify an internal feature driving this behavior—one that can be reversed with minimal fine-tuning.",
      "publishedAt": "2025-06-18T10:00:00.000Z",
      "source": "OpenAI Blog",
      "sourceUrl": "https://openai.com/index/emergent-misalignment",
      "category": "companies",
      "company": "OpenAI",
      "imageUrl": null,
      "tags": [
        "AI"
      ],
      "featured": true
    },
    {
      "id": "mcamdfmenv4xctvtup",
      "title": "Gemini 2.5: Updates to our family of thinking models",
      "summary": "Explore the latest Gemini 2.5 model updates with enhanced performance and accuracy: Gemini 2.5 Pro now stable, Flash generally available, and the new Flash-Lite in preview.",
      "content": "Explore the latest Gemini 2.5 model updates with enhanced performance and accuracy: Gemini 2.5 Pro now stable, Flash generally available, and the new Flash-Lite in preview.",
      "publishedAt": "2025-06-17T16:03:39.000Z",
      "source": "DeepMind Blog",
      "sourceUrl": "https://deepmind.google/discover/blog/gemini-25-updates-to-our-family-of-thinking-models/",
      "category": "research",
      "company": "Google",
      "imageUrl": null,
      "tags": [
        "Gemini",
        "AI"
      ],
      "featured": false
    },
    {
      "id": "mcamdfme9tn8dejuf1p",
      "title": "We’re expanding our Gemini 2.5 family of models",
      "summary": "Gemini 2.5 Flash and Pro are now generally available, and we’re introducing 2.5 Flash-Lite, our most cost-efficient and fastest 2.5 model yet.",
      "content": "Gemini 2.5 Flash and Pro are now generally available, and we’re introducing 2.5 Flash-Lite, our most cost-efficient and fastest 2.5 model yet.",
      "publishedAt": "2025-06-17T16:01:00.000Z",
      "source": "DeepMind Blog",
      "sourceUrl": "https://deepmind.google/discover/blog/were-expanding-our-gemini-25-family-of-models/",
      "category": "research",
      "company": "Google",
      "imageUrl": null,
      "tags": [
        "Gemini",
        "AI"
      ],
      "featured": false
    },
    {
      "id": "mcamd8ot1m0i3h5x5ep",
      "title": "Introducing OpenAI for Government",
      "summary": "We’re launching OpenAI for Government, a new initiative focused on bringing our most advanced AI tools to public servants across the United States. We're supporting the U.S. government's efforts in adopting best-in-class technology and deploying these tools in service of the public good.",
      "content": "We’re launching OpenAI for Government, a new initiative focused on bringing our most advanced AI tools to public servants across the United States. We're supporting the U.S. government's efforts in adopting best-in-class technology and deploying these tools in service of the public good.",
      "publishedAt": "2025-06-16T00:00:00.000Z",
      "source": "OpenAI Blog",
      "sourceUrl": "https://openai.com/global-affairs/introducing-openai-for-government",
      "category": "companies",
      "company": "OpenAI",
      "imageUrl": null,
      "tags": [
        "AI",
        "OpenAI"
      ],
      "featured": false
    },
    {
      "id": "mcamd9v1ucpw768lnr",
      "title": "Groq on Hugging Face Inference Providers 🔥",
      "summary": "",
      "content": "",
      "publishedAt": "2025-06-16T00:00:00.000Z",
      "source": "Hugging Face Blog",
      "sourceUrl": "https://huggingface.co/blog/inference-providers-groq",
      "category": "tools",
      "company": "Hugging Face",
      "imageUrl": null,
      "tags": [],
      "featured": false
    },
    {
      "id": "mcamdfme3bdhafoyr2x",
      "title": "Behind “ANCESTRA”: combining Veo with live-action filmmaking",
      "summary": "We partnered with Darren Aronofsky, Eliza McNitt and a team of more than 200 people to make a film using Veo and live-action filmmaking.",
      "content": "We partnered with Darren Aronofsky, Eliza McNitt and a team of more than 200 people to make a film using Veo and live-action filmmaking.",
      "publishedAt": "2025-06-13T13:30:00.000Z",
      "source": "DeepMind Blog",
      "sourceUrl": "https://deepmind.google/discover/blog/behind-ancestra-combining-veo-with-live-action-filmmaking/",
      "category": "research",
      "company": "Google",
      "imageUrl": null,
      "tags": [],
      "featured": false
    },
    {
      "id": "mcamdfme6l3b27e9u2f",
      "title": "How we're supporting better tropical cyclone prediction with AI",
      "summary": "We’re launching Weather Lab, featuring our experimental cyclone predictions, and we’re partnering with the U.S. National Hurricane Center to support their forecasts and warnings this cyclone season.",
      "content": "We’re launching Weather Lab, featuring our experimental cyclone predictions, and we’re partnering with the U.S. National Hurricane Center to support their forecasts and warnings this cyclone season.",
      "publishedAt": "2025-06-12T15:00:00.000Z",
      "source": "DeepMind Blog",
      "sourceUrl": "https://deepmind.google/discover/blog/weather-lab-cyclone-predictions-with-ai/",
      "category": "research",
      "company": "Google",
      "imageUrl": null,
      "tags": [
        "AI"
      ],
      "featured": false
    },
    {
      "id": "mcamd8otjewu6yynlal",
      "title": "Bringing the Magic of AI to Mattel’s Iconic Brands",
      "summary": "OpenAI and Mattel are partnering to integrate AI into iconic brands such as Barbie and Hot Wheels, aiming to enhance creative development, streamline workflows, and create new ways for fans to engage.",
      "content": "OpenAI and Mattel are partnering to integrate AI into iconic brands such as Barbie and Hot Wheels, aiming to enhance creative development, streamline workflows, and create new ways for fans to engage.",
      "publishedAt": "2025-06-12T00:00:00.000Z",
      "source": "OpenAI Blog",
      "sourceUrl": "https://openai.com/index/mattels-iconic-brands",
      "category": "companies",
      "company": "OpenAI",
      "imageUrl": null,
      "tags": [
        "AI",
        "ML",
        "OpenAI"
      ],
      "featured": false
    },
    {
      "id": "mcamd9v165pfklu1dhb",
      "title": "Enhance Your Models in 5 Minutes with the Hugging Face Kernel Hub",
      "summary": "",
      "content": "",
      "publishedAt": "2025-06-12T00:00:00.000Z",
      "source": "Hugging Face Blog",
      "sourceUrl": "https://huggingface.co/blog/hello-hf-kernels",
      "category": "tools",
      "company": "Hugging Face",
      "imageUrl": null,
      "tags": [],
      "featured": false
    },
    {
      "id": "mcamd9v1v0tl45ull49",
      "title": "Featherless AI on Hugging Face Inference Providers 🔥",
      "summary": "",
      "content": "",
      "publishedAt": "2025-06-12T00:00:00.000Z",
      "source": "Hugging Face Blog",
      "sourceUrl": "https://huggingface.co/blog/inference-providers-featherless",
      "category": "tools",
      "company": "Hugging Face",
      "imageUrl": null,
      "tags": [
        "AI"
      ],
      "featured": false
    },
    {
      "id": "mcamdn0zqffzicokld",
      "title": "Hugging Face: V-JEPA 2 (based on v4.52.4)",
      "summary": "A new model is added to transformers: V-JEPA 2\r\nIt is added on top of the v4.52.4 release, and can be installed from the following tag: `v4.52.4-VJEPA-2-preview`.\r\n\r\nIn order to install this version, please install with the following command:\r\n\r\n```\r\npip install git+https://github.com/huggingface/tr...",
      "content": "A new model is added to transformers: V-JEPA 2\r\nIt is added on top of the v4.52.4 release, and can be installed from the following tag: `v4.52.4-VJEPA-2-preview`.\r\n\r\nIn order to install this version, please install with the following command:\r\n\r\n```\r\npip install git+https://github.com/huggingface/transformers@v4.52.4-VJEPA-2-preview\r\n```\r\n\r\nIf fixes are needed, they will be applied to this release; this installation may therefore be considered as stable and improving.\r\n\r\nAs the tag implies, this tag is a **_preview_** of the VJEPA-2 model. This tag is a tagged version of the `main` branch and does not follow semantic versioning. This model will be included in the next minor release: `v4.53.0`.\r\n\r\n## VJEPA-2\r\n\r\n<div class=\"flex justify-center\">\r\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/vjepa.gif\" alt=\"drawing\" width=\"600\"/>\r\n</div>\r\n\r\nV-JEPA 2 is a self-supervised approach to training video encoders developed by FAIR, Meta. Using internet-scale video data, V-JEPA 2 attains state-of-the-art performance on motion understanding and human action anticipation tasks. V-JEPA 2-AC is a latent action-conditioned world model post-trained from V-JEPA 2 (using a small amount of robot trajectory interaction data) that solves robot manipulation tasks without environment-specific data collection or task-specific training or calibration.\r\n\r\nThe abstract from the technical report is the following:\r\n\r\n\r\n## Usage example\r\n\r\nVJEPA-2 can be found on the [Huggingface Hub](https://huggingface.co/models?other=vjepa2). V-JEPA 2 is intended to represent any video (and image) to perform video classification, retrieval, or as a video encoder for VLMs.\r\n\r\nThe snippet below shows how to load the V-JEPA 2 model using the `AutoModel` class.\r\n\r\n```py\r\nimport torch\r\nfrom torchcodec.decoders import VideoDecoder\r\nimport numpy as np\r\n\r\nprocessor = AutoVideoProcessor.from_pretrained(\"facebook/vjepa2-vitl-fpc64-256\")\r\nmodel = AutoModel.from_pretrained(\r\n    \"facebook/vjepa2-vitl-fpc64-256\",\r\n    torch_dtype=torch.float16,\r\n    device_map=\"auto\",\r\n    attn_implementation=\"sdpa\"\r\n)\r\n\r\nvideo_url = \"https://huggingface.co/datasets/nateraw/kinetics-mini/resolve/main/val/archery/-Qz25rXdMjE_000014_000024.mp4\"\r\n\r\nvr = VideoDecoder(video_url)\r\nframe_idx = np.arange(0, 64) # choosing some frames. here, you can define more complex sampling strategy\r\nvideo = vr.get_frames_at(indices=frame_idx).data  # T x C x H x W\r\nvideo = processor(video, return_tensors=\"pt\").to(model.device)\r\noutputs = model(**video)\r\n\r\n# V-JEPA 2 encoder outputs, same as calling `model.get_vision_features()`\r\nencoder_outputs = outputs.last_hidden_state\r\n\r\n# V-JEPA 2 predictor outputs\r\npredictor_outputs = outputs.predictor_output.last_hidden_state\r\n```\r\n",
      "publishedAt": "2025-06-11T14:59:35.000Z",
      "source": "Hugging Face GitHub",
      "sourceUrl": "https://github.com/huggingface/transformers/releases/tag/v4.52.4-VJEPA-2-preview",
      "category": "tools",
      "company": "Hugging Face",
      "imageUrl": null,
      "tags": [
        "AI",
        "Transformer",
        "Meta"
      ],
      "featured": false
    },
    {
      "id": "mcamdgpar39e1bkeo47",
      "title": "Anthropic: v0.54.0",
      "summary": "## 0.54.0 (2025-06-10)\n\nFull Changelog: [v0.53.0...v0.54.0](https://github.com/anthropics/anthropic-sdk-python/compare/v0.53.0...v0.54.0)\n\n### Features\n\n* **client:** add support for fine-grained-tool-streaming-2025-05-14 ([07ec081](https://github.com/anthropics/anthropic-sdk-python/commit/07ec08119...",
      "content": "## 0.54.0 (2025-06-10)\n\nFull Changelog: [v0.53.0...v0.54.0](https://github.com/anthropics/anthropic-sdk-python/compare/v0.53.0...v0.54.0)\n\n### Features\n\n* **client:** add support for fine-grained-tool-streaming-2025-05-14 ([07ec081](https://github.com/anthropics/anthropic-sdk-python/commit/07ec08119dbc328934fea5ec6eacd00c8dbda089))\n\n\n### Bug Fixes\n\n* **httpx:** resolve conflict between default transport and proxy settings ([#969](https://github.com/anthropics/anthropic-sdk-python/issues/969)) ([a6efded](https://github.com/anthropics/anthropic-sdk-python/commit/a6efdedcfef881ae3466bb77d92d0338c8338e20))\n* **tests:** update test ([99c2433](https://github.com/anthropics/anthropic-sdk-python/commit/99c243363e94f5f3f627cb8b80e3f238503c89f5))\n\n\n### Chores\n\n* **internal:** version bump ([45029f4](https://github.com/anthropics/anthropic-sdk-python/commit/45029f41c96f62f26ead99a5989c9ad974fc21b9))\n\n\n### Documentation\n\n* **contributing:** fix uv script for bootstrapping ([d2bde52](https://github.com/anthropics/anthropic-sdk-python/commit/d2bde52286ee8fa65995e73c579a8962087c1da4))",
      "publishedAt": "2025-06-11T02:45:52.000Z",
      "source": "Anthropic GitHub",
      "sourceUrl": "https://github.com/anthropics/anthropic-sdk-python/releases/tag/v0.54.0",
      "category": "tools",
      "company": "Anthropic",
      "imageUrl": null,
      "tags": [
        "AI",
        "Anthropic"
      ],
      "featured": false
    },
    {
      "id": "mcamd8ot0bpbuimp6d1b",
      "title": "Scaling security with responsible disclosure",
      "summary": "OpenAI introduces its Outbound Coordinated Disclosure Policy to guide how it responsibly reports vulnerabilities in third-party software—emphasizing integrity, collaboration, and proactive security at scale.",
      "content": "OpenAI introduces its Outbound Coordinated Disclosure Policy to guide how it responsibly reports vulnerabilities in third-party software—emphasizing integrity, collaboration, and proactive security at scale.",
      "publishedAt": "2025-06-09T10:00:00.000Z",
      "source": "OpenAI Blog",
      "sourceUrl": "https://openai.com/index/scaling-coordinated-vulnerability-disclosure",
      "category": "companies",
      "company": "OpenAI",
      "imageUrl": null,
      "tags": [
        "AI",
        "OpenAI"
      ],
      "featured": true
    },
    {
      "id": "mcamdn0z09i6yu0qx279",
      "title": "Hugging Face: ColQwen2 (based on v4.52.4)",
      "summary": "A new model is added to transformers: ColQwen2\r\nIt is added on top of the v4.52.4 release, and can be installed from the following tag: `v4.52.4-ColQwen2-preview`.\r\n\r\nIn order to install this version, please install with the following command:\r\n\r\n```\r\npip install git+https://github.com/huggingface/t...",
      "content": "A new model is added to transformers: ColQwen2\r\nIt is added on top of the v4.52.4 release, and can be installed from the following tag: `v4.52.4-ColQwen2-preview`.\r\n\r\nIn order to install this version, please install with the following command:\r\n\r\n```\r\npip install git+https://github.com/huggingface/transformers@v4.52.4-ColQwen2-preview\r\n```\r\n\r\nIf fixes are needed, they will be applied to this release; this installation may therefore be considered as stable and improving.\r\n\r\nAs the tag implies, this tag is a **_preview_** of the ColQwen2 model. This tag is a tagged version of the `main` branch and does not follow semantic versioning. This model will be included in the next minor release: `v4.53.0`.\r\n\r\n## ColQwen2\r\n\r\n![image](https://github.com/user-attachments/assets/2e92bb68-eb97-4532-b5ae-4fa378a1ee01)\r\n\r\n[ColQwen2](https://doi.org/10.48550/arXiv.2407.01449) is a variant of the [ColPali](https://github.com/huggingface/transformers/blob/c72ba6944171e2e6dd4f4a93d61b24fa52b718f5/docs/source/en/model_doc/colpali) model designed to retrieve documents by analyzing their visual features. Unlike traditional systems that rely heavily on text extraction and OCR, ColQwen2 treats each page as an image. It uses the [Qwen2-VL](https://github.com/huggingface/transformers/blob/c72ba6944171e2e6dd4f4a93d61b24fa52b718f5/docs/source/en/model_doc/qwen2_vl) backbone to capture not only text, but also the layout, tables, charts, and other visual elements to create detailed multi-vector embeddings that can be used for retrieval by computing pairwise late interaction similarity scores. This offers a more comprehensive understanding of documents and enables more efficient and accurate retrieval.\r\n\r\n\r\n## Usage example\r\n\r\nColQwen2 can be found on the [Huggingface Hub](https://huggingface.co/models?other=colqwen2).\r\n\r\n```python\r\nimport requests\r\nimport torch\r\nfrom PIL import Image\r\n\r\nfrom transformers import ColQwen2ForRetrieval, ColQwen2Processor\r\nfrom transformers.utils.import_utils import is_flash_attn_2_available\r\n\r\n# Load the model and the processor\r\nmodel_name = \"vidore/colqwen2-v1.0-hf\"\r\n\r\nmodel = ColQwen2ForRetrieval.from_pretrained(\r\n    model_name,\r\n    torch_dtype=torch.bfloat16,\r\n    device_map=\"auto\",  # \"cpu\", \"cuda\", or \"mps\" for Apple Silicon\r\n    attn_implementation=\"flash_attention_2\" if is_flash_attn_2_available() else \"sdpa\",\r\n)\r\nprocessor = ColQwen2Processor.from_pretrained(model_name)\r\n\r\n# The document page screenshots from your corpus\r\nurl1 = \"https://upload.wikimedia.org/wikipedia/commons/8/89/US-original-Declaration-1776.jpg\"\r\nurl2 = \"https://upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Romeoandjuliet1597.jpg/500px-Romeoandjuliet1597.jpg\"\r\n\r\nimages = [\r\n    Image.open(requests.get(url1, stream=True).raw),\r\n    Image.open(requests.get(url2, stream=True).raw),\r\n]\r\n\r\n# The queries you want to retrieve documents for\r\nqueries = [\r\n    \"When was the United States Declaration of Independence proclaimed?\",\r\n    \"Who printed the edition of Romeo and Juliet?\",\r\n]\r\n\r\n# Process the inputs\r\ninputs_images = processor(images=images).to(model.device)\r\ninputs_text = processor(text=queries).to(model.device)\r\n\r\n# Forward pass\r\nwith torch.no_grad():\r\n    image_embeddings = model(**inputs_images).embeddings\r\n    query_embeddings = model(**inputs_text).embeddings\r\n\r\n# Score the queries against the images\r\nscores = processor.score_retrieval(query_embeddings, image_embeddings)\r\n\r\nprint(\"Retrieval scores (query x image):\")\r\nprint(scores)\r\n```\r\n\r\nIf you have issue with loading the images with PIL, you can use the following code to create dummy images:\r\n\r\n```python\r\nimages = [\r\n    Image.new(\"RGB\", (128, 128), color=\"white\"),\r\n    Image.new(\"RGB\", (64, 32), color=\"black\"),\r\n]\r\n```\r\n\r\nQuantization reduces the memory burden of large models by representing the weights in a lower precision. Refer to the [Quantization](../quantization/overview) overview for more available quantization backends.\r\n\r\nThe example below uses [bitsandbytes](../quantization/bitsandbytes.md) to quantize the weights to int4.\r\n\r\n```python\r\nimport requests\r\nimport torch\r\nfrom PIL import Image\r\n\r\nfrom transformers import BitsAndBytesConfig, ColQwen2ForRetrieval, ColQwen2Processor\r\n\r\nmodel_name = \"vidore/colqwen2-v1.0-hf\"\r\n\r\n# 4-bit quantization configuration\r\nbnb_config = BitsAndBytesConfig(\r\n    load_in_4bit=True,\r\n    bnb_4bit_use_double_quant=True,\r\n    bnb_4bit_quant_type=\"nf4\",\r\n    bnb_4bit_compute_dtype=torch.float16,\r\n)\r\n\r\nmodel = ColQwen2ForRetrieval.from_pretrained(\r\n    model_name,\r\n    quantization_config=bnb_config,\r\n    device_map=\"cuda\",\r\n).eval()\r\n\r\nprocessor = ColQwen2Processor.from_pretrained(model_name)\r\n\r\nurl1 = \"https://upload.wikimedia.org/wikipedia/commons/8/89/US-original-Declaration-1776.jpg\"\r\nurl2 = \"https://upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Romeoandjuliet1597.jpg/500px-Romeoandjuliet1597.jpg\"\r\n\r\nimages = [\r\n    Image.open(requests.get(url1, stream=True).raw),\r\n    Image.open(requests.get(url2, stream=True).raw),\r\n]\r\n\r\nqueries = [\r\n    \"When was the United States Declaration of Independence proclaimed?\",\r\n    \"Who printed the edition of Romeo and Juliet?\",\r\n]\r\n\r\n# Process the inputs\r\ninputs_images = processor(images=images, return_tensors=\"pt\").to(model.device)\r\ninputs_text = processor(text=queries, return_tensors=\"pt\").to(model.device)\r\n\r\n# Forward pass\r\nwith torch.no_grad():\r\n    image_embeddings = model(**inputs_images).embeddings\r\n    query_embeddings = model(**inputs_text).embeddings\r\n\r\n# Score the queries against the images\r\nscores = processor.score_retrieval(query_embeddings, image_embeddings)\r\n\r\nprint(\"Retrieval scores (query x image):\")\r\nprint(scores)\r\n```",
      "publishedAt": "2025-06-02T13:07:17.000Z",
      "source": "Hugging Face GitHub",
      "sourceUrl": "https://github.com/huggingface/transformers/releases/tag/v4.52.4-ColQwen2-preview",
      "category": "tools",
      "company": "Hugging Face",
      "imageUrl": null,
      "tags": [
        "AI",
        "Transformer"
      ],
      "featured": false
    },
    {
      "id": "mcamdht72d98qmregfo",
      "title": "Meta: v0.2.0",
      "summary": "Llama 4 Support ( https://www.llama.com ) \r\n\r\n...",
      "content": "Llama 4 Support ( https://www.llama.com ) \r\n\r\n",
      "publishedAt": "2025-04-05T19:02:56.000Z",
      "source": "Meta GitHub",
      "sourceUrl": "https://github.com/meta-llama/llama-models/releases/tag/v0.2.0",
      "category": "research",
      "company": "Meta",
      "imageUrl": null,
      "tags": [
        "Llama"
      ],
      "featured": true
    },
    {
      "id": "mcamdht7kxf668yk7ik",
      "title": "Meta: v0.1.4",
      "summary": "## What's Changed\r\n* fix: do not use python_tag when encoding non-code_interpreter tool_calls by @ehhuang in https://github.com/meta-llama/llama-models/pull/283\r\n* fix: tool_call was not encoded by @ehhuang in https://github.com/meta-llama/llama-models/pull/284\r\n\r\n\r\n**Full Changelog**: https://githu...",
      "content": "## What's Changed\r\n* fix: do not use python_tag when encoding non-code_interpreter tool_calls by @ehhuang in https://github.com/meta-llama/llama-models/pull/283\r\n* fix: tool_call was not encoded by @ehhuang in https://github.com/meta-llama/llama-models/pull/284\r\n\r\n\r\n**Full Changelog**: https://github.com/meta-llama/llama-models/compare/v0.1.3...v0.1.4",
      "publishedAt": "2025-02-25T00:04:44.000Z",
      "source": "Meta GitHub",
      "sourceUrl": "https://github.com/meta-llama/llama-models/releases/tag/v0.1.4",
      "category": "research",
      "company": "Meta",
      "imageUrl": null,
      "tags": [
        "Llama",
        "Meta"
      ],
      "featured": true
    },
    {
      "id": "mcamdsouwrnlhhmfs5",
      "title": "Ask HN: Have LLM or generative AI made you more productive?",
      "summary": "I work in the gaming industry, and while there was a push to try to find integrations for AI, the most promising usages of machine learning algorithms are in areas like anti-cheat. One engineer reported success using a GPT product to generate some code which he could then correct, but the majority o...",
      "content": "I work in the gaming industry, and while there was a push to try to find integrations for AI, the most promising usages of machine learning algorithms are in areas like anti-cheat. One engineer reported success using a GPT product to generate some code which he could then correct, but the majority of his job doesn&#x27;t actually involve writing large amounts of new code so the utility is extremely limited. I don&#x27;t know of any other usages of LLM across our teams.<p>I am extremely interested in hearing of success stories from other industries, or conversely I&#x27;d love to hear about what other experiments others have been doing.",
      "publishedAt": "2024-10-04T15:28:28.000Z",
      "source": "Hacker News AI",
      "sourceUrl": "https://news.ycombinator.com/item?id=41742430",
      "category": "industry",
      "company": "Community",
      "imageUrl": null,
      "tags": [
        "GPT",
        "LLM",
        "AI"
      ],
      "featured": false
    },
    {
      "id": "mcamdsouehy6w522nen",
      "title": "Ask HN: What's the role GCP/Google's LLM play in GenAI market",
      "summary": "This ask post is very opinionated.<p>I used be a die-hard Google&#x2F;GCP lover, from wanting to enter Google as an engineer when I was a student, to proposing to try out GCP for a new project at my first internship (and succeeded). Used Firebase for my on-campus goods trading platform, course proje...",
      "content": "This ask post is very opinionated.<p>I used be a die-hard Google&#x2F;GCP lover, from wanting to enter Google as an engineer when I was a student, to proposing to try out GCP for a new project at my first internship (and succeeded). Used Firebase for my on-campus goods trading platform, course projects, and contract projects, and got good experiences. I also used Colab as my go-to choice for my study of machine learning, running models for tasks including object detection, image segmentation, basic machine learning projects, and fine-tuning transformer models for my NLP course projects.<p>I still remembered the release Google&#x27;s LaMDA model in 2021, and the beautiful blog post site. They used to be the leading role in GenAI or AI serving in general with TensorFlow serve on GCP. However, ever since last year, I really felt that Google is severely lagging behind in the cloud service or GenAI in general.<p>For GCP, especially the AI related workspace, I see no meaningful improvement over the years. The file system still has a weird triangle that when you click it does nothing, the UI is extremely laggy when the data is huge just like in colab. I don&#x27;t even want to mention the accidental deletion of Unisuper&#x27;s data.<p>For Gemini, despite it has large context window (2M for 1.5 Pro, 1M for 1.5 Flash), I see no value at all to use this. I generally judge a service&#x27;s usability on three things: model competency &gt; API (design, price, speed) &gt; playground ease of use (because you can always use other playgrounds when you have the API). Gemini sucks on every aspect. It&#x27;s never in par with GPT-4o (opinionated, tested on all my graduate CS quiz questions). The API is such a pain in the ass (500, 400 all over the place when I was following the cookbook and Gemini&#x27;s team could not even solve my issue at a hackathon!) They playground is the most counter-intuitive thing I&#x27;ve ever used in my life.<p>What&#x27;s more, the executive team of the Google seems to believe that Gemini 1.5 Pro is as satisfying as GPT-4, and Gemini 1.5 Flash is better, faster, and cheaper than GPT-3.5. First statement is false (to me). Second statement is meaningless, because you should be comparing Gemini 1.5 Flash to Llama 3 or Mistral instead of GPT-3.5. Don&#x27;t even bring up that Gemini is not fine-tunable at this moment.<p>They have a slide number that over 60% GenAI startup is using GCP. I simply don&#x27;t understand where this number is from. Almost every startup I encountered is either using OpenAI or Azure Studio (if they have some scale). I used to be a MSFT hater, but Azure AI Studio just has far better DX and model catalog compared to Vertex AI.<p>Therefore, unless my company has already signed a contract with GCP, I really don&#x27;t see the point of a new startup starting to use GCP or Gemini in general.",
      "publishedAt": "2024-05-16T07:04:28.000Z",
      "source": "Hacker News AI",
      "sourceUrl": "https://news.ycombinator.com/item?id=40375903",
      "category": "industry",
      "company": "Community",
      "imageUrl": null,
      "tags": [
        "GPT",
        "Gemini",
        "Llama",
        "LLM",
        "AI"
      ],
      "featured": false
    },
    {
      "id": "mcamdsou70wwj3uz6o6",
      "title": "Show HN: UpTrain (YC W23) – open-source tool to evaluate LLM response quality",
      "summary": "Hello, we are Shikha and Sourabh, founders of UpTrain(YC W23) - an open-source tool to evaluate the performance of your LLM applications on aspects such as correctness, tonality, hallucination, fluency, etc.<p>The Problem: Unlike traditional Machine learning or Deep learning models where we always h...",
      "content": "Hello, we are Shikha and Sourabh, founders of UpTrain(YC W23) - an open-source tool to evaluate the performance of your LLM applications on aspects such as correctness, tonality, hallucination, fluency, etc.<p>The Problem: Unlike traditional Machine learning or Deep learning models where we always have a unique Ground Truth and can define metrics like Precision, Recall, accuracy, etc. to quantify the model’s performance, LLMs are trickier and it is very difficult to estimate if their response is correct or not. If you are using GPT-4 to write a recruitment email, there is no unique correct email to do a word-to-word comparison against.<p>As you build an LLM application, you want to compare it against different model providers, prompt configurations, etc., and figure out the best working combination. Instead of manually skimming through a couple of model responses, you want to run them through hundreds of test cases, aggregate their scores, and make an informed decision. Additionally, as your application generates responses for real user queries, you don’t want to wait for them to complain about the model inaccuracy, instead, you want to monitor the model’s performance over time and get alerted in case of any drifts.<p>Again, at the core of it, you want a tool to evaluate the quality of your LLM response and assign quantitative scores.<p>The Solution: To solve this, we are building UpTrain which has a set of evaluation metrics so that you can know when your application is going wrong. These metrics include traditional NLP metrics like Rogue, Bleu, etc., embeddings similarity metrics as well as model grading scores i.e. where we use LLMs to evaluate different aspects of your response. A few of these evaluation metrics include:<p>1. Response Relevancy: Measures if the response contains any irrelevant information\n2. Response Completeness: Measures if the response answers all aspects of the given question\n3. Factual Accuracy: Measures hallucinations i.e. if the response has any made-up information or not with respect to the provided context\n4. Retrieved Context Quality: Measures if the retrieved context has sufficient information to answer the given question\n5. Response Tonality: Measures if the response aligns with a specific persona or desired tone\netc.<p>We have designed workflows so that you can easily add your testing dataset, configure which checks you want to run (you can also define custom checks suitable for your use case) and conveniently access the results via Streamlit dashboards.<p>UpTrain also has experimentation capabilities where you can specify different prompt variations and models to test across and use these quantitative checks to find the best configuration for your application.<p>You can also use UpTrain to monitor your application’s performance and find avenues for improvement. We integrate directly with your databases (BigQuery, Postgres, MongoDB, etc.) and can run daily evaluations.<p>We’ve launched the tool under an Apache 2.0 license to make it easy for everyone to integrate it into their LLM workflows. Additionally, we also provide managed service (with a free trial) where you can run LLM evaluations via an API request or through UpTrain testing console.<p>We would love for you to try it out and give your feedback.<p>Links:\nDemo: <a href=\"https:&#x2F;&#x2F;demo.uptrain.ai&#x2F;evals_demo&#x2F;\">https:&#x2F;&#x2F;demo.uptrain.ai&#x2F;evals_demo&#x2F;</a>\nGithub repo: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;uptrain-ai&#x2F;uptrain\">https:&#x2F;&#x2F;github.com&#x2F;uptrain-ai&#x2F;uptrain</a>\nCreate an account (free): <a href=\"https:&#x2F;&#x2F;uptrain.ai&#x2F;dashboard\">https:&#x2F;&#x2F;uptrain.ai&#x2F;dashboard</a>\nUpTrain testing console (need an account): <a href=\"https:&#x2F;&#x2F;demo.uptrain.ai&#x2F;dashboard\">https:&#x2F;&#x2F;demo.uptrain.ai&#x2F;dashboard</a>\nWebsite: <a href=\"https:&#x2F;&#x2F;uptrain.ai&#x2F;\">https:&#x2F;&#x2F;uptrain.ai&#x2F;</a>",
      "publishedAt": "2023-08-22T14:08:00.000Z",
      "source": "Hacker News AI",
      "sourceUrl": "https://demo.uptrain.ai/evals_demo/",
      "category": "industry",
      "company": "Community",
      "imageUrl": null,
      "tags": [
        "GPT",
        "LLM",
        "AI",
        "ML",
        "Deep Learning"
      ],
      "featured": false
    }
  ]
}