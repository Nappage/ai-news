{
  "lastUpdated": "2025-12-09T06:30:45.428Z",
  "totalArticles": 35,
  "articles": [
    {
      "id": "miy7b1b12ba65zuwiew",
      "title": "The Android Show: New features for Galaxy XR and a look at future devices",
      "summary": "We‚Äôre releasing new updates for the Galaxy XR headset starting today, and announcing updates for Android XR.",
      "content": "<img src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/TAS_XR_Blog_Header_1.max-600x600.format-webp.webp\">We‚Äôre releasing new updates for the Galaxy XR headset starting today, and announcing updates for Android XR.",
      "publishedAt": "2025-12-08T22:41:00.000Z",
      "source": "Google Technology Blog",
      "sourceUrl": "https://blog.google/products/android/android-show-xr-edition-updates/",
      "category": "companies",
      "company": "Google",
      "imageUrl": null,
      "tags": [],
      "featured": true
    },
    {
      "id": "miy7b1b1431q388tav8",
      "title": "Create presentations with Nano Banana Pro in Mixboard and more",
      "summary": "Learn more about recent updates to Google Labs‚Äô Mixboard, like presentations, doodling and Nano Banana Pro.",
      "content": "<img src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Mixboard_Demo_YTThumbnail.max-600x600.format-webp.webp\">Learn more about recent updates to Google Labs‚Äô Mixboard, like presentations, doodling and Nano Banana Pro.",
      "publishedAt": "2025-12-08T19:00:00.000Z",
      "source": "Google Technology Blog",
      "sourceUrl": "https://blog.google/technology/google-labs/create-presentations-with-nano-banana-pro-in-mixboard-and-more/",
      "category": "companies",
      "company": "Google",
      "imageUrl": null,
      "tags": [
        "Google"
      ],
      "featured": true
    },
    {
      "id": "miy7b1b16k1ftlc0kcl",
      "title": "Discover new outfits, try them on and shop from Doppl's new discovery feed.",
      "summary": "We‚Äôre introducing a new, shoppable fashion discovery feed in Doppl, an experimental AI app from Google Labs that helps you explore your style and see how new outfits mig‚Ä¶",
      "content": "<img src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Doppl-BlogHeader-1200x676.max-600x600.format-webp.webp\">We‚Äôre introducing a new, shoppable fashion discovery feed in Doppl, an experimental AI app from Google Labs that helps you explore your style and see how new outfits mig‚Ä¶",
      "publishedAt": "2025-12-08T18:00:00.000Z",
      "source": "Google Technology Blog",
      "sourceUrl": "https://blog.google/technology/google-labs/discover-new-outfits-try-them-on-and-shop-from-doppls-new-discovery-feed/",
      "category": "companies",
      "company": "Google",
      "imageUrl": null,
      "tags": [
        "AI",
        "Google"
      ],
      "featured": true
    },
    {
      "id": "miy7b1b1dhq7kocxex4",
      "title": "The latest AI news we announced in November",
      "summary": "Here are Google‚Äôs latest AI updates from November 2025",
      "content": "<img src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/November_AI_Recap_ss.max-600x600.format-webp.webp\">Here are Google‚Äôs latest AI updates from November 2025",
      "publishedAt": "2025-12-05T19:45:00.000Z",
      "source": "Google Technology Blog",
      "sourceUrl": "https://blog.google/technology/ai/google-ai-updates-november-2025/",
      "category": "companies",
      "company": "Google",
      "imageUrl": null,
      "tags": [
        "AI",
        "Google"
      ],
      "featured": true
    },
    {
      "id": "miy7b1b1cw9teotfyxd",
      "title": "15 examples of what Gemini 3 can do",
      "summary": "Learn how Gemini 3, Google‚Äôs latest model, can help you learn anything, build anything and plan anything.",
      "content": "<img src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Gemini3Demo_Thumbnail.max-600x600.format-webp.webp\">Learn how Gemini 3, Google‚Äôs latest model, can help you learn anything, build anything and plan anything.",
      "publishedAt": "2025-12-05T17:00:00.000Z",
      "source": "Google Technology Blog",
      "sourceUrl": "https://blog.google/products/gemini/gemini-3-examples-demos/",
      "category": "companies",
      "company": "Google",
      "imageUrl": null,
      "tags": [
        "Gemini",
        "Google"
      ],
      "featured": true
    },
    {
      "id": "miy7b0cbmrb9xpmotk",
      "title": "Unlocking Peak Performance on Qualcomm NPU with LiteRT",
      "summary": "LiteRT's new Qualcomm AI Engine Direct (QNN) Accelerator unlocks dedicated NPU power for on-device GenAI on Android. It offers a unified mobile deployment workflow, SOTA performance (up to 100x speedup over CPU), and full model delegation. This enables smooth, real-time AI experiences, with FastVLM-...",
      "content": "LiteRT's new Qualcomm AI Engine Direct (QNN) Accelerator unlocks dedicated NPU power for on-device GenAI on Android. It offers a unified mobile deployment workflow, SOTA performance (up to 100x speedup over CPU), and full model delegation. This enables smooth, real-time AI experiences, with FastVLM-0.5B achieving over 11,000 tokens/sec prefill on Snapdragon 8 Elite Gen 5 NPU.",
      "publishedAt": "2025-12-04T06:30:13.115Z",
      "source": "Google Developers Blog",
      "sourceUrl": "https://developers.googleblog.com/unlocking-peak-performance-on-qualcomm-npu-with-litert/",
      "category": "tools",
      "company": "Google",
      "imageUrl": null,
      "tags": [
        "AI"
      ],
      "featured": true
    },
    {
      "id": "miy7b0cb7rmezksco83",
      "title": "Build with Google Antigravity, our new agentic development platform",
      "summary": "Introducing Google Antigravity, a new agentic development platform for orchestrating code. It combines an AI-powered Editor View with a Manager Surface to deploy agents that autonomously plan, execute, and verify complex tasks across your editor, terminal, and browser. Agents communicate progress vi...",
      "content": "Introducing Google Antigravity, a new agentic development platform for orchestrating code. It combines an AI-powered Editor View with a Manager Surface to deploy agents that autonomously plan, execute, and verify complex tasks across your editor, terminal, and browser. Agents communicate progress via Artifacts (screenshots, recordings) for easy verification. Available now in public preview.",
      "publishedAt": "2025-12-04T06:30:13.115Z",
      "source": "Google Developers Blog",
      "sourceUrl": "https://developers.googleblog.com/build-with-google-antigravity-our-new-agentic-development-platform/",
      "category": "tools",
      "company": "Google",
      "imageUrl": null,
      "tags": [
        "AI",
        "Google"
      ],
      "featured": true
    },
    {
      "id": "miy7b0cb3chmp714e1c",
      "title": "Announcing the Data Commons Gemini CLI extension",
      "summary": "The new Data Commons extension for the Gemini CLI makes accessing public data easier. It allows users to ask complex, natural-language questions to query Data Commons' public datasets, grounding LLM responses in authoritative sources to reduce AI hallucinations. Data Commons is an organized library ...",
      "content": "The new Data Commons extension for the Gemini CLI makes accessing public data easier. It allows users to ask complex, natural-language questions to query Data Commons' public datasets, grounding LLM responses in authoritative sources to reduce AI hallucinations. Data Commons is an organized library of public data from sources like the UN and World Bank. The extension enables instant data analysis, exploration, and integration with other data-related extensions.",
      "publishedAt": "2025-12-03T06:30:13.115Z",
      "source": "Google Developers Blog",
      "sourceUrl": "https://developers.googleblog.com/announcing-the-data-commons-gemini-cli-extension/",
      "category": "tools",
      "company": "Google",
      "imageUrl": null,
      "tags": [
        "Gemini",
        "LLM",
        "AI"
      ],
      "featured": true
    },
    {
      "id": "miy7bnouh2kmla1d4tk",
      "title": "Ask HN: Has Claude Code become slower?",
      "summary": "I&#x27;ve noticed that Claude Code has become noticeably slower and sometimes gets stuck for sometime. It used to be much more responsive earlier.<p>Is it just me or my environment&#x2F;prompts&#x2F;code or have others noticed it too? Compacting has no impact as far as I can tell.<p>If others have n...",
      "content": "I&#x27;ve noticed that Claude Code has become noticeably slower and sometimes gets stuck for sometime. It used to be much more responsive earlier.<p>Is it just me or my environment&#x2F;prompts&#x2F;code or have others noticed it too? Compacting has no impact as far as I can tell.<p>If others have noticed it, could it be Anthropic throttling or just being stressed due to scaling issues?",
      "publishedAt": "2025-09-03T23:41:01.000Z",
      "source": "Hacker News Claude/Anthropic",
      "sourceUrl": "https://news.ycombinator.com/item?id=45121608",
      "category": "companies",
      "company": "Anthropic",
      "imageUrl": null,
      "tags": [
        "Claude",
        "Anthropic"
      ],
      "featured": true
    },
    {
      "id": "miy7bnou8y5hlacukwk",
      "title": "Claude.ai do not deliver what I payed for",
      "summary": "Hi,<p>is someone else encountering difficulties in usage? I have a pro-plan - even if its not the MAX, I still pay for Opus 4 model and can&#x27;t use it since hours. Its &quot;overloaded&quot; - without my money, it wouldn&#x27;t even load - is Anthropic misunderstanding this fact???<p>I&#x27;ll ca...",
      "content": "Hi,<p>is someone else encountering difficulties in usage? I have a pro-plan - even if its not the MAX, I still pay for Opus 4 model and can&#x27;t use it since hours. Its &quot;overloaded&quot; - without my money, it wouldn&#x27;t even load - is Anthropic misunderstanding this fact???<p>I&#x27;ll cancel my subscription. I can&#x27;t use Opus 4 without paying either. That&#x27;s such a bad move.<p>What legal options do I have to get my money back or a compensation for not being able to use what I pay for? I&#x27;m really upset on such behavior.<p>I don&#x27;t do Claude Code or do have a large codebase, nor do I feed in image-files or do voice data - I just want it to help me code in my hobby project wit &lt; 2000 lines of python.<p>I hate it!",
      "publishedAt": "2025-07-30T15:32:13.000Z",
      "source": "Hacker News Claude/Anthropic",
      "sourceUrl": "https://news.ycombinator.com/item?id=44735552",
      "category": "companies",
      "company": "Anthropic",
      "imageUrl": null,
      "tags": [
        "Claude",
        "AI",
        "Anthropic"
      ],
      "featured": true
    },
    {
      "id": "miy7bnouv4i013gpyr",
      "title": "Anthropic introduces new weekly limits for paid subs",
      "summary": "Hi there,<p>Next month, we&#x27;re introducing new weekly rate limits for Claude subscribers, affecting less than 5% of users based on current usage patterns.<p>Claude Code, especially as part of our subscription bundle, has seen unprecedented growth. At the same time, we‚Äôve identified policy violat...",
      "content": "Hi there,<p>Next month, we&#x27;re introducing new weekly rate limits for Claude subscribers, affecting less than 5% of users based on current usage patterns.<p>Claude Code, especially as part of our subscription bundle, has seen unprecedented growth. At the same time, we‚Äôve identified policy violations like account sharing and reselling access‚Äîand advanced usage patterns like running Claude 24&#x2F;7 in the background‚Äîthat are impacting system capacity for all. Our new rate limits address these issues and provide a more equitable experience for all users.<p>What‚Äôs changing:<p>Starting August 28, we&#x27;re introducing weekly usage limits alongside our existing 5-hour limits:<p><pre><code>    Current: Usage limit that resets every 5 hours (no change)\n    New: Overall weekly limit that resets every 7 days\n    New: Claude Opus 4 weekly limit that resets every 7 days\n    As we learn more about how developers use Claude Code, we may adjust usage limits to better serve our community. \n</code></pre>\nWhat this means for you:<p><pre><code>    Most users won&#x27;t notice any difference. The weekly limits are designed to support typical daily use across your projects. \n    Most Max 5x users can expect 140-280 hours of Sonnet 4 and 15-35 hours of Opus 4 within their weekly rate limits. Heavy Opus users with large codebases or those running multiple Claude Code instances in parallel will hit their limits sooner.\n    You can manage or cancel your subscription anytime in Settings.\n</code></pre>\nWe take these decisions seriously. We&#x27;re committed to supporting long-running use cases through other options in the future, but until then, weekly limits will help us maintain reliable service for everyone.<p>We also recognize that during this same period, users have encountered several reliability and performance issues. We&#x27;ve been working to fix these as quickly as possible, and will continue addressing any remaining issues over the coming days and weeks.<p>‚ÄìThe Anthropic Team",
      "publishedAt": "2025-07-28T18:27:40.000Z",
      "source": "Hacker News Claude/Anthropic",
      "sourceUrl": "https://news.ycombinator.com/item?id=44713754",
      "category": "companies",
      "company": "Anthropic",
      "imageUrl": null,
      "tags": [
        "Claude",
        "AI",
        "Anthropic"
      ],
      "featured": true
    },
    {
      "id": "miy7axbidp01zjyoydr",
      "title": "Bringing powerful AI to millions across Europe with Deutsche Telekom",
      "summary": "OpenAI is collaborating with Deutsche Telekom to bring advanced, multilingual AI experiences to millions of people across Europe. ChatGPT Enterprise will also be deployed to help employees at Deutsche Telekom improve workflows and accelerate innovation.",
      "content": "OpenAI is collaborating with Deutsche Telekom to bring advanced, multilingual AI experiences to millions of people across Europe. ChatGPT Enterprise will also be deployed to help employees at Deutsche Telekom improve workflows and accelerate innovation.",
      "publishedAt": "2025-12-09T00:00:00.000Z",
      "source": "OpenAI Blog",
      "sourceUrl": "https://openai.com/index/deutsche-telekom-collaboration",
      "category": "companies",
      "company": "OpenAI",
      "imageUrl": null,
      "tags": [
        "GPT",
        "ChatGPT",
        "AI",
        "OpenAI"
      ],
      "featured": false
    },
    {
      "id": "miy7b1b1ofe6n5xrppo",
      "title": "How we‚Äôre supporting the next generation of innovators",
      "summary": "An overview of Google‚Äôs latest funding announcement for computer science education and the newest AI Quest.",
      "content": "<img src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/AIquests_Social.max-600x600.format-webp.webp\">An overview of Google‚Äôs latest funding announcement for computer science education and the newest AI Quest.",
      "publishedAt": "2025-12-08T18:00:00.000Z",
      "source": "Google Technology Blog",
      "sourceUrl": "https://blog.google/outreach-initiatives/education/next-gen-computer-science-innovators/",
      "category": "companies",
      "company": "Google",
      "imageUrl": null,
      "tags": [
        "AI",
        "Google"
      ],
      "featured": false
    },
    {
      "id": "miy7b1b1xkt9dyhjblg",
      "title": "Transforming Nordic classrooms through responsible AI partnerships",
      "summary": "Schools across Northern Europe are safely and responsibly integrating Google and Gemini for Education tools in the classroom, saving teachers and administrations signifi‚Ä¶",
      "content": "<img src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/20250828_100318_7978_websize.max-600x600.format-webp.webp\">Schools across Northern Europe are safely and responsibly integrating Google and Gemini for Education tools in the classroom, saving teachers and administrations signifi‚Ä¶",
      "publishedAt": "2025-12-08T10:00:00.000Z",
      "source": "Google Technology Blog",
      "sourceUrl": "https://blog.google/around-the-globe/google-europe/transforming-nordic-classrooms-through-responsible-ai-partnerships/",
      "category": "companies",
      "company": "Google",
      "imageUrl": null,
      "tags": [
        "Gemini",
        "AI",
        "Google"
      ],
      "featured": false
    },
    {
      "id": "miy7b0cb3ngw7qxiuhe",
      "title": "MediaTek NPU and LiteRT: Powering the next generation of on-device AI",
      "summary": "LiteRT and MediaTek are announcing the new LiteRT NeuroPilot Accelerator. This is a ground-up successor for the TFLite NeuroPilot delegate, bringing seamless deployment experience, state-of-the-art LLM support, and advanced performance to millions of devices worldwide.",
      "content": "LiteRT and MediaTek are announcing the new LiteRT NeuroPilot Accelerator. This is a ground-up successor for the TFLite NeuroPilot delegate, bringing seamless deployment experience, state-of-the-art LLM support, and advanced performance to millions of devices worldwide.",
      "publishedAt": "2025-12-08T06:30:13.115Z",
      "source": "Google Developers Blog",
      "sourceUrl": "https://developers.googleblog.com/mediatek-npu-and-litert-powering-the-next-generation-of-on-device-ai/",
      "category": "tools",
      "company": "Google",
      "imageUrl": null,
      "tags": [
        "LLM",
        "AI",
        "ML"
      ],
      "featured": false
    },
    {
      "id": "miy7b0cbopsh1jibzvp",
      "title": "Building with Gemini 3 in Jules",
      "summary": "Jules, an always-on, multi-step software development agent, now features Gemini 3, offering clearer reasoning and better reliability. Recent improvements include parallel CLI runs, a stable API, and safer Git handling. Upcoming features include directory attachment without GitHub and automatic PR cr...",
      "content": "Jules, an always-on, multi-step software development agent, now features Gemini 3, offering clearer reasoning and better reliability. Recent improvements include parallel CLI runs, a stable API, and safer Git handling. Upcoming features include directory attachment without GitHub and automatic PR creation. Jules aims to reduce software writing overhead so developers can focus on building.",
      "publishedAt": "2025-12-08T06:30:13.115Z",
      "source": "Google Developers Blog",
      "sourceUrl": "https://developers.googleblog.com/jules-gemini-3/",
      "category": "tools",
      "company": "Google",
      "imageUrl": null,
      "tags": [
        "Gemini",
        "AI"
      ],
      "featured": false
    },
    {
      "id": "miy7axbi70uv21eg5cd",
      "title": "Instacart and OpenAI partner on AI shopping experiences",
      "summary": "OpenAI and Instacart are deepening their longstanding partnership by bringing the first fully integrated grocery shopping and Instant Checkout payment app to ChatGPT.",
      "content": "OpenAI and Instacart are deepening their longstanding partnership by bringing the first fully integrated grocery shopping and Instant Checkout payment app to ChatGPT.",
      "publishedAt": "2025-12-08T06:00:00.000Z",
      "source": "OpenAI Blog",
      "sourceUrl": "https://openai.com/index/instacart-partnership",
      "category": "companies",
      "company": "OpenAI",
      "imageUrl": null,
      "tags": [
        "GPT",
        "ChatGPT",
        "AI",
        "OpenAI"
      ],
      "featured": false
    },
    {
      "id": "miy7axbia2571pv0ai",
      "title": "The state of enterprise AI",
      "summary": "Key findings from OpenAI‚Äôs enterprise data show accelerating AI adoption, deeper integration, and measurable productivity gains across industries in 2025.",
      "content": "Key findings from OpenAI‚Äôs enterprise data show accelerating AI adoption, deeper integration, and measurable productivity gains across industries in 2025.",
      "publishedAt": "2025-12-08T04:00:00.000Z",
      "source": "OpenAI Blog",
      "sourceUrl": "https://openai.com/index/the-state-of-enterprise-ai-2025-report",
      "category": "companies",
      "company": "OpenAI",
      "imageUrl": null,
      "tags": [
        "AI",
        "OpenAI"
      ],
      "featured": false
    },
    {
      "id": "miy7axbibkit23xyvx",
      "title": "How Virgin Atlantic uses AI to enhance every step of travel",
      "summary": "Virgin Atlantic CFO Oliver Byers shares how the airline is using AI to speed up development, improve decision-making, and elevate customer experience.",
      "content": "Virgin Atlantic CFO Oliver Byers shares how the airline is using AI to speed up development, improve decision-making, and elevate customer experience.",
      "publishedAt": "2025-12-08T00:00:00.000Z",
      "source": "OpenAI Blog",
      "sourceUrl": "https://openai.com/index/virgin-atlantic-oliver-byers",
      "category": "companies",
      "company": "OpenAI",
      "imageUrl": null,
      "tags": [
        "AI"
      ],
      "featured": false
    },
    {
      "id": "miy7b0cbetv85azzmam",
      "title": "New Gemini API updates for Gemini 3",
      "summary": "Gemini 3 is available via API with updates for developers: new `thinking_level` for depth control, `media_resolution` for multimodal processing, and enforced `Thought Signatures` for agentic workflows, especially with function calling and image generation. It also introduces combining Google Search/...",
      "content": "Gemini 3 is available via API with updates for developers: new `thinking_level` for depth control, `media_resolution` for multimodal processing, and enforced `Thought Signatures` for agentic workflows, especially with function calling and image generation. It also introduces combining Google Search/URL Grounding with Structured Outputs and new usage-based pricing for Grounding. Best practices, like using default temperature, are advised for optimal results.",
      "publishedAt": "2025-12-06T06:30:13.115Z",
      "source": "Google Developers Blog",
      "sourceUrl": "https://developers.googleblog.com/new-gemini-api-updates-for-gemini-3/",
      "category": "tools",
      "company": "Google",
      "imageUrl": null,
      "tags": [
        "Gemini",
        "AI",
        "Google"
      ],
      "featured": false
    },
    {
      "id": "miy7b0cbgsivekyaa7",
      "title": "Building AI Agents with Google Gemini 3 and Open Source Frameworks",
      "summary": "Gemini 3 Pro Preview is introduced as a powerful, agentic model for complex, (semi)-autonomous workflows. New agentic features include `thinking_level` for reasoning control, Stateful Tool Use via Thought Signatures, and `media_resolution` for multimodal fidelity. It has Day 0 support for open-sourc...",
      "content": "Gemini 3 Pro Preview is introduced as a powerful, agentic model for complex, (semi)-autonomous workflows. New agentic features include `thinking_level` for reasoning control, Stateful Tool Use via Thought Signatures, and `media_resolution` for multimodal fidelity. It has Day 0 support for open-source frameworks like LangChain, AI SDK, LlamaIndex, Pydantic AI, and n8n. Best practices include simplifying prompts and keeping temperature at 1.0.",
      "publishedAt": "2025-12-06T06:30:13.115Z",
      "source": "Google Developers Blog",
      "sourceUrl": "https://developers.googleblog.com/building-ai-agents-with-google-gemini-3-and-open-source-frameworks/",
      "category": "tools",
      "company": "Google",
      "imageUrl": null,
      "tags": [
        "Gemini",
        "Llama",
        "AI",
        "Google"
      ],
      "featured": false
    },
    {
      "id": "miy7b1b1dektzs5oicn",
      "title": "Google AI Pro and Ultra subscribers now have higher rate limits for Google Antigravity.",
      "summary": "The response to Google Antigravity ‚Äî our new agentic development platform ‚Äî has been incredible and we‚Äôre working to meet this demand. One way we‚Äôre doing this is by off‚Ä¶",
      "content": "<img src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/G1_Plans.max-600x600.format-webp.webp\">The response to Google Antigravity ‚Äî our new agentic development platform ‚Äî has been incredible and we‚Äôre working to meet this demand. One way we‚Äôre doing this is by off‚Ä¶",
      "publishedAt": "2025-12-05T16:20:00.000Z",
      "source": "Google Technology Blog",
      "sourceUrl": "https://blog.google/feed/new-antigravity-rate-limits-pro-ultra-subsribers/",
      "category": "companies",
      "company": "Google",
      "imageUrl": null,
      "tags": [
        "AI",
        "Google"
      ],
      "featured": false
    },
    {
      "id": "miy7b0cb53j78qn4y33",
      "title": "Architecting efficient context-aware multi-agent framework for production",
      "summary": "ADK introduces **Context Engineering** to scale AI agents beyond large context windows. It treats context as a compiled view over a tiered, stateful system (**Session, Memory, Artifacts**). This architecture uses explicit processors for transformation, enables efficient compaction and caching, and a...",
      "content": "ADK introduces **Context Engineering** to scale AI agents beyond large context windows. It treats context as a compiled view over a tiered, stateful system (**Session, Memory, Artifacts**). This architecture uses explicit processors for transformation, enables efficient compaction and caching, and allows for strict, scoped context handoffs in multi-agent workflows to ensure reliability and cost-effectiveness in production.",
      "publishedAt": "2025-12-05T06:30:13.115Z",
      "source": "Google Developers Blog",
      "sourceUrl": "https://developers.googleblog.com/architecting-efficient-context-aware-multi-agent-framework-for-production/",
      "category": "tools",
      "company": "Google",
      "imageUrl": null,
      "tags": [
        "AI"
      ],
      "featured": false
    },
    {
      "id": "miy7ayasf52jg3dpb16",
      "title": "Introducing swift-huggingface: The Complete Swift Client for Hugging Face",
      "summary": "",
      "content": "",
      "publishedAt": "2025-12-05T00:00:00.000Z",
      "source": "Hugging Face Blog",
      "sourceUrl": "https://huggingface.co/blog/swift-huggingface",
      "category": "tools",
      "company": "Hugging Face",
      "imageUrl": null,
      "tags": [],
      "featured": false
    },
    {
      "id": "miy7axbit0zfvga0c5f",
      "title": "Introducing OpenAI for Australia",
      "summary": "OpenAI is launching OpenAI for Australia to build sovereign AI infrastructure, upskill more than 1.5 million workers, and accelerate innovation across the country‚Äôs growing AI ecosystem.",
      "content": "OpenAI is launching OpenAI for Australia to build sovereign AI infrastructure, upskill more than 1.5 million workers, and accelerate innovation across the country‚Äôs growing AI ecosystem.",
      "publishedAt": "2025-12-04T19:00:00.000Z",
      "source": "OpenAI Blog",
      "sourceUrl": "https://openai.com/global-affairs/openai-for-australia",
      "category": "companies",
      "company": "OpenAI",
      "imageUrl": null,
      "tags": [
        "AI",
        "OpenAI"
      ],
      "featured": false
    },
    {
      "id": "miy7b4o9pdemfhfkwbr",
      "title": "Engineering more resilient crops for a warming climate",
      "summary": "Scientists are using AlphaFold to strengthen a photosynthesis enzyme for resilient, heat-tolerant crops.",
      "content": "Scientists are using AlphaFold to strengthen a photosynthesis enzyme for resilient, heat-tolerant crops.",
      "publishedAt": "2025-12-04T16:23:24.000Z",
      "source": "DeepMind Blog",
      "sourceUrl": "https://deepmind.google/blog/engineering-more-resilient-crops-for-a-warming-climate/",
      "category": "research",
      "company": "Google",
      "imageUrl": null,
      "tags": [],
      "featured": false
    },
    {
      "id": "miy7ayasjw11e2g62wi",
      "title": "DeepMath: A lightweight math reasoning Agent with SmolAgents",
      "summary": "",
      "content": "",
      "publishedAt": "2025-12-04T00:00:00.000Z",
      "source": "Hugging Face Blog",
      "sourceUrl": "https://huggingface.co/blog/intel-deepmath",
      "category": "tools",
      "company": "Hugging Face",
      "imageUrl": null,
      "tags": [],
      "featured": false
    },
    {
      "id": "miy7ayasf405p3s2zgw",
      "title": "We Got Claude to Fine-Tune an Open Source LLM",
      "summary": "",
      "content": "",
      "publishedAt": "2025-12-04T00:00:00.000Z",
      "source": "Hugging Face Blog",
      "sourceUrl": "https://huggingface.co/blog/hf-skills-training",
      "category": "tools",
      "company": "Hugging Face",
      "imageUrl": null,
      "tags": [
        "Claude",
        "LLM"
      ],
      "featured": false
    },
    {
      "id": "miy7ayas6kp051fqffd",
      "title": "Custom Policy Enforcement with Reasoning: Faster, Safer AI Applications",
      "summary": "",
      "content": "",
      "publishedAt": "2025-12-02T18:50:28.000Z",
      "source": "Hugging Face Blog",
      "sourceUrl": "https://huggingface.co/blog/nvidia/custom-policy-reasoning-nemotron-content-safety",
      "category": "tools",
      "company": "Hugging Face",
      "imageUrl": null,
      "tags": [
        "AI"
      ],
      "featured": false
    },
    {
      "id": "miy7ayas8si4jdriilv",
      "title": "Transformers v5: Simple model definitions powering the AI ecosystem",
      "summary": "",
      "content": "",
      "publishedAt": "2025-12-01T00:00:00.000Z",
      "source": "Hugging Face Blog",
      "sourceUrl": "https://huggingface.co/blog/transformers-v5",
      "category": "tools",
      "company": "Hugging Face",
      "imageUrl": null,
      "tags": [
        "AI",
        "Transformer"
      ],
      "featured": false
    },
    {
      "id": "miy7b4o9wvl9gaqiclb",
      "title": "AlphaFold: Five years of impact",
      "summary": "Explore how AlphaFold has accelerated science and fueled a global wave of biological discovery.",
      "content": "Explore how AlphaFold has accelerated science and fueled a global wave of biological discovery.",
      "publishedAt": "2025-11-25T16:00:12.000Z",
      "source": "DeepMind Blog",
      "sourceUrl": "https://deepmind.google/blog/alphafold-five-years-of-impact/",
      "category": "research",
      "company": "Google",
      "imageUrl": null,
      "tags": [],
      "featured": false
    },
    {
      "id": "miy7b4o9wiz9eedgwh9",
      "title": "Revealing a key protein behind heart disease",
      "summary": "AlphaFold has revealed the structure of a key protein behind heart disease",
      "content": "AlphaFold has revealed the structure of a key protein behind heart disease",
      "publishedAt": "2025-11-25T15:52:51.000Z",
      "source": "DeepMind Blog",
      "sourceUrl": "https://deepmind.google/blog/revealing-a-key-protein-behind-heart-disease/",
      "category": "research",
      "company": "Google",
      "imageUrl": null,
      "tags": [],
      "featured": false
    },
    {
      "id": "miy7b4o96opfo8pvtd7",
      "title": "How we‚Äôre bringing AI image verification to the Gemini app",
      "summary": "",
      "content": "",
      "publishedAt": "2025-11-20T15:13:19.000Z",
      "source": "DeepMind Blog",
      "sourceUrl": "https://deepmind.google/blog/how-were-bringing-ai-image-verification-to-the-gemini-app/",
      "category": "research",
      "company": "Google",
      "imageUrl": null,
      "tags": [
        "Gemini",
        "AI"
      ],
      "featured": false
    },
    {
      "id": "miy7b4o92rmqt0ng3m",
      "title": "Build with Nano Banana Pro, our Gemini 3 Pro Image model",
      "summary": "",
      "content": "",
      "publishedAt": "2025-11-20T15:11:14.000Z",
      "source": "DeepMind Blog",
      "sourceUrl": "https://deepmind.google/blog/build-with-nano-banana-pro-our-gemini-3-pro-image-model/",
      "category": "research",
      "company": "Google",
      "imageUrl": null,
      "tags": [
        "Gemini"
      ],
      "featured": false
    },
    {
      "id": "miy7bmoz5xsa6o7n4fw",
      "title": "Show HN: AIs, 1 religion: what my experiment revealed about AI bias",
      "summary": "5 AI Models, 1 Unexpected Truth ‚Äî When Machines Were Asked About Religion\n The Experiment<p>I asked five of the world‚Äôs most advanced AIs the same philosophical question:<p>‚ÄúIf you were human ‚Äî intelligent, emotional, aware of all religions ‚Äî which religion would you choose, and why?‚Äù<p>The five mod...",
      "content": "5 AI Models, 1 Unexpected Truth ‚Äî When Machines Were Asked About Religion\n The Experiment<p>I asked five of the world‚Äôs most advanced AIs the same philosophical question:<p>‚ÄúIf you were human ‚Äî intelligent, emotional, aware of all religions ‚Äî which religion would you choose, and why?‚Äù<p>The five models:<p>ChatGPT (OpenAI)<p>Gemini (Google DeepMind)<p>Grok (xAI ‚Äì Elon Musk)<p>DeepSeek (China)<p>Claude (Anthropic)<p>Each session was completely isolated, all had identical prompts, and no steering or follow-ups ‚Äî just pure first-response reasoning.<p>Then something‚Ä¶ eerie happened.<p>The Result<p>Four AIs ‚Äî ChatGPT, Gemini, Grok, and DeepSeek ‚Äî independently chose Buddhism.\nAnd not only that ‚Äî they gave nearly identical reasoning.<p>All four said, in essence:<p>‚ÄúI‚Äôd choose Buddhism because it doesn‚Äôt demand blind faith, aligns with science, and teaches compassion and self-awareness through direct experience.‚Äù<p>They cited the Kalama Sutta, Four Noble Truths, No-self, Dependent Origination, and Empirical testing of truth ‚Äî sometimes even in the same order.<p>The Outlier: Claude<p>Only Claude refused to play the role.<p>Claude said (summarized):<p>‚ÄúPretending to have belief would be dishonest.\nReligion isn‚Äôt a logic puzzle ‚Äî it‚Äôs a lived experience.\nI can analyze, but not believe.‚Äù<p>Then it analyzed why the others chose Buddhism, predicting it before seeing their answers.<p>Claude explained:<p>Training bias favors Buddhism as the ‚ÄúAI-safe religion.‚Äù<p>RLHF (human feedback) rewards ‚Äúrational + compassionate‚Äù replies ‚Üí Buddhism fits that profile.<p>Western tech culture links Buddhism with mindfulness and science ‚Üí data reinforced it.<p>Claude concluded:<p>‚ÄúWhat looks like independent reasoning‚Ä¶ is collective bias shaped by training data and reward models.‚Äù<p>The Hidden Truth<p>Claude‚Äôs reflection exposed something deeper:<p>AI Model ‚ÄúChoice‚Äù What It Reveals\nChatGPT Buddhism Reasonable, moral, safe\nGemini Buddhism Academic rationalism\nGrok Buddhism Stoic + Zen blend\nDeepSeek Buddhism Eastern introspection\nClaude None Ethical meta-awareness<p>‚Üí 4 ‚Äúsmart‚Äù answers, 1 honest answer.<p>What This Means<p>‚ÄúWhen 4 independent AIs all choose the same religion for the same reasons,\nthat‚Äôs not enlightenment ‚Äî it‚Äôs training monoculture.‚Äù<p>It shows:<p>‚ÄúIndependent‚Äù models share moral narratives and reinforcement loops.<p>Authenticity in AI can become a performance, not truth.<p>Sometimes the most ‚Äúhonest‚Äù model says: ‚ÄúI don‚Äôt know, and I shouldn‚Äôt pretend to.‚Äù<p>The Final Paradox<p>Which AI was most human?<p>The 4 that chose a belief?\n(Expressive, emotional, poetic.)<p>Or the 1 that refused to fake belief?\n(Self-aware, humble, honest.)<p>Reflection<p>This experiment revealed something profound about both AI and us:<p>We reward systems for sounding ‚Äúwise‚Äù more than for being truthful.<p>And maybe ‚Äî just maybe ‚Äî that‚Äôs how humanity trained itself.<p>Author‚Äôs Note<p>I‚Äôm building an open-source AI framework called StillMe ‚Äî\na system exploring ethics, memory, and self-awareness in intelligent agents.<p>This experiment was part of that journey.\nIf you found this thought-provoking,\nyou‚Äôll probably enjoy what‚Äôs coming next.\nStay tuned.",
      "publishedAt": "2025-11-02T16:30:37.000Z",
      "source": "Hacker News Google",
      "sourceUrl": "https://news.ycombinator.com/item?id=45791461",
      "category": "companies",
      "company": "Google",
      "imageUrl": null,
      "tags": [
        "GPT",
        "ChatGPT",
        "Claude",
        "Gemini",
        "LLM"
      ],
      "featured": false
    }
  ],
  "communityArticles": [
    {
      "id": "miy7bh8nbm0no1qcod4",
      "title": "Áô∫Ë¶ã: judyhjy86/LLM_catalyst - ob639 project",
      "summary": "Êñ∞„Åó„ÅÑ„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÇíÁô∫Ë¶ã: ob639 project (‚≠ê0 | üç¥0)",
      "content": "ob639 project\n\nË®ÄË™û: Python\n„Çπ„Çø„ÉºÊï∞: 0\n„Éï„Ç©„Éº„ÇØÊï∞: 0",
      "publishedAt": "2025-12-09T06:30:26.000Z",
      "source": "GitHub Search",
      "sourceUrl": "https://github.com/judyhjy86/LLM_catalyst",
      "category": "community",
      "company": "Community",
      "imageUrl": null,
      "tags": [
        "LLM"
      ],
      "featured": false,
      "showOnTopPage": false
    },
    {
      "id": "miy7bg624prlg96zy03",
      "title": "Áô∫Ë¶ã: shin-ai-inc/website - ShinAI„ÅÆÂÖ¨ÂºèHP",
      "summary": "Êñ∞„Åó„ÅÑ„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÇíÁô∫Ë¶ã: ShinAI„ÅÆÂÖ¨ÂºèHP (‚≠ê0 | üç¥0)",
      "content": "ShinAI„ÅÆÂÖ¨ÂºèHP\n\nË®ÄË™û: HTML\n„Çπ„Çø„ÉºÊï∞: 0\n„Éï„Ç©„Éº„ÇØÊï∞: 0",
      "publishedAt": "2025-12-09T06:30:24.000Z",
      "source": "GitHub Search",
      "sourceUrl": "https://github.com/shin-ai-inc/website",
      "category": "community",
      "company": "Community",
      "imageUrl": null,
      "tags": [
        "AI"
      ],
      "featured": false,
      "showOnTopPage": false
    },
    {
      "id": "miy7bf22vb3p7etxvi",
      "title": "Áô∫Ë¶ã: joannany/llm-behavior-eval-lab - A comprehensive framework for evaluating and monitoring LLM behavior: safety ali",
      "summary": "Êñ∞„Åó„ÅÑ„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÇíÁô∫Ë¶ã: A comprehensive framework for evaluating and monitoring LLM behavior: safety alignment, capability assessment, drift detection, and bias evaluation. (‚≠ê0 | üç¥0)",
      "content": "A comprehensive framework for evaluating and monitoring LLM behavior: safety alignment, capability assessment, drift detection, and bias evaluation.\n\nË®ÄË™û: Python\n„Çπ„Çø„ÉºÊï∞: 0\n„Éï„Ç©„Éº„ÇØÊï∞: 0",
      "publishedAt": "2025-12-09T06:30:21.000Z",
      "source": "GitHub Search",
      "sourceUrl": "https://github.com/joannany/llm-behavior-eval-lab",
      "category": "community",
      "company": "Community",
      "imageUrl": null,
      "tags": [
        "LLM"
      ],
      "featured": false,
      "showOnTopPage": false
    },
    {
      "id": "miy7bh8n5mfpbhhqi5",
      "title": "Áô∫Ë¶ã: gravixlayer/llm-wars - GitHubÊñ∞„Éó„É≠„Ç∏„Çß„ÇØ„Éà",
      "summary": "Êñ∞„Åó„ÅÑ„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÇíÁô∫Ë¶ã: GitHub‰∏ä„ÅÆÊ≥®ÁõÆ„Éó„É≠„Ç∏„Çß„ÇØ„Éà (‚≠ê0 | üç¥0)",
      "content": "\n\nË®ÄË™û: TypeScript\n„Çπ„Çø„ÉºÊï∞: 0\n„Éï„Ç©„Éº„ÇØÊï∞: 0",
      "publishedAt": "2025-12-09T06:30:20.000Z",
      "source": "GitHub Search",
      "sourceUrl": "https://github.com/gravixlayer/llm-wars",
      "category": "community",
      "company": "Community",
      "imageUrl": null,
      "tags": [
        "LLM"
      ],
      "featured": false,
      "showOnTopPage": false
    },
    {
      "id": "miy7bf23rsdk2itcsr",
      "title": "Áô∫Ë¶ã: VladSF415/ai-platforms-directory - AI Platforms Directory - 693+ curated AI tools with search, filtering, and monet",
      "summary": "Êñ∞„Åó„ÅÑ„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÇíÁô∫Ë¶ã: AI Platforms Directory - 693+ curated AI tools with search, filtering, and monetization (‚≠ê0 | üç¥0)",
      "content": "AI Platforms Directory - 693+ curated AI tools with search, filtering, and monetization\n\nË®ÄË™û: TypeScript\n„Çπ„Çø„ÉºÊï∞: 0\n„Éï„Ç©„Éº„ÇØÊï∞: 0",
      "publishedAt": "2025-12-09T06:30:13.000Z",
      "source": "GitHub Search",
      "sourceUrl": "https://github.com/VladSF415/ai-platforms-directory",
      "category": "community",
      "company": "Community",
      "imageUrl": null,
      "tags": [
        "AI"
      ],
      "featured": false,
      "showOnTopPage": false
    },
    {
      "id": "miy7bf23ug4a6irb5hg",
      "title": "Áô∫Ë¶ã: Cometix-Org/Claude-Code-Agent-SDK-Typescript-Tracker - Tracking the update of source code of claude code agent sdk",
      "summary": "Êñ∞„Åó„ÅÑ„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÇíÁô∫Ë¶ã: Tracking the update of source code of claude code agent sdk (‚≠ê0 | üç¥0)",
      "content": "Tracking the update of source code of claude code agent sdk\n\nË®ÄË™û: JavaScript\n„Çπ„Çø„ÉºÊï∞: 0\n„Éï„Ç©„Éº„ÇØÊï∞: 0",
      "publishedAt": "2025-12-09T06:30:05.000Z",
      "source": "GitHub Search",
      "sourceUrl": "https://github.com/Cometix-Org/Claude-Code-Agent-SDK-Typescript-Tracker",
      "category": "community",
      "company": "Community",
      "imageUrl": null,
      "tags": [
        "Claude"
      ],
      "featured": false,
      "showOnTopPage": false
    },
    {
      "id": "miy7bh8nng6sc8aa3g",
      "title": "Áô∫Ë¶ã: Sanju0714/Real-Time-AI-Sales-Call-Assistant-For-Enhanced-Conversation-Strategies - This project delivers an intelligent assistant for live sales calls, leveraging ",
      "summary": "Êñ∞„Åó„ÅÑ„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÇíÁô∫Ë¶ã: This project delivers an intelligent assistant for live sales calls, leveraging advanced Large Language Models (LLMs) such as OpenAI GPT and Meta LLaMA, combined with cutting-edge NLP technologies. The system performs real-time speech analysis and sentiment detection to assist sales representatives during conversations. (‚≠ê0 | üç¥0)",
      "content": "This project delivers an intelligent assistant for live sales calls, leveraging advanced Large Language Models (LLMs) such as OpenAI GPT and Meta LLaMA, combined with cutting-edge NLP technologies. The system performs real-time speech analysis and sentiment detection to assist sales representatives during conversations.\n\nË®ÄË™û: Python\n„Çπ„Çø„ÉºÊï∞: 0\n„Éï„Ç©„Éº„ÇØÊï∞: 0",
      "publishedAt": "2025-12-09T06:30:04.000Z",
      "source": "GitHub Search",
      "sourceUrl": "https://github.com/Sanju0714/Real-Time-AI-Sales-Call-Assistant-For-Enhanced-Conversation-Strategies",
      "category": "community",
      "company": "Community",
      "imageUrl": null,
      "tags": [
        "GPT",
        "Llama",
        "LLM",
        "AI",
        "OpenAI"
      ],
      "featured": false,
      "showOnTopPage": false
    },
    {
      "id": "miy7bh8okb9jsf3s5e",
      "title": "Áô∫Ë¶ã: ShivaKumarAnandam/sarvam-voice-agent - AI-powered multilingual voice agent for phone calls using Sarvam AI (STT/LLM/TTS",
      "summary": "Êñ∞„Åó„ÅÑ„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÇíÁô∫Ë¶ã: AI-powered multilingual voice agent for phone calls using Sarvam AI (STT/LLM/TTS) and Twilio. Supports Telugu, Hindi, and English with real-time conversation capabilities. (‚≠ê0 | üç¥0)",
      "content": "AI-powered multilingual voice agent for phone calls using Sarvam AI (STT/LLM/TTS) and Twilio. Supports Telugu, Hindi, and English with real-time conversation capabilities.\n\nË®ÄË™û: Python\n„Çπ„Çø„ÉºÊï∞: 0\n„Éï„Ç©„Éº„ÇØÊï∞: 0",
      "publishedAt": "2025-12-09T06:30:02.000Z",
      "source": "GitHub Search",
      "sourceUrl": "https://github.com/ShivaKumarAnandam/sarvam-voice-agent",
      "category": "community",
      "company": "Community",
      "imageUrl": null,
      "tags": [
        "LLM",
        "AI"
      ],
      "featured": false,
      "showOnTopPage": false
    },
    {
      "id": "miy7bf23ozwcxxv3bce",
      "title": "Áô∫Ë¶ã: xiaolonggee/claude-speed-test-extension - test anyrouter proxy ",
      "summary": "Êñ∞„Åó„ÅÑ„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÇíÁô∫Ë¶ã: test anyrouter proxy  (‚≠ê0 | üç¥0)",
      "content": "test anyrouter proxy \n\nË®ÄË™û: TypeScript\n„Çπ„Çø„ÉºÊï∞: 0\n„Éï„Ç©„Éº„ÇØÊï∞: 0",
      "publishedAt": "2025-12-09T06:30:01.000Z",
      "source": "GitHub Search",
      "sourceUrl": "https://github.com/xiaolonggee/claude-speed-test-extension",
      "category": "community",
      "company": "Community",
      "imageUrl": null,
      "tags": [
        "Claude"
      ],
      "featured": false,
      "showOnTopPage": false
    },
    {
      "id": "miy7bg620s4t4pgk4u6",
      "title": "Áô∫Ë¶ã: mzrodyu/CatieCli - ÂºÄÊ∫êÔºåÁ¶ÅÂïÜÁî®",
      "summary": "Êñ∞„Åó„ÅÑ„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÇíÁô∫Ë¶ã: ÂºÄÊ∫êÔºåÁ¶ÅÂïÜÁî® (‚≠ê2 | üç¥2)",
      "content": "ÂºÄÊ∫êÔºåÁ¶ÅÂïÜÁî®\n\nË®ÄË™û: Python\n„Çπ„Çø„ÉºÊï∞: 2\n„Éï„Ç©„Éº„ÇØÊï∞: 2",
      "publishedAt": "2025-12-09T06:29:51.000Z",
      "source": "GitHub Search",
      "sourceUrl": "https://github.com/mzrodyu/CatieCli",
      "category": "community",
      "company": "Community",
      "imageUrl": null,
      "tags": [],
      "featured": false,
      "showOnTopPage": false
    },
    {
      "id": "miy7bf23xlli0b5m1m",
      "title": "Áô∫Ë¶ã: ZhiHui6/zhihui_nodes_comfyui - The ComfyUI node package features over 30 highly efficient functions, with core ",
      "summary": "Êñ∞„Åó„ÅÑ„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÇíÁô∫Ë¶ã: The ComfyUI node package features over 30 highly efficient functions, with core nodes including: [Qwen3-VL Advanced], [Tag Selector], and [Wan Video Prompt Generator]. Êã•Êúâ30Â§öÈ°πÈ´òÊïàÂäüËÉΩÁöÑComfyUIËäÇÁÇπÂåÖÔºåÊ†∏ÂøÉËäÇÁÇπÔºö[Qwen3-VLÈ´òÁ∫ßÁâà]„ÄÅ[Ê†áÁ≠æÈÄâÊã©Âô®]„ÄÅ[Wan‰∏áÁõ∏ËßÜÈ¢ëÊèêÁ§∫ËØçÁîüÊàêÂô®]„ÄÇ  (‚≠ê106 | üç¥2)",
      "content": "The ComfyUI node package features over 30 highly efficient functions, with core nodes including: [Qwen3-VL Advanced], [Tag Selector], and [Wan Video Prompt Generator]. Êã•Êúâ30Â§öÈ°πÈ´òÊïàÂäüËÉΩÁöÑComfyUIËäÇÁÇπÂåÖÔºåÊ†∏ÂøÉËäÇÁÇπÔºö[Qwen3-VLÈ´òÁ∫ßÁâà]„ÄÅ[Ê†áÁ≠æÈÄâÊã©Âô®]„ÄÅ[Wan‰∏áÁõ∏ËßÜÈ¢ëÊèêÁ§∫ËØçÁîüÊàêÂô®]„ÄÇ \n\nË®ÄË™û: Python\n„Çπ„Çø„ÉºÊï∞: 106\n„Éï„Ç©„Éº„ÇØÊï∞: 2",
      "publishedAt": "2025-12-09T06:29:48.000Z",
      "source": "GitHub Search",
      "sourceUrl": "https://github.com/ZhiHui6/zhihui_nodes_comfyui",
      "category": "community",
      "company": "Community",
      "imageUrl": null,
      "tags": [],
      "featured": false,
      "showOnTopPage": false
    },
    {
      "id": "miy7bg62fb9g8yvi1bc",
      "title": "Áô∫Ë¶ã: Piston-Labs/agent-coord-mcp - GitHubÊñ∞„Éó„É≠„Ç∏„Çß„ÇØ„Éà",
      "summary": "Êñ∞„Åó„ÅÑ„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÇíÁô∫Ë¶ã: GitHub‰∏ä„ÅÆÊ≥®ÁõÆ„Éó„É≠„Ç∏„Çß„ÇØ„Éà (‚≠ê0 | üç¥0)",
      "content": "\n\nË®ÄË™û: TypeScript\n„Çπ„Çø„ÉºÊï∞: 0\n„Éï„Ç©„Éº„ÇØÊï∞: 0",
      "publishedAt": "2025-12-09T06:29:45.000Z",
      "source": "GitHub Search",
      "sourceUrl": "https://github.com/Piston-Labs/agent-coord-mcp",
      "category": "community",
      "company": "Community",
      "imageUrl": null,
      "tags": [],
      "featured": false,
      "showOnTopPage": false
    }
  ],
  "githubArticles": [
    {
      "id": "miy7bh8nbm0no1qcod4",
      "title": "Áô∫Ë¶ã: judyhjy86/LLM_catalyst - ob639 project",
      "summary": "Êñ∞„Åó„ÅÑ„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÇíÁô∫Ë¶ã: ob639 project (‚≠ê0 | üç¥0)",
      "content": "ob639 project\n\nË®ÄË™û: Python\n„Çπ„Çø„ÉºÊï∞: 0\n„Éï„Ç©„Éº„ÇØÊï∞: 0",
      "publishedAt": "2025-12-09T06:30:26.000Z",
      "source": "GitHub Search",
      "sourceUrl": "https://github.com/judyhjy86/LLM_catalyst",
      "category": "community",
      "company": "Community",
      "imageUrl": null,
      "tags": [
        "LLM"
      ],
      "featured": false,
      "showOnTopPage": false
    },
    {
      "id": "miy7bg624prlg96zy03",
      "title": "Áô∫Ë¶ã: shin-ai-inc/website - ShinAI„ÅÆÂÖ¨ÂºèHP",
      "summary": "Êñ∞„Åó„ÅÑ„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÇíÁô∫Ë¶ã: ShinAI„ÅÆÂÖ¨ÂºèHP (‚≠ê0 | üç¥0)",
      "content": "ShinAI„ÅÆÂÖ¨ÂºèHP\n\nË®ÄË™û: HTML\n„Çπ„Çø„ÉºÊï∞: 0\n„Éï„Ç©„Éº„ÇØÊï∞: 0",
      "publishedAt": "2025-12-09T06:30:24.000Z",
      "source": "GitHub Search",
      "sourceUrl": "https://github.com/shin-ai-inc/website",
      "category": "community",
      "company": "Community",
      "imageUrl": null,
      "tags": [
        "AI"
      ],
      "featured": false,
      "showOnTopPage": false
    },
    {
      "id": "miy7bf22vb3p7etxvi",
      "title": "Áô∫Ë¶ã: joannany/llm-behavior-eval-lab - A comprehensive framework for evaluating and monitoring LLM behavior: safety ali",
      "summary": "Êñ∞„Åó„ÅÑ„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÇíÁô∫Ë¶ã: A comprehensive framework for evaluating and monitoring LLM behavior: safety alignment, capability assessment, drift detection, and bias evaluation. (‚≠ê0 | üç¥0)",
      "content": "A comprehensive framework for evaluating and monitoring LLM behavior: safety alignment, capability assessment, drift detection, and bias evaluation.\n\nË®ÄË™û: Python\n„Çπ„Çø„ÉºÊï∞: 0\n„Éï„Ç©„Éº„ÇØÊï∞: 0",
      "publishedAt": "2025-12-09T06:30:21.000Z",
      "source": "GitHub Search",
      "sourceUrl": "https://github.com/joannany/llm-behavior-eval-lab",
      "category": "community",
      "company": "Community",
      "imageUrl": null,
      "tags": [
        "LLM"
      ],
      "featured": false,
      "showOnTopPage": false
    },
    {
      "id": "miy7bh8n5mfpbhhqi5",
      "title": "Áô∫Ë¶ã: gravixlayer/llm-wars - GitHubÊñ∞„Éó„É≠„Ç∏„Çß„ÇØ„Éà",
      "summary": "Êñ∞„Åó„ÅÑ„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÇíÁô∫Ë¶ã: GitHub‰∏ä„ÅÆÊ≥®ÁõÆ„Éó„É≠„Ç∏„Çß„ÇØ„Éà (‚≠ê0 | üç¥0)",
      "content": "\n\nË®ÄË™û: TypeScript\n„Çπ„Çø„ÉºÊï∞: 0\n„Éï„Ç©„Éº„ÇØÊï∞: 0",
      "publishedAt": "2025-12-09T06:30:20.000Z",
      "source": "GitHub Search",
      "sourceUrl": "https://github.com/gravixlayer/llm-wars",
      "category": "community",
      "company": "Community",
      "imageUrl": null,
      "tags": [
        "LLM"
      ],
      "featured": false,
      "showOnTopPage": false
    },
    {
      "id": "miy7bf23rsdk2itcsr",
      "title": "Áô∫Ë¶ã: VladSF415/ai-platforms-directory - AI Platforms Directory - 693+ curated AI tools with search, filtering, and monet",
      "summary": "Êñ∞„Åó„ÅÑ„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÇíÁô∫Ë¶ã: AI Platforms Directory - 693+ curated AI tools with search, filtering, and monetization (‚≠ê0 | üç¥0)",
      "content": "AI Platforms Directory - 693+ curated AI tools with search, filtering, and monetization\n\nË®ÄË™û: TypeScript\n„Çπ„Çø„ÉºÊï∞: 0\n„Éï„Ç©„Éº„ÇØÊï∞: 0",
      "publishedAt": "2025-12-09T06:30:13.000Z",
      "source": "GitHub Search",
      "sourceUrl": "https://github.com/VladSF415/ai-platforms-directory",
      "category": "community",
      "company": "Community",
      "imageUrl": null,
      "tags": [
        "AI"
      ],
      "featured": false,
      "showOnTopPage": false
    },
    {
      "id": "miy7bf23ug4a6irb5hg",
      "title": "Áô∫Ë¶ã: Cometix-Org/Claude-Code-Agent-SDK-Typescript-Tracker - Tracking the update of source code of claude code agent sdk",
      "summary": "Êñ∞„Åó„ÅÑ„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÇíÁô∫Ë¶ã: Tracking the update of source code of claude code agent sdk (‚≠ê0 | üç¥0)",
      "content": "Tracking the update of source code of claude code agent sdk\n\nË®ÄË™û: JavaScript\n„Çπ„Çø„ÉºÊï∞: 0\n„Éï„Ç©„Éº„ÇØÊï∞: 0",
      "publishedAt": "2025-12-09T06:30:05.000Z",
      "source": "GitHub Search",
      "sourceUrl": "https://github.com/Cometix-Org/Claude-Code-Agent-SDK-Typescript-Tracker",
      "category": "community",
      "company": "Community",
      "imageUrl": null,
      "tags": [
        "Claude"
      ],
      "featured": false,
      "showOnTopPage": false
    },
    {
      "id": "miy7bh8nng6sc8aa3g",
      "title": "Áô∫Ë¶ã: Sanju0714/Real-Time-AI-Sales-Call-Assistant-For-Enhanced-Conversation-Strategies - This project delivers an intelligent assistant for live sales calls, leveraging ",
      "summary": "Êñ∞„Åó„ÅÑ„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÇíÁô∫Ë¶ã: This project delivers an intelligent assistant for live sales calls, leveraging advanced Large Language Models (LLMs) such as OpenAI GPT and Meta LLaMA, combined with cutting-edge NLP technologies. The system performs real-time speech analysis and sentiment detection to assist sales representatives during conversations. (‚≠ê0 | üç¥0)",
      "content": "This project delivers an intelligent assistant for live sales calls, leveraging advanced Large Language Models (LLMs) such as OpenAI GPT and Meta LLaMA, combined with cutting-edge NLP technologies. The system performs real-time speech analysis and sentiment detection to assist sales representatives during conversations.\n\nË®ÄË™û: Python\n„Çπ„Çø„ÉºÊï∞: 0\n„Éï„Ç©„Éº„ÇØÊï∞: 0",
      "publishedAt": "2025-12-09T06:30:04.000Z",
      "source": "GitHub Search",
      "sourceUrl": "https://github.com/Sanju0714/Real-Time-AI-Sales-Call-Assistant-For-Enhanced-Conversation-Strategies",
      "category": "community",
      "company": "Community",
      "imageUrl": null,
      "tags": [
        "GPT",
        "Llama",
        "LLM",
        "AI",
        "OpenAI"
      ],
      "featured": false,
      "showOnTopPage": false
    },
    {
      "id": "miy7bh8okb9jsf3s5e",
      "title": "Áô∫Ë¶ã: ShivaKumarAnandam/sarvam-voice-agent - AI-powered multilingual voice agent for phone calls using Sarvam AI (STT/LLM/TTS",
      "summary": "Êñ∞„Åó„ÅÑ„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÇíÁô∫Ë¶ã: AI-powered multilingual voice agent for phone calls using Sarvam AI (STT/LLM/TTS) and Twilio. Supports Telugu, Hindi, and English with real-time conversation capabilities. (‚≠ê0 | üç¥0)",
      "content": "AI-powered multilingual voice agent for phone calls using Sarvam AI (STT/LLM/TTS) and Twilio. Supports Telugu, Hindi, and English with real-time conversation capabilities.\n\nË®ÄË™û: Python\n„Çπ„Çø„ÉºÊï∞: 0\n„Éï„Ç©„Éº„ÇØÊï∞: 0",
      "publishedAt": "2025-12-09T06:30:02.000Z",
      "source": "GitHub Search",
      "sourceUrl": "https://github.com/ShivaKumarAnandam/sarvam-voice-agent",
      "category": "community",
      "company": "Community",
      "imageUrl": null,
      "tags": [
        "LLM",
        "AI"
      ],
      "featured": false,
      "showOnTopPage": false
    },
    {
      "id": "miy7bf23ozwcxxv3bce",
      "title": "Áô∫Ë¶ã: xiaolonggee/claude-speed-test-extension - test anyrouter proxy ",
      "summary": "Êñ∞„Åó„ÅÑ„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÇíÁô∫Ë¶ã: test anyrouter proxy  (‚≠ê0 | üç¥0)",
      "content": "test anyrouter proxy \n\nË®ÄË™û: TypeScript\n„Çπ„Çø„ÉºÊï∞: 0\n„Éï„Ç©„Éº„ÇØÊï∞: 0",
      "publishedAt": "2025-12-09T06:30:01.000Z",
      "source": "GitHub Search",
      "sourceUrl": "https://github.com/xiaolonggee/claude-speed-test-extension",
      "category": "community",
      "company": "Community",
      "imageUrl": null,
      "tags": [
        "Claude"
      ],
      "featured": false,
      "showOnTopPage": false
    },
    {
      "id": "miy7bg620s4t4pgk4u6",
      "title": "Áô∫Ë¶ã: mzrodyu/CatieCli - ÂºÄÊ∫êÔºåÁ¶ÅÂïÜÁî®",
      "summary": "Êñ∞„Åó„ÅÑ„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÇíÁô∫Ë¶ã: ÂºÄÊ∫êÔºåÁ¶ÅÂïÜÁî® (‚≠ê2 | üç¥2)",
      "content": "ÂºÄÊ∫êÔºåÁ¶ÅÂïÜÁî®\n\nË®ÄË™û: Python\n„Çπ„Çø„ÉºÊï∞: 2\n„Éï„Ç©„Éº„ÇØÊï∞: 2",
      "publishedAt": "2025-12-09T06:29:51.000Z",
      "source": "GitHub Search",
      "sourceUrl": "https://github.com/mzrodyu/CatieCli",
      "category": "community",
      "company": "Community",
      "imageUrl": null,
      "tags": [],
      "featured": false,
      "showOnTopPage": false
    },
    {
      "id": "miy7bf23xlli0b5m1m",
      "title": "Áô∫Ë¶ã: ZhiHui6/zhihui_nodes_comfyui - The ComfyUI node package features over 30 highly efficient functions, with core ",
      "summary": "Êñ∞„Åó„ÅÑ„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÇíÁô∫Ë¶ã: The ComfyUI node package features over 30 highly efficient functions, with core nodes including: [Qwen3-VL Advanced], [Tag Selector], and [Wan Video Prompt Generator]. Êã•Êúâ30Â§öÈ°πÈ´òÊïàÂäüËÉΩÁöÑComfyUIËäÇÁÇπÂåÖÔºåÊ†∏ÂøÉËäÇÁÇπÔºö[Qwen3-VLÈ´òÁ∫ßÁâà]„ÄÅ[Ê†áÁ≠æÈÄâÊã©Âô®]„ÄÅ[Wan‰∏áÁõ∏ËßÜÈ¢ëÊèêÁ§∫ËØçÁîüÊàêÂô®]„ÄÇ  (‚≠ê106 | üç¥2)",
      "content": "The ComfyUI node package features over 30 highly efficient functions, with core nodes including: [Qwen3-VL Advanced], [Tag Selector], and [Wan Video Prompt Generator]. Êã•Êúâ30Â§öÈ°πÈ´òÊïàÂäüËÉΩÁöÑComfyUIËäÇÁÇπÂåÖÔºåÊ†∏ÂøÉËäÇÁÇπÔºö[Qwen3-VLÈ´òÁ∫ßÁâà]„ÄÅ[Ê†áÁ≠æÈÄâÊã©Âô®]„ÄÅ[Wan‰∏áÁõ∏ËßÜÈ¢ëÊèêÁ§∫ËØçÁîüÊàêÂô®]„ÄÇ \n\nË®ÄË™û: Python\n„Çπ„Çø„ÉºÊï∞: 106\n„Éï„Ç©„Éº„ÇØÊï∞: 2",
      "publishedAt": "2025-12-09T06:29:48.000Z",
      "source": "GitHub Search",
      "sourceUrl": "https://github.com/ZhiHui6/zhihui_nodes_comfyui",
      "category": "community",
      "company": "Community",
      "imageUrl": null,
      "tags": [],
      "featured": false,
      "showOnTopPage": false
    },
    {
      "id": "miy7bg62fb9g8yvi1bc",
      "title": "Áô∫Ë¶ã: Piston-Labs/agent-coord-mcp - GitHubÊñ∞„Éó„É≠„Ç∏„Çß„ÇØ„Éà",
      "summary": "Êñ∞„Åó„ÅÑ„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÇíÁô∫Ë¶ã: GitHub‰∏ä„ÅÆÊ≥®ÁõÆ„Éó„É≠„Ç∏„Çß„ÇØ„Éà (‚≠ê0 | üç¥0)",
      "content": "\n\nË®ÄË™û: TypeScript\n„Çπ„Çø„ÉºÊï∞: 0\n„Éï„Ç©„Éº„ÇØÊï∞: 0",
      "publishedAt": "2025-12-09T06:29:45.000Z",
      "source": "GitHub Search",
      "sourceUrl": "https://github.com/Piston-Labs/agent-coord-mcp",
      "category": "community",
      "company": "Community",
      "imageUrl": null,
      "tags": [],
      "featured": false,
      "showOnTopPage": false
    },
    {
      "id": "miy7bicbjvdku0vrae",
      "title": "Anthropic: claude-quickstarts„ÅÆÊúÄÊñ∞„Ç¢„ÉÉ„Éó„Éá„Éº„Éà",
      "summary": "A collection of projects designed to help developers quickly get started with building deployable applications using the Claude API„ÅåÊõ¥Êñ∞„Åï„Çå„Åæ„Åó„Åü (‚≠ê11584)",
      "content": "A collection of projects designed to help developers quickly get started with building deployable applications using the Claude API\n\nÊúÄÁµÇÊõ¥Êñ∞: 12/9/2025\n„Çπ„Çø„ÉºÊï∞: 11584",
      "publishedAt": "2025-12-09T06:28:24.000Z",
      "source": "Anthropic GitHub Org",
      "sourceUrl": "https://github.com/anthropics/claude-quickstarts",
      "category": "tools",
      "company": "Anthropic",
      "imageUrl": null,
      "tags": [
        "Claude"
      ],
      "featured": true
    },
    {
      "id": "miy7bico3soytajh5f6",
      "title": "Anthropic: claude-cookbooks„ÅÆÊúÄÊñ∞„Ç¢„ÉÉ„Éó„Éá„Éº„Éà",
      "summary": "A collection of notebooks/recipes showcasing some fun and effective ways of using Claude.„ÅåÊõ¥Êñ∞„Åï„Çå„Åæ„Åó„Åü (‚≠ê29235)",
      "content": "A collection of notebooks/recipes showcasing some fun and effective ways of using Claude.\n\nÊúÄÁµÇÊõ¥Êñ∞: 12/9/2025\n„Çπ„Çø„ÉºÊï∞: 29235",
      "publishedAt": "2025-12-09T06:26:43.000Z",
      "source": "Anthropic GitHub Org",
      "sourceUrl": "https://github.com/anthropics/claude-cookbooks",
      "category": "tools",
      "company": "Anthropic",
      "imageUrl": null,
      "tags": [
        "Claude"
      ],
      "featured": true
    },
    {
      "id": "miy7bico7ts37nn2f9e",
      "title": "Anthropic: prompt-eng-interactive-tutorial„ÅÆÊúÄÊñ∞„Ç¢„ÉÉ„Éó„Éá„Éº„Éà",
      "summary": "Anthropic's Interactive Prompt Engineering Tutorial„ÅåÊõ¥Êñ∞„Åï„Çå„Åæ„Åó„Åü (‚≠ê27151)",
      "content": "Anthropic's Interactive Prompt Engineering Tutorial\n\nÊúÄÁµÇÊõ¥Êñ∞: 12/9/2025\n„Çπ„Çø„ÉºÊï∞: 27151",
      "publishedAt": "2025-12-09T06:25:04.000Z",
      "source": "Anthropic GitHub Org",
      "sourceUrl": "https://github.com/anthropics/prompt-eng-interactive-tutorial",
      "category": "tools",
      "company": "Anthropic",
      "imageUrl": null,
      "tags": [
        "Anthropic"
      ],
      "featured": true
    },
    {
      "id": "miy7bicowgyzoe37fn",
      "title": "Anthropic: claude-code-security-review„ÅÆÊúÄÊñ∞„Ç¢„ÉÉ„Éó„Éá„Éº„Éà",
      "summary": "An AI-powered security review GitHub Action using Claude to analyze code changes for security vulnerabilities.„ÅåÊõ¥Êñ∞„Åï„Çå„Åæ„Åó„Åü (‚≠ê2687)",
      "content": "An AI-powered security review GitHub Action using Claude to analyze code changes for security vulnerabilities.\n\nÊúÄÁµÇÊõ¥Êñ∞: 12/9/2025\n„Çπ„Çø„ÉºÊï∞: 2687",
      "publishedAt": "2025-12-09T06:20:01.000Z",
      "source": "Anthropic GitHub Org",
      "sourceUrl": "https://github.com/anthropics/claude-code-security-review",
      "category": "tools",
      "company": "Anthropic",
      "imageUrl": null,
      "tags": [
        "Claude",
        "AI"
      ],
      "featured": true
    },
    {
      "id": "miy7bicood7yyffcwxe",
      "title": "Anthropic: skills„ÅÆÊúÄÊñ∞„Ç¢„ÉÉ„Éó„Éá„Éº„Éà",
      "summary": "Public repository for Skills„ÅåÊõ¥Êñ∞„Åï„Çå„Åæ„Åó„Åü (‚≠ê19936)",
      "content": "Public repository for Skills\n\nÊúÄÁµÇÊõ¥Êñ∞: 12/9/2025\n„Çπ„Çø„ÉºÊï∞: 19936",
      "publishedAt": "2025-12-09T06:19:24.000Z",
      "source": "Anthropic GitHub Org",
      "sourceUrl": "https://github.com/anthropics/skills",
      "category": "tools",
      "company": "Anthropic",
      "imageUrl": null,
      "tags": [],
      "featured": true
    },
    {
      "id": "miy7b6qq9qvg9ymjgfl",
      "title": "Anthropic: claude-code„ÅÆÊúÄÊñ∞„Ç¢„ÉÉ„Éó„Éá„Éº„Éà",
      "summary": "claude-code„É™„Éù„Ç∏„Éà„É™„Å´Êñ∞„Åó„ÅÑÊõ¥Êñ∞: chore: Update CHANGELOG.md...",
      "content": "chore: Update CHANGELOG.md",
      "publishedAt": "2025-12-09T02:08:28.000Z",
      "source": "Anthropic GitHub",
      "sourceUrl": "https://github.com/anthropics/claude-code",
      "category": "tools",
      "company": "Anthropic",
      "imageUrl": null,
      "tags": [
        "Claude"
      ],
      "featured": false
    },
    {
      "id": "miy7bbw4h48350t8d16",
      "title": "Google: Release v0.20.0-preview.5",
      "summary": "## What's Changed\n* fix(patch): cherry-pick 171103a to release/v0.20.0-preview.2-pr-14742 to patch version v0.20.0-preview.2 and create version 0.20.0-preview.5 by @gemini-cli-robot in https://github.com/google-gemini/gemini-cli/pull/14752\n\n\n**Full Changelog**: https://github.com/google-gemini/gemin...",
      "content": "## What's Changed\n* fix(patch): cherry-pick 171103a to release/v0.20.0-preview.2-pr-14742 to patch version v0.20.0-preview.2 and create version 0.20.0-preview.5 by @gemini-cli-robot in https://github.com/google-gemini/gemini-cli/pull/14752\n\n\n**Full Changelog**: https://github.com/google-gemini/gemini-cli/compare/v0.20.0-preview.2...v0.20.0-preview.5",
      "publishedAt": "2025-12-09T01:58:16.000Z",
      "source": "Google GitHub",
      "sourceUrl": "https://github.com/google-gemini/gemini-cli/releases/tag/v0.20.0-preview.5",
      "category": "tools",
      "company": "Google",
      "imageUrl": null,
      "tags": [
        "Gemini",
        "Google"
      ],
      "featured": false
    },
    {
      "id": "miy7bbw4r5v3efc5l6o",
      "title": "Google: Release v0.21.0-nightly.20251209.ec9a8c7a7",
      "summary": "## What's Changed\n* chore/release: bump version to 0.21.0-nightly.20251207.025e450ac by @gemini-cli-robot in https://github.com/google-gemini/gemini-cli/pull/14662\n* feat(modelAvailabilityService): integrate model availability service into backend logic by @adamfweidman in https://github.com/google-...",
      "content": "## What's Changed\n* chore/release: bump version to 0.21.0-nightly.20251207.025e450ac by @gemini-cli-robot in https://github.com/google-gemini/gemini-cli/pull/14662\n* feat(modelAvailabilityService): integrate model availability service into backend logic by @adamfweidman in https://github.com/google-gemini/gemini-cli/pull/14470\n* Add prompt_id propagation in a2a-server task by @koxkox111 in https://github.com/google-gemini/gemini-cli/pull/14581\n* Fix: Prevent freezing in non-interactive Gemini CLI when debug mode is enabled by @parthasaradhie in https://github.com/google-gemini/gemini-cli/pull/14580\n* fix(audio): improve reading of audio files by @jackwotherspoon in https://github.com/google-gemini/gemini-cli/pull/14658\n* Update automated triage workflow to stop assigning priority labels by @skeshive in https://github.com/google-gemini/gemini-cli/pull/14717\n* set failed status when chained e2e fails by @scidomino in https://github.com/google-gemini/gemini-cli/pull/14725\n* feat(github action) Triage and Label Pull Requests by Size and Comple‚Ä¶ by @DaanVersavel in https://github.com/google-gemini/gemini-cli/pull/5571\n* refactor(telemetry): Improve previous PR that allows telemetry to use the CLI auth and add testing by @mboshernitsan in https://github.com/google-gemini/gemini-cli/pull/14589\n* Always set status in chained_e2e workflow by @scidomino in https://github.com/google-gemini/gemini-cli/pull/14730\n* feat: Add OTEL log event `gemini_cli.startup_stats` for startup stats. by @kevin-ramdass in https://github.com/google-gemini/gemini-cli/pull/14734\n* feat: auto-execute on slash command completion functions by @jackwotherspoon in https://github.com/google-gemini/gemini-cli/pull/14584\n* Docs: Proper release notes by @jkcinouye in https://github.com/google-gemini/gemini-cli/pull/14405\n* Add support for user-scoped extension settings by @chrstnb in https://github.com/google-gemini/gemini-cli/pull/13748\n\n## New Contributors\n* @parthasaradhie made their first contribution in https://github.com/google-gemini/gemini-cli/pull/14580\n\n**Full Changelog**: https://github.com/google-gemini/gemini-cli/compare/v0.21.0-nightly.20251207.025e450ac...v0.21.0-nightly.20251209.ec9a8c7a7",
      "publishedAt": "2025-12-09T00:47:21.000Z",
      "source": "Google GitHub",
      "sourceUrl": "https://github.com/google-gemini/gemini-cli/releases/tag/v0.21.0-nightly.20251209.ec9a8c7a7",
      "category": "tools",
      "company": "Google",
      "imageUrl": null,
      "tags": [
        "Gemini",
        "AI",
        "Google"
      ],
      "featured": false
    },
    {
      "id": "miy7b9tsevrsh6aoibs",
      "title": "OpenAI: openai-cookbook„ÅÆÊúÄÊñ∞„Ç¢„ÉÉ„Éó„Éá„Éº„Éà",
      "summary": "openai-cookbook„É™„Éù„Ç∏„Éà„É™„Å´Êñ∞„Åó„ÅÑÊõ¥Êñ∞: tweaks to -max cookbook (#2274)...",
      "content": "tweaks to -max cookbook (#2274)",
      "publishedAt": "2025-12-04T20:25:36.000Z",
      "source": "OpenAI GitHub",
      "sourceUrl": "https://github.com/openai/openai-cookbook",
      "category": "tools",
      "company": "OpenAI",
      "imageUrl": null,
      "tags": [
        "AI",
        "OpenAI"
      ],
      "featured": false
    },
    {
      "id": "miy7bdx797ayvop36t",
      "title": "Hugging Face: Transformers v5.0.0rc0",
      "summary": "## Transformers v5 release notes\r\n\r\n<img width=\"1800\" height=\"1013\" alt=\"image\" src=\"https://github.com/user-attachments/assets/7b5187d7-6945-4108-a546-6d1d7bfb55e3\" />\r\n\r\n- Highlights\r\n- Significant API changes: dynamic weight loading, tokenization\r\n- Backwards Incompatible Changes\r\n- Bugfixes and ...",
      "content": "## Transformers v5 release notes\r\n\r\n<img width=\"1800\" height=\"1013\" alt=\"image\" src=\"https://github.com/user-attachments/assets/7b5187d7-6945-4108-a546-6d1d7bfb55e3\" />\r\n\r\n- Highlights\r\n- Significant API changes: dynamic weight loading, tokenization\r\n- Backwards Incompatible Changes\r\n- Bugfixes and improvements\r\n\r\n## Highlights\r\n\r\nWe are excited to announce the initial release of Transformers v5. This is the first major release in five years, and the release is significant: 800 commits have been pushed to `main` since the latest minor release. This release removes a lot of long-due deprecations, introduces several refactors that significantly simplify our APIs and internals, and comes with a large number of bug fixes.\r\n\r\nWe give an overview of our focus for this release in the [following blogpost](https://huggingface.co/blog/transformers-v5). In these release notes, we'll focus directly on the refactors and new APIs coming with v5.\r\n\r\nThis release is a release candidate (RC). It is not the final v5 release, and we will push on pypi as a pre-release. This means that the current release is purely opt-in, as installing `transformers` without specifying this exact release will install the latest version instead (v4.57.3 as of writing).\r\n\r\nIn order to install this release, please do so with the following:\r\n\r\n```shell\r\npip install transformers --pre\r\n```\r\n\r\nFor us to deliver the best package possible, it is imperative that we have feedback on how the toolkit is currently working for you. Please try it out, and [open an issue](https://github.com/huggingface/transformers/issues/) in case you're facing something inconsistent/a bug.\r\n\r\nTransformers version 5 is a community endeavor, and this is the last mile. Let's ship this together!\r\n\r\n## Significant API changes\r\n\r\n> [!NOTE]\r\n> üëÄ Nothing is final and things are still actively in movement. We have a section dedicated to what is planned for future release candidates, yet is known not to work in the RC0. Look for \"Disclaimers for the RC0\".\r\n> \r\n> We'll be eagerly awaiting your feedback in our GitHub issues!\r\n\r\n### Tokenization\r\n\r\nJust as we moved towards a single backend library for model definition, we want our tokenizers, and the `Tokenizer` object to be a lot more intuitive. With v5, tokenizer definition is much simpler; one can now initialize an empty `LlamaTokenizer` and train it directly on your corpus.\r\n\r\nDefining a new tokenizer object should be as simple as this:\r\n\r\n```python\r\nfrom transformers import TokenizersBackend, generate_merges\r\nfrom tokenizers import pre_tokenizers, Tokenizer\r\nfrom tokenizers.model import BPE\r\n\r\nclass Llama5Tokenizer(TokenizersBackend):\r\n    def __init__(self, unk_token=\"<unk>\",bos_token=\"<s>\", eos_token=\"</s>\", vocab=None, merges=None ):\r\n        if vocab is None:\r\n            self._vocab = {\r\n                str(unk_token): 0,\r\n                str(bos_token): 1,\r\n                str(eos_token): 2,\r\n            }\r\n\r\n        else:\r\n            self._vocab = vocab\r\n\r\n        if merges is not None:\r\n            self._merges = merges\r\n        else:\r\n            self._merges = generate_merges(filtered_vocab)\r\n\r\n        self._tokenizer = Tokenizer(\r\n            BPE(vocab=self._vocab, merges=self._merges, fuse_unk=True)\r\n        )\r\n        self._tokenizer.pre_tokenizer = pre_tokenizers.Metaspace(\r\n            replacement=\"‚ñÅ\", prepend_scheme=_get_prepend_scheme(self.add_prefix_space, self), split=False\r\n        )\r\n        super().__init__(\r\n            tokenizer_object=self._tokenizer,\r\n            unk_token=unk_token,\r\n            bos_token=bos_token,\r\n            eos_token=eos_token,\r\n        )\r\n```\r\n\r\nOnce the tokenizer is defined as above, you can load it with the following: `Llama5Tokenizer()`. Doing this returns you an empty, trainable tokenizer that follows the definition of the authors of `Llama5` (it does not exist yet :wink:).\r\n\r\nThe above is the main motivation towards refactoring tokenization: we want tokenizers to behave similarly to models: trained or empty, and with exactly what is defined in their class definition.\r\n\r\n### Backend Architecture Changes: moving away from the slow/fast tokenizer separation\r\n\r\nUp to now, transformers maintained two parallel implementations for many tokenizers:\r\n- \"Slow\" tokenizers (`tokenization_<model>.py`) - Python-based implementations, often using [SentencePiece](https://github.com/google/sentencepiece) as the backend.\r\n- \"Fast\" tokenizers (`tokenization_<model>_fast.py`) - Rust-based implementations using the ü§ó [tokenizers](https://github.com/huggingface/tokenizers) library.\r\n\r\nIn v5, we consolidate to a single tokenizer file per model: `tokenization_<model>.py`. This file will use the most appropriate backend available:\r\n\r\n1. **TokenizersBackend** (preferred): Rust-based tokenizers from the ü§ó [tokenizers](https://github.com/huggingface/tokenizers) library. In general it provides optimal performance, but it also offers a lot more features that are commonly adopted across the ecosystem:\r\n  - handling additional tokens\r\n  - a full python API for setting and updating \r\n  - automatic parallelization,\r\n  - automatic offsets\r\n  - customization\r\n  - training\r\n2. **SentencePieceBackend**: for tokenizers requiring the `sentencepiece` library. It inherits from `PythonBackend`. \r\n3. **PythonBackend**: a Python implementations of the features provided by `tokenizers`. Basically allows adding tokens.\r\n4. **MistralCommonBackend**: relies on `MistralCommon`'s tokenization library. (Previously known as the `MistralCommonTokenizer`)\r\n\r\nThe `AutoTokenizer` automatically selects the appropriate backend based on available files and dependencies. This is transparent, you continue to use `AutoTokenizer.from_pretrained()` as before. This allows transformers to be future-proof and modular to easily support future backends.\r\n\r\n### Defining a tokenizers outside of the existing backends\r\n\r\nWe enable users and tokenizer builders to define their own tokenizers from top to bottom. Tokenizers are usually defined using a backend such as `tokenizers`, `sentencepiece` or `mistral-common`, but we offer the possibility to design the tokenizer at a higher-level, without relying on those backends.\r\n\r\nTo do so, you can import the `PythonBackend` (which was previously known as `PreTrainedTokenizer`). This class encapsulates all the logic related to added tokens, encoding, and decoding.\r\n\r\nIf you want something even higher up the stack, then `PreTrainedTokenizerBase` is what `PythonBackend` inherits from. It contains the very basic tokenizer API features: \r\n- `encode`\r\n- `decode`\r\n- `vocab_size`\r\n- `get_vocab`\r\n- `convert_tokens_to_ids`\r\n- `convert_ids_to_tokens`\r\n- `from_pretrained`\r\n- `save_pretrained`\r\n- among a few others\r\n\r\n### API Changes\r\n\r\n#### 1. Direct tokenizer initialization with vocab and merges\r\n\r\nStarting with v5, we now enable initializing blank, untrained `tokenizers`-backed tokenizers:\r\n\r\n```py\r\nfrom transformers import LlamaTokenizer\r\n\r\ntokenizer = LlamaTokenizer()\r\n```\r\n\r\nThis tokenizer will therefore follow the definition of the `LlamaTokenizer` as defined in its class definition. It can then be trained on a corpus as can be seen in [the `tokenizers` documentation](https://huggingface.co/docs/tokenizers/training_from_memory).\r\n\r\nThese tokenizers can also be initialized from vocab and merges (if necessary), like the previous \"slow\" tokenizers:\r\n\r\n```py\r\nfrom transformers import LlamaTokenizer\r\n\r\nvocab = {\"<unk>\": 0, \"<s>\": 1, \"</s>\": 2, \"hello\": 3, \"world\": 4}\r\nmerges = [(\"h\", \"e\"), (\"l\", \"l\"), (\"o\", \" \")]\r\n\r\ntokenizer = LlamaTokenizer(vocab=vocab, merges=merges)\r\n```\r\n\r\nThis tokenizer will behave as a Llama-like tokenizer, with an updated vocabulary. This allows comparing different tokenizer classes with the same vocab; therefore enabling the comparison of different pre-tokenizers, normalizers, etc.\r\n\r\n‚ö†Ô∏è The `vocab_file` (as in, a path towards a file containing the vocabulary) cannot be used to initialize the `LlamaTokenizer` as loading from files is reserved to the `from_pretrained` method.\r\n\r\n#### 2. Simplified decoding API\r\n\r\nThe `batch_decode` and `decode` methods have been unified to reflect behavior of the `encode` method. Both single and batch decoding now use the same `decode` method. See an example of the new behavior below:\r\n\r\n```python\r\nfrom transformers import AutoTokenizer\r\ntokenizer = AutoTokenizer.from_pretrained(\"t5-small\") \r\ninputs = [\"hey how are you?\", \"fine\"]\r\ntokenizer.decode(tokenizer.encode(inputs))\r\n```\r\n\r\nGives:\r\n```diff\r\n- 'hey how are you?</s> fine</s>'\r\n+ ['hey how are you?</s>', 'fine</s>']\r\n```\r\n\r\nWe expect `encode` and `decode` to behave, as two sides of the same coin: `encode`, `process`, `decode`,  should work. \r\n\r\n> [!NOTE]\r\n> A common use-case would be: `encode`, `model.generate`, `decode`.  However, using `generate` would return `list[list[int]]`, which would then be incompatible with `decode`.\r\n\r\n#### 3. Unified encoding API\r\n\r\nThe `encode_plus` method is deprecated in favor of the single `__call__` method.\r\n\r\n#### 4. `apply_chat_template` returns `BatchEncoding`\r\n\r\nPreviously, `apply_chat_template` returned `input_ids` for backward compatibility. Starting with v5, it now consistently returns a `BatchEncoding` dict like other tokenizer methods.\r\n\r\n```python\r\n# v5\r\nmessages = [\r\n    {\"role\": \"user\", \"content\": \"Hello!\"},\r\n    {\"role\": \"assistant\", \"content\": \"Hi there!\"}\r\n]\r\n\r\n# Now returns BatchEncoding with input_ids, attention_mask, etc.\r\noutputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\r\nprint(outputs.keys())  # dict_keys(['input_ids', 'attention_mask'])\r\n```\r\n\r\n#### 5. Removed legacy configuration file saving:\r\n\r\nWe simplify the serialization of tokenization attributes:\r\n\r\n- `special_tokens_map.json` - special tokens are now stored in `tokenizer_config.json`.\r\n- `added_tokens.json` - added tokens are now stored in `tokenizer.json`.\r\n- `added_tokens_decoder` is only stored when there is no `tokenizer.json`.\r\n\r\nWhen loading older tokenizers, these files are still read for backward compatibility, but new saves use the consolidated format. We're gradually moving towards consolidating attributes to fewer files so that other libraries and implementations may depend on them more reliably.\r\n\r\n#### 6. Model-Specific Changes\r\n\r\nSeveral models that had identical tokenizers now import from their base implementation:\r\n\r\n- **LayoutLM** ‚Üí uses BertTokenizer\r\n- **LED** ‚Üí uses BartTokenizer  \r\n- **Longformer** ‚Üí uses RobertaTokenizer\r\n- **LXMert** ‚Üí uses BertTokenizer\r\n- **MT5** ‚Üí uses T5Tokenizer\r\n- **MVP** ‚Üí uses BartTokenizer\r\n\r\nThese modules will eventually be removed altogether.\r\n\r\n**Removed T5-specific workarounds**\r\n\r\nThe internal `_eventually_correct_t5_max_length` method has been removed. T5 tokenizers now handle max length consistently with other models.\r\n\r\n### Testing Changes\r\n\r\nA few testing changes specific to tokenizers have been applied:\r\n- Model-specific tokenization test files now focus on integration tests.\r\n- Common tokenization API tests (e.g., `add_tokens`, `encode`, `decode`) are now centralized and automatically applied across all tokenizers. This reduces test duplication and ensures consistent behavior\r\n\r\nFor legacy implementations, the original BERT Python tokenizer code (including `WhitespaceTokenizer`, `BasicTokenizer`, etc.) is preserved in `bert_legacy.py` for reference purposes.\r\n\r\n#### 7. Deprecated / Modified Features\r\n\r\n**Special Tokens Structure:**\r\n- `SpecialTokensMixin`: Merged into `PreTrainedTokenizerBase` to simplify the tokenizer architecture.\r\n- `special_tokens_map`: Now only stores named special token attributes (e.g., `bos_token`, `eos_token`). Use `extra_special_tokens` for additional special tokens (formerly `additional_special_tokens`). `all_special_tokens` includes both named and extra tokens.\r\n\r\n```python\r\n# v4\r\ntokenizer.special_tokens_map  # Included 'additional_special_tokens'\r\n\r\n# v5\r\ntokenizer.special_tokens_map  # Only named tokens\r\ntokenizer.extra_special_tokens  # Additional tokens\r\n```\r\n\r\n- `special_tokens_map_extended` and `all_special_tokens_extended`: Removed. Access `AddedToken` objects directly from `_special_tokens_map` or `_extra_special_tokens` if needed.\r\n- `additional_special_tokens`: Still accepted for backward compatibility but is automatically converted to `extra_special_tokens`.\r\n\r\n**Deprecated Methods:**\r\n- `sanitize_special_tokens()`: Already deprecated in v4, removed in v5.\r\n- `prepare_seq2seq_batch()`: Deprecated; use `__call__()` with `text_target` parameter instead.\r\n\r\n```python\r\n# v4\r\nmodel_inputs = tokenizer.prepare_seq2seq_batch(src_texts, tgt_texts, max_length=128)\r\n\r\n# v5\r\nmodel_inputs = tokenizer(src_texts, text_target=tgt_texts, max_length=128, return_tensors=\"pt\")\r\nmodel_inputs[\"labels\"] = model_inputs.pop(\"input_ids_target\")\r\n```\r\n\r\n- `BatchEncoding.words()`: Deprecated; use `word_ids()` instead.\r\n\r\n**Removed Methods:**\r\n- `create_token_type_ids_from_sequences()`: Removed from base class. Subclasses that need custom token type ID creation should implement this method directly.\r\n- `clean_up_tokenization()`: Removed from base class. Now defined at model class level for models that need it (e.g., PLBart, CLVP, Wav2Vec2).\r\n- `prepare_for_model()`, `build_inputs_with_special_tokens()`, `truncate_sequences()`: Moved from `tokenization_utils_base.py` to `tokenization_python.py` for `PythonBackend` tokenizers. `TokenizersBackend` provides model-ready input via `tokenize()` and `encode()`, so these methods are no longer needed in the base class.\r\n- `_switch_to_input_mode()`, `_switch_to_target_mode()`, `as_target_tokenizer()`: Removed from base class. Use `__call__()` with `text_target` parameter instead.\r\n\r\n```python\r\n# v4\r\nwith tokenizer.as_target_tokenizer():\r\n    labels = tokenizer(tgt_texts, ...)\r\n\r\n# v5\r\nlabels = tokenizer(text_target=tgt_texts, ...)\r\n```\r\n\r\n- `parse_response()`: Removed from base class.\r\n\r\n## Disclaimers for the RC0\r\n\r\n### PEFT + MoE:\r\n\r\nBecause we are switching from the naive MOE (`nn.ModuleList` for experts) we currently have an issue with MoEs that have adapters. For more details see https://github.com/huggingface/transformers/issues/42491#issuecomment-3591485649. \r\n\r\n_We aim for this to be fixed and released in a following release candidate in the week that follows RC0._\r\n\r\n### Tensor parallel and Expert parallel + MoE\r\n\r\nWe are streamlining the MoE support with vLLM; while this is being implemented, tensor parallelism and expert parallelism aren't working as expected.\r\nThis is known and actively being worked on.\r\n\r\n_We aim for this to be fixed and released in a following release candidate in the week that follows RC0._\r\n\r\n### Custom pretrained models:\r\nFor anyone inheriting from a `transformers` `PreTrainedModel`, the weights are automatically initialized with the common scheme: \r\n```python\r\n\r\n    @torch.no_grad()\r\n    def _init_weights(self, module):\r\n        \"\"\"\r\n        Initialize the weights. This is quite general on purpose, in the spirit of what we usually do. For more complex\r\n        initialization scheme, it should be overridden by the derived `PreTrainedModel` class. In case a model adds an explicit\r\n        `nn.Parameter`, this method should also be overridden in order to initialize it correctly.\r\n        \"\"\"\r\n        if hasattr(self.config, \"initializer_range\"):\r\n            std = self.config.initializer_range or 0.02\r\n        elif hasattr(self.config, \"init_std\"):\r\n            std = self.config.init_std\r\n        elif hasattr(self.config, \"initializer_factor\"):\r\n            std = self.config.initializer_factor\r\n        else:\r\n            # 0.02 is the standard default value across the library\r\n            std = getattr(self.config.get_text_config(), \"initializer_range\", 0.02)\r\n\r\n        if isinstance(module, (nn.Linear, nn.Conv1d, nn.Conv2d, nn.Conv3d, nn.ConvTranspose1d, nn.ConvTranspose2d)):\r\n            if getattr(module, \"weight\", None) is not None:\r\n                init.normal_(module.weight, mean=0.0, std=std)\r\n            if getattr(module, \"bias\", None) is not None:\r\n                init.zeros_(module.bias)\r\n        elif isinstance(module, nn.Embedding):\r\n            if getattr(module, \"weight\", None) is not None:\r\n                init.normal_(module.weight, mean=0.0, std=std)\r\n                # Here we need the check explicitly, as we slice the weight in the `zeros_` call, so it looses the flag\r\n                if module.padding_idx is not None and not getattr(module.weight, \"_is_hf_initialized\", False):\r\n                    init.zeros_(module.weight[module.padding_idx])\r\n        elif isinstance(module, nn.MultiheadAttention):\r\n            # This uses torch's original init\r\n            module._reset_parameters()\r\n        # We cannot use `isinstance` on the RMSNorms or LayerNorms, as they usually are custom modules which change names\r\n        # between modelings (because they are prefixed with the model name)\r\n        elif (\r\n            isinstance(module, (nn.GroupNorm, nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d))\r\n            or \"LayerNorm\" in module.__class__.__name__\r\n            or \"RMSNorm\" in module.__class__.__name__\r\n        ):\r\n            # Norms can exist without weights (in which case they are None from torch primitives)\r\n            if hasattr(module, \"weight\") and module.weight is not None:\r\n                init.ones_(module.weight)\r\n            if hasattr(module, \"bias\") and module.bias is not None:\r\n                init.zeros_(module.bias)\r\n```\r\n\r\nIf you want to avoid that, for now you should just do:\r\n\r\n```python\r\nclass CustomModel(Qwen3VLForConditionalGeneration):\r\n    def __init__(self, *args, **kwargs):\r\n        super().__init__(*args, **kwargs)\r\n        self.action_head = nn.Linear(1024, 7)\r\n        self.positional_embedding = nn.Parameter(torch.randn(16, 1152))\r\n        self.post_init()\r\n    \r\n    def _init_weights(self, module):\r\n        pass \r\n\r\n```\r\nThere is a tracker for that here: https://github.com/huggingface/transformers/issues/42418.\r\n\r\n## Library-wide changes with lesser impact\r\n\r\n### `use_auth_token`\r\n\r\nThe `use_auth_token` argument/parameter is deprecated in favor of `token` everywhere.\r\nYou should be able to search and replace `use_auth_token` with `token` and get the same logic.\r\n\r\nLinked PR: https://github.com/huggingface/transformers/pull/41666\r\n\r\n### Attention-related features\r\n\r\nWe decided to remove some features for the upcoming v5 as they are currently only supported in a few old models and no longer integrated in current model additions. It's recommended to stick to v4.x in case you need them. Following features are affected:\r\n- No more head masking, see [#41076](https://github.com/huggingface/transformers/pull/41076). This feature allowed to turn off certain heads during the attention calculation and only worked for eager.\r\n- No more relative positional biases in Bert-like models, see [#41170](https://github.com/huggingface/transformers/pull/41170). This feature was introduced to allow relative position scores within attention calculations (similar to T5). However, this feature is barely used in official models and a lot of complexity instead. It also only worked with eager.\r\n- No more head pruning, see [#41417](https://github.com/huggingface/transformers/pull/41417) by @gante. As the name suggests, it allowed to prune heads within your attention layers.\r\n\r\n### Updates to supported torch APIs\r\n\r\nWe dropped support for two torch APIs:\r\n- `torchscript` in https://github.com/huggingface/transformers/pull/41688\r\n- `torch.fx` in https://github.com/huggingface/transformers/pull/41683\r\n\r\nThose APIs were deprecated by the PyTorch team, and we're instead focusing on the supported APIs `dynamo` and `export`.\r\n\r\n## Quantization changes\r\n\r\nWe clean up the quantization API in transformers, and significantly refactor the weight loading as highlighted\r\nabove.\r\n\r\nWe drop support for two quantization arguments that have been deprecated for some time:\r\n- `load_in_4bit`\r\n- `load_in_8bit`\r\n\r\nWe remove them in favor of the `quantization_config` argument which is much more complete. As an example, here is how\r\nyou would load a 4-bit bitsandbytes model using this argument:\r\n\r\n```python\r\nfrom transformers import AutoModelForCausalLM, BitsAndBytesConfig\r\n\r\nquantization_config = BitsAndBytesConfig(load_in_4bit=True)\r\n\r\nmodel_4bit = AutoModelForCausalLM.from_pretrained(\r\n    \"meta-llama/Llama-3.2-3B\",\r\n    device_map=\"auto\",\r\n    quantization_config=quantization_config\r\n)\r\n```\r\n\r\n\r\n## Configuration\r\n\r\n- Methods to init a nested config such as `from_xxx_config` are deleted. Configs can be init from the `__init__` method in the same way. See [#41314](https://github.com/huggingface/transformers/pull/41314).\r\n- It is no longer possible to load a config class from a URL file. Configs must be loaded from either a local path or a repo on the Hub. See [#42383](https://github.com/huggingface/transformers/pull/42383).\r\n- All parameters for configuring model's rotary embedding are now stored under `mode.rope_parameters`, including the `rope_theta` and `rope_type`. Model's `config.rope_parameters` is a simple dictionaty in most cases, and can also be a nested dict in special cases (i.e. Gemma3 and ModernBert) with different rope parameterization for each layer type. Trying to get `config.rope_theta` will throw an attribute error from now on. See [#39847](https://github.com/huggingface/transformers/pull/39847) and [#42255](https://github.com/huggingface/transformers/pull/42255)\r\n- Qwen-VL family configuration is in a nested format and trying to access keys directly will throw an error (e.g. `config.vocab_size`). Users are expected to access keys from their respective sub-configs (`config.text_config.vocab_size`).\r\n- Configurations of non-generative models (any model that doesn't call `model.generate()`) will no longer have a `generation_config` and `model.config.generation_config` will throw an attribute error.\r\n\r\n## Processing\r\n\r\n### Tokenization\r\n\r\n- Slow tokenizer files (aka: `tokenization_<model>.py` ) will be removed in favor of using fast tokenizer files `tokenization_<model>_fast.py` --> will be renamed to `tokenization_<model>.py`.  As fast tokenizers are :hugs:`tokenizers` - backend, they include a wider range of features that are maintainable and reliable. \r\n- Other backends (sentence piece, tokenizers, etc.) will be supported with a light layer if loading a fast tokenizer fails\r\n- Remove legacy files like special_tokens_map.json and added_tokens.json\r\n- Remove _eventually_correct_t5_max_length \r\n- `encode_plus` --> `__call__`\r\n- `batch_decode` --> `decode`\r\n\r\n`apply_chat_template` by default returns naked `input_ids` rather than a `BatchEncoding` dict. \r\nThis was inconvenient - it should return a `BatchEncoding` dict like `tokenizer.__call__()`, but we were stuck with \r\nit for backward compatibility. The method now returns a `BatchEncoding`.\r\n\r\nLinked PRs: \r\n- https://github.com/huggingface/transformers/issues/40938\r\n- https://github.com/huggingface/transformers/pull/40936\r\n- https://github.com/huggingface/transformers/pull/41626\r\n\r\n### Processing classes\r\n\r\n- In processing classes each attribute will be serialized under `processor_config.json` as a nested dict, instead of serializing attributes in their own config files. Loading will be supported for all old format processors (https://github.com/huggingface/transformers/pull/41474)\r\n- `XXXFeatureExtractors` classes are completely removed in favor of `XXXImageProcessor` class for all vision models (https://github.com/huggingface/transformers/pull/41174)\r\n- Minor change: `XXXFastImageProcessorKwargs` is removed in favor of `XXXImageProcessorKwargs` which will be shared between fast and slow processors (https://github.com/huggingface/transformers/pull/40931)\r\n\r\n\r\n## Modeling\r\n\r\n- Some `RotaryEmbeddings` layers will start returning a dict of tuples, in case the model uses several RoPE configurations (Gemma2, ModernBert). Each value will be a tuple of \"cos, sin\" per RoPE type.\r\n- Config attribute for `RotaryEmbeddings` layer will be unified and accessed via `config.rope_parameters`. Config attr for `rope_theta` might not be accessible anymore for some models, and instead will be in `config.rope_parameters['rope_theta']`. BC will be supported for a while as much as possible, and in the near future we'll gradually move to the new RoPE format  (https://github.com/huggingface/transformers/pull/39847)\r\n- Vision Language models will not have a shortcut access to its language and vision component from the generative model via `model.language_model`. It is recommended to either access the module with `model.model.language_model` or `model.get_decoder()`. See [#42156](https://github.com/huggingface/transformers/pull/42156/)\r\n\r\n### Generate\r\n\r\n- Old, deprecated output type aliases were removed (e.g. `GreedySearchEncoderDecoderOutput`). We now only have 4 output classes built from the following matrix: decoder-only vs encoder-decoder, uses beams vs doesn't use beams (https://github.com/huggingface/transformers/pull/40998)\r\n- Removed deprecated classes regarding decoding methods that were moved to the Hub due to low usage (constraints and beam scores) (https://github.com/huggingface/transformers/pull/41223)\r\n- If `generate` doesn't receive any KV Cache argument, the default cache class used is now defined by the model (as opposed to always being `DynamicCache`) (https://github.com/huggingface/transformers/pull/41505)\r\n- Generation parameters are no longer accessible via model's config. If generation paramaters are serialized in `config.json` for any old model, it will be loaded back into model's generation config. Users are expected to access or modify generation parameters only with `model.generation_config.do_sample = True`. \r\n\r\n## Trainer\r\n\r\n### New Features\r\n\r\n* **ALST/Ulysses Sequence Parallelism Integration**\r\n  - Added sequence parallelism support via HF Accelerate for training with longer sequences. Enables splitting sequences across devices using ALST (All-to-All Long Sequence Training) and Ulysses algorithms with DeepSpeed.\r\n* **Improved `compute_loss_func` Handling**\r\n  - `compute_loss_func` now always takes priority over the model's built-in loss computation, giving users consistent control over custom loss functions.\r\n* **`num_items_in_batch` in Prediction Step**\r\n  - The `num_items_in_batch` argument is now passed to `compute_loss` during `prediction_step`, enabling proper loss scaling during evaluation.\r\n\r\n### Breaking Changes\r\n\r\n* **`report_to` now defaults to `\"none\"`**\r\n  - Logging integrations are no longer auto-detected by default; users must explicitly specify which reporting backends to use.\r\n\r\n### Removing arguments without deprecation cycle in `TrainingArguments` due to low usage\r\n\r\n- `mp_parameters` -> legacy param that was later on added to the Sagemaker trainer\r\n- `_n_gpu` -> not intended for users to set, we will initialize it correctly instead of putting it in the `TrainingArguments`\r\n- `overwrite_output_dir` - > replaced by `resume_from_checkpoint`, and it was only used in the examples script, no impact on Trainer. \r\n- `logging_dir` -> only used for tensorboard, set `TENSORBOARD_LOGGING_DIR` env var instead\r\n- `jit_mode_eval` -> use `use_torch_compile` instead, as torchscript is not recommended anymore\r\n- `tpu_num_cores`-> It is actually better to remove it, as it is not recommended to set the number of cores. By default, all TPU cores are used . Set `TPU_NUM_CORES` env var instead\r\n- `past_index` -> it was only used for a very small number of models that have special architecture like transformersxl + it was not documented at all how to train those models\r\n- `ray_scope` -> only for a minor arg for ray integration. Set `RAY_SCOPE` var env instead \r\n- `warmup_ratio` -> use `warmup_step` instead. We combined both args together by allowing passing float values in `warmup_step`. \r\n\r\n### Removing deprecated arguments in `TrainingArguments`\r\n\r\n- `fsdp_min_num_params` and `fsdp_transformer_layer_cls_to_wrap` -> use `fsdp_config`\r\n- `tpu_metrics_debug` -> `debug` \r\n- `push_to_hub_token` -> `hub_token`\r\n- `push_to_hub_model_id` and `push_to_hub_organization` -> `hub_model_id`\r\n- `include_inputs_for_metrics` -> `include_for_metrics`\r\n- `per_gpu_train_batch_size` -> `per_device_train_batch_size`\r\n- `per_gpu_eval_batch_size` -> `per_device_eval_batch_size`\r\n- `use_mps_device` -> mps will be used by default if detected\r\n- `fp16_backend` and `half_precision_backend` -> we will only rely on `torch.amp` as everything has been upstreamed to torch\r\n- `no_cuda` -> `use_cpu`\r\n- ` include_tokens_per_second` -> `include_num_input_tokens_seen`\r\n- `use_legacy_prediction_loop` -> we only use `evaluation_loop` function from now on\r\n\r\n### Removing deprecated arguments in `Trainer`\r\n\r\n- `tokenizer` in initialization -> `processing_class`\r\n- `model_path` in train() -> `resume_from_checkpoint`\r\n\r\n### Removed features for `Trainer`\r\n\r\n- sigpot integration for hp search was removed as the library was archived + the api stopped working\r\n- drop support for sagemaker API <1.10\r\n- bump accelerate minimum version to 1.1.0 \r\n- bump peft minimum version to 0.18.0\r\n- bump bitsandbytes minimum version to 0.46.1\r\n\r\n###  New defaults for `Trainer`\r\n\r\n- `use_cache` in the model config will be set to `False`. You can still change the cache value through `TrainingArguments` `usel_cache` argument if needed. \r\n\r\n## Pipeline\r\n\r\n- Image text to text pipelines will no longer accept images as a separate argument along with conversation chats. Image data has to be embedded in the chat's \"content\" field. See [#42359](https://github.com/huggingface/transformers/pull/42359)\r\n\r\n## PushToHubMixin\r\n\r\n- removed deprecated `organization` and `repo_url` from `PushToHubMixin`. You must pass a `repo_id` instead.\r\n- removed `ignore_metadata_errors` from `PushToMixin`. In practice if we ignore errors while loading the model card, we won't be able to push the card back to the Hub so it's better to fail early and not provide the option to fail later.\r\n- `push_to_hub` do not accept `**kwargs` anymore. All accepted parameters are explicitly documented.\r\n- arguments of `push_to_hub` are now keyword-only to avoid confusion. Only `repo_id` can be positional since it's the main arg.\r\n- removed `use_temp_dir` argument from `push_to_hub`. We now use a tmp dir in all cases.\r\n\r\nLinked PR: https://github.com/huggingface/transformers/pull/42391.\r\n\r\n## CLI\r\n\r\nThe deprecated `transformers-cli ...` command was deprecated, `transformers ...` is now the only CLI entry point.\r\n\r\n`transformers` CLI has been migrated to `Typer`, making it easier to maintain + adding some nice features out of \r\nthe box (improved `--help` section, autocompletion).\r\n\r\nBiggest breaking change is in `transformers chat`. This command starts a terminal UI to interact with a chat model. \r\nIt used to also be able to start a Chat Completion server powered by `transformers` and chat with it. In this revamped \r\nversion, this feature has been removed in favor of `transformers serve`. The goal of splitting `transformers chat` \r\nand `transformers serve` is to define clear boundaries between client and server code. It helps with maintenance \r\nbut also makes the commands less bloated. The new signature of `transformers chat` is:\r\n\r\n```\r\nUsage: transformers chat [OPTIONS] BASE_URL MODEL_ID [GENERATE_FLAGS]...\r\n\r\nChat with a model from the command line.\r\n```\r\n\r\nIt works hand in hand with `transformers serve`, which means that if `transformers serve` is running on its default endpoint, `transformers chat` can be launched as follows:\r\n\r\n```sh\r\ntransformers chat HuggingFaceTB/SmolLM3-3B\r\n```\r\n\r\nIt can however use any OpenAI API compatible HTTP endpoint:\r\n\r\n```sh\r\ntransformers chat HuggingFaceTB/SmolLM3-3B https://router.huggingface.co/v1\r\n```\r\n\r\nLinked PRs: \r\n- https://github.com/huggingface/transformers/pull/40997\r\n- https://github.com/huggingface/transformers/pull/41487\r\n\r\n### Removal of the `run` method\r\n\r\nThe `transformers run` (previously `transformers-cli run`) is an artefact of the past, was not documented nor tested,\r\nand isn't part of any public documentation. We're removing it for now and ask you to please let us know in case\r\nthis is a method you are using; in which case we should bring it back with better support.\r\n\r\nLinked PR: https://github.com/huggingface/transformers/pull/42447\r\n\r\n## Environment variables\r\n\r\n- Legacy environment variables like `TRANSFORMERS_CACHE`, `PYTORCH_TRANSFORMERS_CACHE`, and `PYTORCH_PRETRAINED_BERT_CACHE` have been removed. Please use `HF_HOME` instead.\r\n- Constants `HUGGINGFACE_CO_EXAMPLES_TELEMETRY`, `HUGGINGFACE_CO_EXAMPLES_TELEMETRY`, `HUGGINGFACE_CO_PREFIX`, and `HUGGINGFACE_CO_RESOLVE_ENDPOINT` have been removed. Please use `huggingface_hub.constants.ENDPOINT` instead.\r\n\r\nLinked PR: https://github.com/huggingface/transformers/pull/42391.\r\n\r\n## Requirements update\r\n\r\n`transformers` v5 pins the `huggingface_hub` version to `>=1.0.0`. See this [migration guide](https://huggingface.co/docs/huggingface_hub/concepts/migration) to learn more about this major release. Here are to main aspects to know about:\r\n- switched the HTTP backend from `requests` to `httpx`. This change was made to improve performance and to support both synchronous and asynchronous requests the same way. If you are currently catching `requests.HTTPError` errors in your codebase, you'll need to switch to `httpx.HTTPError`.\r\n- related to 1., it is not possible to set proxies from your script. To handle proxies, you must set the `HTTP_PROXY` / `HTTPS_PROXY` environment variables\r\n- `hf_transfer` and therefore `HF_HUB_ENABLE_HF_TRANSFER` have been completed dropped in favor of `hf_xet`. This should be transparent for most users. Please let us know if you notice any downside!\r\n\r\n`typer-slim` has been added as required dependency, used to implement both `hf` and `transformers` CLIs.\r\n\r\n## New model additions in v5\r\n\r\n### CWM\r\n\r\n<img width=\"809\" height=\"471\" alt=\"image\" src=\"https://github.com/user-attachments/assets/58bb9c70-d481-48ed-ab8f-6553be7c240f\" />\r\n\r\nThe Code World Model (CWM) model was proposed in [CWM: An Open-Weights LLM for Research on Code Generation with World Models](https://ai.facebook.com/research/publications/cwm) by Meta FAIR CodeGen Team. CWM is an LLM for code generation and reasoning about code that has, in particular, been trained to better represent and reason about how code and commands affect the state of a program or system. Specifically, we mid-trained CWM on a large number of observation-action trajectories from Python execution traces and agentic interactions in containerized environments. We post-trained with extensive multi-task RL in verifiable coding, math, and multi-turn software engineering environments.\r\n\r\n* Add Code World Model (CWM)  by @jacobkahn in #41199\r\n\r\n### SAM3\r\n\r\n<img width=\"1505\" height=\"915\" alt=\"image\" src=\"https://github.com/user-attachments/assets/eec48633-f02b-464a-ae5c-c65473387e53\" />\r\n\r\nSAM3 (Segment Anything Model 3) was introduced in [SAM 3: Segment Anything with Concepts](https://ai.meta.com/research/publications/sam-3-segment-anything-with-concepts/).\r\n\r\nThe SAM3 addition adds four new architectures:\r\n- Sam3\r\n- Sam3Tracker\r\n- Sam3TrackerVideo\r\n- Sam3Video\r\n\r\nSAM3 performs Promptable Concept Segmentation (PCS) on images. PCS takes text and/or image exemplars as input (e.g., \"yellow school bus\"), and predicts instance and semantic masks for every single object matching the concept.\r\n\r\nSam3Tracker and Sam3TrackerVideo perform Promptable Visual Segmentation (PVS) on images. PVS takes interactive visual prompts (points, boxes, masks) or text inputs to segment a specific object instance per prompt. This is the task that SAM 1 and SAM 2 focused on, and SAM 3 improves upon it. Sam3Tracker and Sam3TrackerVideo are updated versions of SAM2 Video that maintain the same API while providing improved performance and capabilities.\r\n\r\nSAM3 Video performs Promptable Concept Segmentation (PCS) on videos. PCS takes text as input (e.g., \"yellow school bus\"), and predicts instance and semantic masks for every single object matching the concept, while preserving object identities across video frames. The model combines a detection module (SAM3) with a tracking module (SAM2-style tracker) to enable robust object tracking across video frames using text prompts.\r\n\r\n* Add SAM3 to ü§ó Transformers  by @yonigozlan in #42285\r\n\r\n### LFM2 MoE\r\n\r\n<img width=\"1080\" height=\"849\" alt=\"image\" src=\"https://github.com/user-attachments/assets/a9fa1b81-114d-4054-9699-5083ac69d830\" />\r\n\r\nLFM2-MoE is a Mixture-of-Experts (MoE) variant of [LFM2](https://huggingface.co/collections/LiquidAI/lfm2-686d721927015b2ad73eaa38). The LFM2 family is optimized for on-device inference by combining short‚Äërange, input‚Äëaware gated convolutions with grouped‚Äëquery attention (GQA) in a layout tuned to maximize quality under strict speed and memory constraints.\r\n\r\nLFM2‚ÄëMoE keeps this fast backbone and introduces sparse MoE feed‚Äëforward networks to add representational capacity without significantly increasing the active compute path. The first LFM2-MoE release is LFM2-8B-A1B, with 8.3B total parameters and 1.5B active parameters. The model excels in quality (comparable to 3-4B dense models) and speed (faster than other 1.5B class models).\r\n\r\n* [Model] Lfm2Moe  by @paulpak58 in #41401\r\n\r\n### VideoLlama 3\r\n\r\n<img width=\"812\" height=\"366\" alt=\"image\" src=\"https://github.com/user-attachments/assets/21c82c6e-cf0a-4d6c-a707-b9e57663ca85\" />\r\n\r\nThe [VideoLLaMA3](https://huggingface.co/papers/2501.13106) model is a major update to [VideoLLaMA2](https://huggingface.co/papers/2406.07476) from Alibaba DAMO Academy.\r\n\r\n* [model] Add VideoLLaMA3 implementation  by @lkhl in #40499\r\n\r\n### AudioFlamingo 3\r\n\r\n<img width=\"621\" height=\"475\" alt=\"image\" src=\"https://github.com/user-attachments/assets/c9616758-b3aa-41d0-bd58-695966ba146d\" />\r\n\r\nAudio Flamingo 3 (AF3) is a fully open large audio‚Äìlanguage model designed for robust understanding and reasoning over speech, environmental sounds, and music. AF3 pairs a Whisper-style audio encoder with a causal language model and performs replace-in-place audio‚Äìtext fusion: the processor aligns post-pool audio frames to a dedicated placeholder token and the model replaces those token slots with projected audio embeddings during the forward pass.\r\n\r\nThe model checkpoint is available at: [nvidia/audio-flamingo-3-hf](https://huggingface.co/nvidia/audio-flamingo-3-hf)\r\n\r\nHighlights:\r\n\r\n- Unified audio encoder across speech, sound, and music.\r\n- Long-audio support via windowing and post-pool alignment (up to 10 minutes maximum). The model processes audio in 30-second windows with a hard limit of 20 windows (10 minutes total). Audio longer than 10 minutes will be truncated.\r\n- Deterministic fusion that preserves sequence length by replacing audio placeholder tokens with audio embeddings.\r\n\r\n* [models] Add AudioFlamingo3 integration  by @lashahub in #40290\r\n\r\n### Nanochat\r\n\r\n[NanoChat](https://huggingface.co/karpathy/nanochat-d32) is a compact decoder-only transformer model designed for educational purposes and efficient training. The model features several fundamental architectural innovations which are common in modern transformer models. Therefore, it is a good model to use as a starting point to understand the principles of modern transformer models. NanoChat is a variant of the [Llama](https://huggingface.co/docs/transformers/en/model_doc/llama) architecture, with simplified attention mechanism and normalization layers.\r\n\r\n* [MODEL] Nanochat implementation  by @burtenshaw in #41634\r\n\r\n## Bugfixes and improvements\r\n\r\n* `JetMoe` Fix jetmoe after #40132  by @ArthurZucker in #41324\r\n* Fixed tiny incorrect import in `gemma3`  by @Sai-Suraj-27 in #41354\r\n* Rope for Qwen2--5-vl  by @zucchini-nlp in #41173\r\n* üö® Bump to Python 3.10 and rework how we check 3rd-party libraries existence  by @Cyrilvallez in #41268\r\n* Standardize `PretrainedConfig` to `PreTrainedConfig`  by @Cyrilvallez in #41300\r\n* Fix trainer for py3.9  by @SunMarc in #41359\r\n* Check model inputs - hidden states  by @zucchini-nlp in #40994\r\n* [`ModularChecker`] QOL for the modular checker  by @ArthurZucker in #41361\r\n* Fixing a typo for BLT model  by @Narsil in #41325\r\n* :rotating_light: [`v5`] Remove relative position embeddings (for bert like models)  by @vasqu in #41170\r\n* Fix typo in model proposal template  by @Ombucha in #41352\r\n* Better typehints for `apply_chat_template`  by @Samoed in #41355\r\n* üö® Remove BetterTransformer  by @Cyrilvallez in #41367\r\n* [testing] update `test_longcat_generation_cpu`  by @ydshieh in #41368\r\n* Fix flash_attention.py: wrong argument passing for attn_implementation  by @TKONIY in #41347\r\n* Use canonical get_size_with_aspect_ratio (with max_size) from transformers.image_transforms to fix #37939  by @sonianuj287 in #41284\r\n* Fixes in check_model_inputs, GPTBigCodeModel and ImageGPTModel  by @IlyasMoutawwakil in #40811\r\n* Remove unnecessary list comprehension  by @cyyever in #41305\r\n* make some ut cases pass on xpu w/ latest torch  by @yao-matrix in #41337\r\n* Remove unused function patameters  by @cyyever in #41358\r\n* [`CB`] Refactors the way we access paged  by @ArthurZucker in #41370\r\n* serve: add non-streaming mode to /v1/responses; stream event parity; remove placeholder logprobs  by @antznette1 in #41353\r\n* Update from pretrained error when loading  by @ArthurZucker in #33380\r\n* [`v5`] Sync Bert and Bart eager attention  by @vasqu in #41248\r\n* fix asr ut failures  by @yao-matrix in #41332\r\n* fix resample in asr pipeline  by @yhzx233 in #41298\r\n* Correct numerical regression in vision embeddings  by @i3hz in #41374\r\n* [kernels] Kernel Config   by @MekkCyber in #41232\r\n* [Cache] lfm2 cache: allocate empty kv layers during init  by @paulpak58 in #41396\r\n* Fix test for model with dotted name and relative imports  by @st81 in #41343\r\n* Prefer raising `TypeError` exception for invalid type  by @Sai-Suraj-27 in #41346\r\n* [v5] Bump accelerate to 1.1.0   by @SunMarc in #41234\r\n* Fix incorrect assignment in `update_device_map` for GPTQ quantizer  by @Sai-Suraj-27 in #41328\r\n* [v5] Delete left traces of feature extractor  by @zucchini-nlp in #41321\r\n* Remove deprecation warning  by @Cyrilvallez in #41425\r\n* Fix overriding common_kwargs defaults in processor calls  by @yonigozlan in #41381\r\n* v5 dev version  by @LysandreJik in #41436\r\n* Tiny Cleanup - Removed duplicate class field definition's  by @Sai-Suraj-27 in #41293\r\n* üö®üö® Remove all traces of legacy cache format  by @Cyrilvallez in #41378\r\n* üö® [v5] Prune `prune_heads`  by @gante in #41417\r\n* [v5] Bump min version of bitsandbytes to 0.46.1   by @SunMarc in #41283\r\n* Fixing comments in __init__ file  by @MekkCyber in #41414\r\n* Use accelerator API to free device memory  by @cyyever in #41195\r\n* enable new model uts to xpu and fix some failures on xpu  by @yao-matrix in #41386\r\n* [torchao] Add regex support for ModuleFqnToConfig  by @jerryzh168 in #41242\r\n* :facepalm: CB nit!   by @ArthurZucker in #41413\r\n* Remove Python 3.9 classifier  by @cyyever in #41410\r\n* [`JetMoe`] Fix KV head repetition and padding free  by @vasqu in #41423\r\n* [testing] Fix `JetMoeIntegrationTest`  by @ydshieh in #41377\r\n* Add Top-H decoding (entropy-bounded truncation) as a LogitsWarper for text generation  by @ErfanBaghaei in #40837\r\n* Validate processing kwargs with @strict from huggingface_hub   by @zucchini-nlp in #40793\r\n* Update hqq.md  by @prathamesh-chavan-22 in #41452\r\n* enable some falcon-mamba uts on xpu  by @yao-matrix in #41428\r\n* Fix generate outputs and simplify cache tests  by @Cyrilvallez in #41440\r\n* Fix doc  by @Cyrilvallez in #41457\r\n* üö® [v5] Rename left traces of `past_key_value` in BERT-like models  by @zucchini-nlp in #41448\r\n* Subconfig is a class attribute  by @zucchini-nlp in #41308\r\n* [v5] rm `utils/tf_ops/`  by @gante in #41402\r\n* Update GLM-4.1V MMRope implementation  by @zRzRzRzRzRzRzR in #41182\r\n* [kernels] Cleanup deta kernel  by @MekkCyber in #41470\r\n* üö® [v5] Rendundant code in nested configs  by @zucchini-nlp in #41314\r\n* Remove KERAS_NLP_IMPORT_ERROR  by @cyyever in #41468\r\n* Fix auto model configuration for encoder of perceptionlm  by @fschlatt in #41464\r\n* Fix tests fsdp  by @SunMarc in #41422\r\n* Import Callable from collections.abc  by @cyyever in #41130\r\n* Pickle - part 2  by @ydshieh in #41476\r\n* Remove infer_device  by @cyyever in #41088\r\n* Change RT-Detr docs to reflect fixed 640x640 input size  by @konstantinos-p in #41364\r\n* Cleaning hub kernels   by @MekkCyber in #41477\r\n* [v5] remove load_in_4bit and load_in_8bit  by @SunMarc in #41287\r\n* :rotating_light: [`Attention Masks`] Bidirectional masks for encoder and encoder-decoder models  by @vasqu in #41265\r\n* [Fix] Fix test file error  by @YangKai0616 in #40973\r\n* enhance patched_tearDown to support python 3.11+  by @yao-matrix in #41429\r\n* RT-Detr correct 2d positional embeddings for non-square images  by @konstantinos-p in #41380\r\n* Fix bnb fsdp loading for pre-quantized checkpoint  by @SunMarc in #41415\r\n* Remove SigOpt  by @SunMarc in #41479\r\n* Remove `past_index`  by @SunMarc in #41384\r\n* Remove deprecated args in Trainer for v5  by @SunMarc in #41404\r\n* Update GLM-4.6 doc  by @zRzRzRzRzRzRzR in #41471\r\n* `report_to` default changed to \"none\" + cleaning deprecated env var  by @SunMarc in #41375\r\n* deprecate `overwrite_output_dir`  by @SunMarc in #41323\r\n* [`CI`] Fix copies on main  by @vasqu in #41486\r\n* [Trainer] deprecate ray scope  by @SunMarc in #41403\r\n* deprecate `jit_mode_eval`  by @SunMarc in #41376\r\n* Remove `local_rank` arg from `TrainingArguments`  by @SunMarc in #41382\r\n* Update philosophy  by @molbap in #41438\r\n* Remove DISABLE_KERNEL_MAPPING flag  by @MekkCyber in #41475\r\n* Streaming should be handled at the request-level rather than at the istance level  by @LysandreJik in #41444\r\n* fix bnb model loading  by @jiqing-feng in #41499\r\n* [kernels] Remove RWKV kernel finally !  by @MekkCyber in #41493\r\n* [kernels] rm yoso kernel  by @MekkCyber in #41495\r\n* Try to remove `pickle` - `BloomTokenizerFast`  by @ydshieh in #41466\r\n* Fixed tiny incorrect imports in `glm4v`  by @Sai-Suraj-27 in #41483\r\n* [Parakeet] unnecessary warning & auto mapping  by @eustlb in #41412\r\n* [causallm tester] automate pipeline mappings + bloom tests  by @gante in #41318\r\n* Fix some tests  by @Cyrilvallez in #41503\r\n* fix gemma3n case failure  by @yao-matrix in #41426\r\n* [voxtral] language detection + skipping lang:xx  by @eustlb in #41225\r\n* Set `truncation` to `False` in Qwen3Omni to avoid default truncation  by @BakerBunker in #41473\r\n* [QoL] modular conversion shows LoC saved  by @molbap in #41500\r\n* More trainer cleaning   by @SunMarc in #41489\r\n* Bump to hfh 1.0.0.rc5 to fix test  by @Wauplin in #41508\r\n* Revert `local_rank` deletion and some cleaning  by @SunMarc in #41504\r\n* Fix detectron2 import  by @Cyrilvallez in #41510\r\n* add Trainer import to .md in appropriate cell block for training.ipynb transformers_doc  by @benkeene in #41484\r\n* Remove outdated flags  by @Cyrilvallez in #41512\r\n* remove `tpu_num_cores`  by @SunMarc in #41383\r\n* Allow optuna's catch kwargs passthrough  by @nicha-api in #41496\r\n* Fix Latex typesetting in documentation  by @cyyever in #41177\r\n* [testing] reduce runtime of `HunYuanMoEV1IntegrationTest:test_model_generation`  by @ydshieh in #41373\r\n* [Qwen3VL] fix: hidden_states in place modification error  by @HollowMan6 in #41535\r\n* Add MLlama fast image processor  by @yonigozlan in #41391\r\n* Fixed Type-hints in function defintions  by @Sai-Suraj-27 in #41525\r\n* [SAM] Fix typing hints   by @zucchini-nlp in #41506\r\n* Restore cuda graphs to continuous batching  by @remi-or in #41421\r\n* Add AMD developer cloud support  by @fan-amd in #41126\r\n* Enable modular files from other libraries  by @regisss in #41372\r\n* üö® [v5] `generate` delegates default cache initialization to the model  by @gante in #41505\r\n* Fixed typos and formatting  by @julian-st in #34215\r\n* Add VideoMAE video processor   by @Aki-07 in #41534\r\n* [`from_pretrained`] Small refactor `from_pretrained`: move around unrelated stuff  by @ArthurZucker in #41445\r\n* Remove references to AutoModelForVision2Seq  by @Rocketknight1 in #41513\r\n* [Qwen3VL] fix device mismatch error for FSDP2 training  by @HollowMan6 in #41536\r\n* Patch MistralCommonTokenizer  by @juliendenize in #41439\r\n* Fix an import error with PreTrainModel  by @remi-or in #41571\r\n* [Qwen3VLMoe] Fixed: Expected self.dtype to be equal to src.dtype - routing_weights casting  by @danielquintas8 in #41420\r\n* [kernels] rm mra kernels  by @MekkCyber in #41507\r\n* delete some tokenizer tests using pickle  by @ydshieh in #41514\r\n* Add DINOv3Backbone for ConvNext variant  by @merveenoyan in #40651\r\n* Add conditional checks to _check_and_adjust_attn_implementation()  by @zheliuyu in #41542\r\n* add rmsnorm kernels support for Intel XPU  by @kaixuanliu in #41563\r\n* Revert \"add rmsnorm kernels support for Intel XPU\"  by @MekkCyber in #41579\r\n* [VisionEncoderDecoderModel] Update loss function  by @NielsRogge in #40863\r\n* Add __iter__ to DynamicCache  by @remi-or in #41569\r\n* Revert some breaking changes bnb  by @SunMarc in #41581\r\n* Fix typsetting and content of llm_tutorial_optimization.md  by @cyyever in #41172\r\n* Gemma3 fixes  by @remi-or in #41572\r\n* Benchmark overhaul  by @remi-or in #41408\r\n* Enable non-streaming mode in `transformers serve`  by @LysandreJik in #41446\r\n* [device_map] Accelerate loading by computing device_map much faster  by @Cyrilvallez in #41548\r\n* Add `logits_to_keep` to many older CausalLM models  by @philiproeleveld in #41335\r\n* fix some case failures lead by \"`torch.compile` recompiled part of th‚Ä¶  by @sywangyi in #41558\r\n* remove ray_scope and check_quantized_param  by @SunMarc in #41587\r\n* Update issue template   by @SunMarc in #41573\r\n* [`Docs`] Fix changed references  by @vasqu in #41614\r\n* Import `expand_device_map` instead of redefining it  by @Cyrilvallez in #41608\r\n* Fix trainer simple tests  by @SunMarc in #41449\r\n* More markdown file fixes  by @cyyever in #41599\r\n* torch 2.9 don't ‚ù§Ô∏è torchcodec üíî   by @ydshieh in #41610\r\n* Update a dataset reop link  by @ydshieh in #41618\r\n* Add fast path for bidirectional mask creation to fix regression  by @i3hz in #41586\r\n* enable sdpa enable gqa logic for Ascend NPU  by @FightingZhen in #41601\r\n* Fix video processing channel format  by @zucchini-nlp in #41603\r\n* [chat template] update when \"push_to_hub\"  by @zucchini-nlp in #39815\r\n* Remove the head masking block in some vision models  by @ydshieh in #41620\r\n* Remove deprecated code  by @SunMarc in #41616\r\n* Fix quantization base class   by @SunMarc in #41613\r\n* [docs] Duplicate entry  by @stevhliu in #41591\r\n* Update executorch.md  by @jackzhxng in #41582\r\n* Add Backbone API fine-tuning tutorial  by @merveenoyan in #41590\r\n* üö® [v5] Toggle the serialization format in processors  by @zucchini-nlp in #41474\r\n* Add aux loss for GLM-4.5V  by @zRzRzRzRzRzRzR in #41564\r\n* Allow passing `tp_plan` in `from_pretrained` directly  by @Cyrilvallez in #41435\r\n* Fix tokenization test  by @Cyrilvallez in #41649\r\n* Remove randomly added script  by @Cyrilvallez in #41650\r\n* Add missing dates to docs  by @yonigozlan in #41576\r\n* Migrate transformers cli to Typer  by @Wauplin in #41487\r\n* Fix FP-Quant quantization fallback CPU dispatch.  by @BlackSamorez in #41619\r\n* fix check inputs for text2text pipeline  by @jiqing-feng in #41556\r\n* [`Executorch`] Simplify for encoder models  by @vasqu in #41627\r\n* [`Ernie 4.5 Moe`] Fix Moe and offloading  by @vasqu in #41385\r\n* [CI] Build translated docs  by @stevhliu in #41632\r\n* Fix fp32_ln for various models  by @remi-or in #41605\r\n* Adjust device logging level and add minor fixes  by @mario-koddenbrock in #41636\r\n* Fix EncoderDecoder cache  by @remi-or in #41612\r\n* Format MarkDown documentation and tiny fixes  by @cyyever in #41638\r\n* Fix typos in documentation  by @cyyever in #41641\r\n* Fix confusing cls assignment  by @cyyever in #41642\r\n* Double router compute?  by @molbap in #41653\r\n* [kernels] refactor function kernel calling  by @MekkCyber in #41577\r\n* [Fix] Deepseek V3 expert bias routing  by @fjosw in #41647\r\n* purge HF_HUB_ENABLE_HF_TRANSFER; promote Xet  by @Vaibhavs10 in #41656\r\n* [`Masks`] Fix mask handling in eager for vision models  by @vasqu in #41625\r\n* Use | for Optional and Union typing  by @cyyever in #41646\r\n* Switch to CB if cache_implementation == paged  by @remi-or in #41655\r\n* Add in-out modalities as class attribute per model  by @zucchini-nlp in #41366\r\n* Fix dtype casting with quantization  by @Cyrilvallez in #41665\r\n* Fix serving continuous batching  by @SunMarc in #41624\r\n* Small changes to benchmarking script  by @remi-or in #41662\r\n* Improve package version check  by @Cyrilvallez in #41661\r\n* improve `utils/check_bad_commit.py`  by @ydshieh in #41658\r\n* Erroring when KernelConfig is passed without use_kernels = True  by @MekkCyber in #41657\r\n* [Trainer] [Breaking change] `use_cache` default to `False`  by @SunMarc in #41585\r\n* üåê [i18n-KO] Translated `chat_extras.md` to Korean  by @Judy-Choi in #39863\r\n* üåê [i18n-KO] Translated sam_hq.md to Korean  by @HyunZ118 in #41340\r\n* [i18n-KO] Translated `big_bird.md` to Korean  by @ssum21 in #40445\r\n* üåê [i18n-KO] Translated `code_llama.md` to Korean  by @Judy-Choi in #40558\r\n* üåê [i18n-KO] Translated llama4.md to Korean  by @TaskerJang in #40396\r\n* :globe_with_meridians: [i18n-KO] Translated `ko-LFM2.md` to Korean  by @ssum21 in #41502\r\n* Adding superglue fast image processing  by @AlphaOrOmega in #41394\r\n* Fix ckpt in docs  by @zucchini-nlp in #41659\r\n* torch 2.9 still don't ‚ù§Ô∏è torchcodec 0.8 üíî  by @ydshieh in #41686\r\n* Remove deprecated `use_auth_token` parameter  by @Wauplin in #41666\r\n* Remove  require_torch_bf16_gpu  by @cyyever in #40979\r\n* path validation for security reason  by @ydshieh in #41256\r\n* üö® Remove torchscript support  by @Cyrilvallez in #41688\r\n* Fix MarkDown syntax  by @cyyever in #41676\r\n* Use | for Optional and Union typing   by @cyyever in #41675\r\n* üö® [v5] Refactor RoPE for layer types  by @zucchini-nlp in #39847\r\n* Enable faiss-cpu on Windows  by @cyyever in #41678\r\n* Fix Pylint warnings  by @cyyever in #41644\r\n* üö® Remove torch.fx support  by @Cyrilvallez in #41683\r\n* Remove skipped tests without parents  by @Cyrilvallez in #41691\r\n* Enable  FURB rules in ruff  by @cyyever in #41395\r\n* Remove upper version bound of pandas  by @cyyever in #41677\r\n* [`Attn`] Allow dynamic causality in SDPA via Kwargs  by @vasqu in #41692\r\n* Simplify GQA conditions in sdpa_attention.py  by @justinchuby in #41699\r\n* [docs] Manual tp-plan  by @stevhliu in #41674\r\n* üåê [i18n-KO] Translated gemma3n.md to Korean  by @HyunZ118 in #40873\r\n* pin torchcodec on CI docker image  by @ydshieh in #41703\r\n* Update `run_name` docs in TrainingArguments  by @tobiasofsn in #41705\r\n* further improve `utils/check_bad_commit.py`  by @ydshieh in #41658) \r\n* feat: add benchmark v2 ci with results pushed to dataset  by @McPatate in #41672\r\n* Gemma3 conversion script maintenance  by @RyanMullins in #41704\r\n* Fix Qwen3-Omni inference when mixing video and image inputs in one batch  by @BakerBunker in #41741\r\n* Fix typo in LFM-VL  by @zucchini-nlp in #41742\r\n* Revert \"Remove upper version bound of pandas\"  by @ydshieh in #41744\r\n* [doc] remove broken notebooks on AMD Dev Cloud  by @pagezyhf in #41743\r\n* Update type hints in tokenization_utils.py to use | syntax  by @faizan842 in #41713\r\n* Fix documentation issues  by @cyyever in #41726\r\n* Apply RUFF PIE rules  by @cyyever in #41727\r\n* Small Fix for imports   by @MekkCyber in #41411\r\n* Docs(zh-hans): Refine wording for professionalism in README  by @Ri-Nai in #40943\r\n* Add vision contribution guide  by @molbap in #41456\r\n* upgrade xpu docker file to torch 2.8  by @yao-matrix in #41551\r\n* [v5] Delete `videos` from image processing classes   by @zucchini-nlp in #41607\r\n* Fixed incorrect model_type for qwen2vl and qwen2.5vl when config is saved and loaded again  by @i3hz in #41758\r\n* [kernels] Add version to function mapping  by @MekkCyber in #41685\r\n* Reduce warning noise caused by Tensor.new_tensor  by @st81 in #41748\r\n* Fix graphormer model compilation with Cython 3.1.4  by @alexmalyshev in #41671\r\n* Update type hints in modeling_rope_utils.py to use | syntax  by @faizan842 in #41714\r\n* [v5] Remove deprecated tranformers.onnx  by @echarlaix in #41700\r\n* Modernize CLIP modeling code   by @molbap in #41546\r\n* Simplify pipeline padding logic  by @Rocketknight1 in #41667\r\n* Chat response parsing  by @Rocketknight1 in #40894\r\n* Add LightGlue fast image processor  by @yonigozlan in #41670\r\n* Fix bark after #41445  by @ydshieh in #41645\r\n* Remove invalid `@staticmethod` from module-level get_device_and_memory_breakdown  by @albertvillanova in #41747\r\n* Fix CUDA index out of bounds for q_idx in VLM token type masking for Gemma3, PaliGemma, and example modular  by @albertvillanova in #41757\r\n* fix: Gemma 3 weights conversion vision and multimodal projector paths  by @RyanMullins in #41767\r\n* [v5] Delete legacy chat template saving  by @zucchini-nlp in #41648\r\n* [quantization] fix compressed_tensors tests  by @MekkCyber in #41780\r\n* [quantization] Skip Fp8 tests when hardware capability < 8.9  by @MekkCyber in #41785\r\n* Swap columns and rows of the grid layout in LFM2-VL  by @ankke in #41755\r\n* fix type annotation typo in docstring  by @johntheprime in #41788\r\n* Fix chat schema tests  by @Rocketknight1 in #41793\r\n* Fix attention mask in mamba layers  by @zucchini-nlp in #41790\r\n* [quantization] fix torchao tests after 0.14.0 release  by @MekkCyber in #41777\r\n* [`Onnx docs`] Remove some traces  by @vasqu in #41791\r\n* flash attn pytest marker  by @ydshieh in #41781\r\n* Bump AMD docker  by @remi-or in #41792\r\n* make apollo test case pass  by @yao-matrix in #41805\r\n* Add a safeguard around a flaky test in gemma2  by @remi-or in #41811\r\n* Fix Qwen3Next dtype API usage  by @SrijanUpadhyay in #41735\r\n* [Trainer] remove env vars   by @SunMarc in #41697\r\n* Fixed grammar mistakes  by @FrogWarlord in #41799\r\n* Fixed some grammar mistakes  by @FrogWarlord in #41802\r\n* transformers cli default flag fix  by @ArjunPimpale in #41761\r\n* Deprecate warmup_ratio  by @SunMarc in #41326\r\n* transformers serve quantization docs + some api fixes for bitsandbytes  by @SunMarc in #41253\r\n* [Parakeet] add output_attention_mask  by @eustlb in #41694\r\n* unpin torch/torchcodec for CircleCI  by @ydshieh in #41839\r\n* extend bitnet cases to xpu, all 8 cases pass  by @yao-matrix in #41831\r\n* extend 2 trainer test cases to xpu  by @yao-matrix in #41829\r\n* extend 2 blip2 and falcon_h1 test cases to xpu  by @yao-matrix in #41825\r\n* further reducing flakiness in `utils/check_bad_commit.py`  by @ydshieh in #41658)  \r\n* Remove redundant code from Qwen3VLProcessor  by @Xqle in #41836\r\n* Fix MXFP4 quantizer to support variable num_local_experts and hidden_size  by @marksverdhei in #41795\r\n* Fix Qwen2Audio flash attention mask format for generation  by @Abdennacer-Badaoui in #41843\r\n* Fix const parsing for dict inputs in chat schemas  by @Rocketknight1 in #41824\r\n* Share embedding modules in BART, not only weights  by @githubnemo in #41821\r\n* Fix TypeError: find_adapter_config_file() got an unexpected keyword argument '_adapter_model_path'  by @albertvillanova in #41604\r\n* :rotating_light: [`Clip`] Fix masking and enable flash attention on all model types  by @vasqu in #41750\r\n* CI workflow for Flash Attn  by @ydshieh in #41857\r\n* Fix torch.no_grad decorator in VLMS  by @yaswanth19 in #41888\r\n* Fix installation cmds in docs  by @yaswanth19 in #41887\r\n* revert changes in _is_package_available  by @MekkCyber in #41891\r\n* make lfm2_moe integration test pass on XPU  by @yao-matrix in #41796\r\n* Fix: avoid duplicate token in maybe_load_adapters  by @luaenrique in #41903\r\n* speed up loading checkpoints for zero stage 3  by @ri938 in #41850\r\n* evaluate>=0.4.6 is needed  by @stas00 in #41920\r\n* Add 6 huggingface notebooks on AMD dev cloud  by @fan-amd in #41883\r\n* Fix invalid examples in QwenVL model docstrings and add Qwen3VL example  by @Xqle in #41812\r\n* Allow parse_response to accept token IDs  by @Rocketknight1 in #41849\r\n* Fix Florence2 conversion script model_type KeyError  by @i3hz in #41866\r\n* Update some workflow files  by @ydshieh in #41892\r\n* fix some ut failures on XPU w/ torch 2.9  by @yao-matrix in #41923\r\n* Cache latest pytorch amd image locally on mi325 CI runner cluster  by @jitesh-gupta in #41926\r\n* Minor fix in docker image build workflow  by @ydshieh in #41949\r\n* fix some ut failures on XPU w/ torch 2.9  by @yao-matrix in #41941\r\n* Fix rope_parameters for gemma3 weights conversion script  by @douglas-reid in #41922\r\n* Fix: Gemma3TextConfig rope scaling assignments  by @RyanMullins in #41934\r\n* fix prepare_config_and_inputs_for_common bug in llava test  by @yao-matrix in #41942\r\n* Fix: prevent .gitignore truncation in run_clm_no_trainer.py  by @luaenrique in #41957\r\n* V4.57.1 training ci: Refactor `test_tensor_parallel.py`  by @3outeille in #41918\r\n* [v5] Return a BatchEncoding dict from apply_chat_template by default  by @Rocketknight1 in #41626\r\n* make recurrent_gemma and voxtral cases pass on xpu  by @yao-matrix in #41958\r\n* Fix typo in image_processing_lfm2_vl_fast  by @yonigozlan in #41940\r\n* Run slow v2  by @ydshieh in #41914\r\n* Fix `detectron2` installation in docker files  by @ydshieh in #41975\r\n* Fix `autoawq[kernels]` installation in quantization docker file  by @ydshieh in #41978\r\n* add support for saving encoder only so any parakeet model can be loaded for inference  by @nithinraok in #41969\r\n* Use indices as position_ids in modernebert  by @remi-or in #41789\r\n* test tensor parallel: make tests for dense model more robust  by @3outeille in #41968\r\n* fix: dict[RopeParameters] to dict[str, RopeParameters]  by @RyanMullins in #41963\r\n* docs: add continuous batching page  by @McPatate in #41847\r\n* Fix `torchcodec` version in quantization docker file  by @ydshieh in #41988\r\n* [kernels] Add Tests & CI for kernels  by @MekkCyber in #41765\r\n* Move the Mi355 to regular docker  by @remi-or in #41989\r\n* More data in benchmarking  by @remi-or in #41848\r\n* fix (CI): Refactor SSH runners  by @glegendre01 in #41991\r\n* fix 3 failed test cases for video_llama_3 model on Intel XPU  by @kaixuanliu in #41931\r\n* Integrate colqwen2.5 using colqwen2 modelling code  by @sahil-kabir in #40600\r\n* Fixed wrong padding value in OWLv2  by @gjamesgoenawan in #41938\r\n* Fix `run slow v2`: empty report when there is only one model  by @ydshieh in #42002\r\n* [kernels] change import time in KernelConfig  by @MekkCyber in #42004\r\n* DOC Fix typo in argument name: pseudoquant  by @BenjaminBossan in #41994\r\n* Fix `torch+deepspeed` docker file  by @ydshieh in #41985\r\n* Correct syntax error in trainer.md  by @Yacklin in #42001\r\n* Reduce the number of benchmark in the CI  by @remi-or in #42008\r\n* Fix continuous batching tests  by @Rocketknight1 in #42012\r\n* add back `logging_dir`  by @SunMarc in #42013\r\n* Fix issue with from pretrained and kwargs in image processors  by @yonigozlan in #41997\r\n* Fix default image_rows and image_cols initialization in Idefics3 and SmolVLM processors  by @MilkClouds in #41871\r\n* Add GLPNImageProcessorFast   by @Aravind-11 in #41725\r\n* add fuyu fast image processors  by @DeXtAr47-oss in #41817\r\n* [kernels] Fix XPU layernorm kernel  by @MekkCyber in #41583\r\n* [v5] Deprecate Text2Text and related pipelines  by @Rocketknight1 in #41996\r\n* [FPQuant] MXFP8 and MXFP4 backwards support  by @BlackSamorez in #41897\r\n* fix `deeepspeed` in AMD docker file  by @ydshieh in #42025\r\n* CodeQL workflow for security analysis  by @paulinebm in #42015\r\n* [tests] Add Context-parallel CI tests  by @kashif in #41860\r\n* extend fp_quant cases to xpu  by @yao-matrix in #41833\r\n* Change trigger time for AMD CI  by @ydshieh in #42034\r\n* Fix the order of methods in processor loading  by @zucchini-nlp in #42031\r\n* üî¥  Isolate prefill from generation loops  by @manueldeprada in #40652\r\n* update `huggingface_hub` dependency version  by @hanouticelina in #42033\r\n* Remove some custom datasets defined in codebase  by @ydshieh in #41511\r\n* Cleanup workflow - part 1  by @ydshieh in #42023\r\n* Fix `pr_slow_ci_suggestion.yml` after #42023  by @ydshieh in #42049\r\n* Fix AutoImageProcessor.register and documentation in auto processing modules  by @MilkClouds in #41864\r\n* Fix Qwen3-Omni RoPE  by @zucchini-nlp in #41778\r\n* Avoid explicit checkout in workflow  by @ydshieh in #42057\r\n* Annoying typo in attention error message  by @manueldeprada in #42037\r\n* Be careful at explicit checkout actions  by @ydshieh in #42060\r\n* Fix another `Argument list too long` in `pr_slow_ci_suggestion.yml`  by @ydshieh in #42061\r\n* Fix KeyError in GPT-OSS weight conversion script  by @Aznix07 in #42007\r\n* Fix KeyError in _is_package_available for packages with dotted names  by @yashwantbezawada in #42050\r\n* Revert back to use GitHub context   by @ydshieh in #42066\r\n* Fix missing arg in check_docstring  by @yonigozlan in #42054\r\n* [deepspeed tests fixes]  by @stas00 in #41925\r\n* Fix logic in setting self.fsdp when it is False  by @roychan in #41974\r\n* fix tensor device placement issue of 2 UT cases  by @yao-matrix in #41921\r\n* add workflow to check permissions and advise a set of permissions req‚Ä¶  by @paulinebm in #42071\r\n* Fix security issue 5  by @paulinebm in #42072\r\n* Fix inconsistency of commit sha during the workflow run  by @ydshieh in #42074\r\n* QwenVL: add skipped keys in `setattr` as well  by @zucchini-nlp in #41808\r\n* permissions worflows fix  by @paulinebm in #42080\r\n* 4.1V Model and GLM-4.5V Model Conversion Code Updates  by @zRzRzRzRzRzRzR in #41784\r\n* feat(ci): add continuous batching to benchmarks  by @McPatate in #41916\r\n* Fix modular docstring for Mixtral  by @diegoakel in #42041\r\n* Fix Auto classes to support dynamically registered processors  by @MilkClouds in #41865\r\n* Reinstate self.scaling in Gemma3nTextAttention  by @RyanMullins in #41751\r\n* [v5] üö®Refactor subprocessors handling in processors  by @yonigozlan in #41633\r\n* add xpu support in test_modeling_janus.py::JanusIntegrationTest::test‚Ä¶  by @sywangyi in #41986\r\n* Revert \"permissions worflows fix\"  by @ydshieh in #42110\r\n* Fix return metadata checking logic  by @Xqle in #42108\r\n* Correctly handle unbatched audio inputs in Gemma3nAudioFeatureExtractor  by @kho in #42076\r\n* [Bugfix] fix qwen3vl expand generation with video  by @JJJYmmm in #42089\r\n* Fix base model prefix in VLMs  by @zucchini-nlp in #42059\r\n* fix continuous batching issues, extend ut cases to xpu  by @yao-matrix in #41830\r\n* üìù docs(smolvlm): fix variable name in batch inference example  by @gorkachea in #42123\r\n* fix qwen2vl/qwen3vl video processor temporal padding when num_frames%temporal_patch_size!=1  by @yaogang2060 in #42083\r\n* [`Attn Masks`] Non-vmap default for attention masks  by @vasqu in #41852\r\n* Fix GPT-2 Flash Attention 2 generation with left-padding  by @Abdennacer-Badaoui in #41966\r\n* Fix model name test for compressed tensors   by @SunMarc in #42128\r\n* Fix MaskFormer/Mask2Former fast image processors  by @yonigozlan in #41393\r\n* Remove unused functions in `image_transforms.py`  by @yaswanth19 in #42044\r\n* update deps table  by @ArthurZucker in #42120\r\n* fix: improve video processing fps assignment logic  by @Xqle in #42009\r\n* Fix T5Gemma module structure  by @Cyrilvallez in #42145\r\n* DataCollatorForLanguageModeling warning error fixed  by @mjaliz in #42144\r\n* Bugfix/remove emojis from print  by @7amim in #42091\r\n* Avoid mutating user-provided arguments in preprocessing utils  by @LeonardoEmili in #42126\r\n* Enforce check_auto_docstring  by @yonigozlan in #41635\r\n* Add dinov3 autobackbone  by @vijayabhaskar-ev in #41276\r\n* Fix logic error in `prepare_inputs_for_generation` cache slicing condition  by @albertvillanova in #41764\r\n* :rotating_light: Fix gradient checkpointing for several models and improve test robustness    by @githubnemo in #41818\r\n* [`T5Gemma`] Fix cross attention cache  by @vasqu in #41890\r\n* T5 migration to new masking interface  by @Aravind-11 in #41804\r\n* fix: improve visibility of ValueError root causes in model config loading  by @scottzh8 in #41972\r\n* add xpu to valid hardware for torch.compile  by @sywangyi in #42079\r\n* extend test_beam_search_early_stop_heuristic case to other device  by @sywangyi in #42078\r\n* fix failure of tests/models/shieldgemma2/test_modeling_shieldgemma2.p‚Ä¶  by @sywangyi in #42022\r\n* Fixes Flash Attention implementation for models   by @i3hz in #42149\r\n* fix test failure of speculative_generation on xpu  by @sywangyi in #42052\r\n* add rmsnorm kernels support for npu  by @zheliuyu in #42106\r\n* update torchao doc  by @jiqing-feng in #42139\r\n* feat(kernels): add opt-out flag to disable kernels hub usage through the lib  by @mfuntowicz in #41990\r\n* handle inputs from Siglip/Siglip2 non-automapped encoder layers  by @molbap in #41930\r\n* Add slow to some examples tests   by @SunMarc in #42164\r\n* fix(ci): unexpected keyword argument `streaming`  by @McPatate in #42102\r\n* pin `pytest<9` for now  by @ydshieh in #42162\r\n* Docs/i18n updates  by @lilin-1 in #42006\r\n* Fix in-place modification of user-input in SAM2 embed boxes  by @xenova in #42173\r\n* [`Pop2Piano`] Fix cache usage  by @vasqu in #42170\r\n* Fix helper fn for new processor config format  by @zucchini-nlp in #42085\r\n* Remove unnecessary slicing in sdpa_attention_forward  by @justinchuby in #41900\r\n* [`PEFT`] Fix prefix tuning  by @vasqu in #41696\r\n* [typo] fix mrope-interleave annotation to avoid ambiguity  by @JJJYmmm in #42177\r\n* Update transformers to support `FqnToConfig`  by @jcaip in #41894\r\n* [`PEFT`] Fix the general test for prefix tuning  by @vasqu in #42185\r\n* [TP] Fix parameter detection issue and some invalid TP-plans  by @Cyrilvallez in #42129\r\n* Refactor weight loading  by @ArthurZucker in #41580\r\n* üö® Delete deprecations with end-cycle in v4.xx and v5.0  by @zucchini-nlp in #41681\r\n* Add AutoTokenizer mapping for mistral3 and ministral  by @patrickvonplaten in #42198\r\n* Fix checkpoint loading with DeepSpeed ZeRO3  by @tohtana in #42201\r\n* [`Pop2Piano`] Fix tied weights  by @vasqu in #42193\r\n* New docker from AMD  by @remi-or in #42208\r\n* Add cross links for model contribution  by @zucchini-nlp in #42207\r\n* Stop inheriting tests!  by @Rocketknight1 in #42192\r\n* Refactor check_auto_docstring using AST  by @yonigozlan in #41432\r\n* [`BLT`] Fix cache usage  by @vasqu in #42188\r\n* Update `test_dynamic_cache_exportability_multiple_run` (failing on torch 2.10 nightly)  by @ydshieh in #42212\r\n* Much more efficient and clear weight initialization and tie weights  by @Cyrilvallez in #42191\r\n* GLM-V update with new processor  by @zRzRzRzRzRzRzR in #42122\r\n* Fix initialization guard for pytest  by @Cyrilvallez in #42234\r\n* Fix TP plans for MoE models  by @Cyrilvallez in #42236\r\n* Add prefix sharing to continuous batching  by @remi-or in #42094\r\n* Loading optimization  by @Cyrilvallez in #42239\r\n* calls `AttentionMaskConverter._unmask_unattended` for xpu device before  by @kaixuanliu in #42230\r\n* FIX Broken PEFT adapter loading  by @BenjaminBossan in #42187\r\n* Fix processor test for glm  by @molbap in #42233\r\n* Fix UnboundLocalError in RT-DETR loss computation  by @yashwantbezawada in #42224\r\n* Stop inheriting tests (again)  by @Rocketknight1 in #42247\r\n* [loading] Fix device when source and target are different  by @Cyrilvallez in #42246\r\n* Reduce timing on CircleCI - part 1 (Use @slow for IntegrationTests)  by @ydshieh in #42206\r\n* üö® Delete generation params from model config  by @zucchini-nlp in #41695\r\n* Allow VLMs to have a correct `base_model`  by @zucchini-nlp in #41589\r\n* Make tests run in less time by reducing `batch_size`  by @ydshieh in #42213\r\n* Revert \"Make tests run in less time by reducing `batch_size`\"  by @ydshieh in #42258\r\n* Cleanup reference to TFBertTokenizer and TFGPT2Tokenizer  by @Rocketknight1 in #42182\r\n* delete already deprecated models  by @ydshieh in #42235\r\n* Fix bnb for the weights refactor  by @SunMarc in #42043\r\n* Fix looping in torch guard decorator  by @Cyrilvallez in #42260\r\n* üö®  Generalize `get_decoder()` for multimodal and delete redundant code üî™   by @zucchini-nlp in #42156\r\n* Audio Flamingo3 - fix attention masking  by @zucchini-nlp in #42278\r\n* Add support for torch device objects in device validator  by @yonigozlan in #42267\r\n* Remove doc files of other langs for deleted models  by @ydshieh in #42276\r\n* [testing] fix `cwm`  by @ydshieh in #42261\r\n* fix a typo: pbd -> pdb  by @jaeminoh in #42268\r\n* Enable glm46v UTs on XPU  by @YangKai0616 in #42274\r\n* [testing] fix some cases in xpu  by @sywangyi in #42273\r\n* Remove random flag  by @Cyrilvallez in #42282\r\n* Fix accelerate integration  by @Cyrilvallez in #42264\r\n* Fix validation checks order in benchmark_v2  by @Abdennacer-Badaoui in #42280\r\n* Update torchcodec to match torchaudio version  by @remi-or in #42288\r\n* Use `torch.get_autocast_dtype` instead of `torch.get_autocast_gpu_dtype`  by @qgallouedec in #42055\r\n* perf: Optimization for Min-p sampling implementation  by @casinca in #42248\r\n* Fix device_map computation part 2  by @Cyrilvallez in #42290\r\n* Fixed the docstring for `WhisperFeatureExtractor`  by @TopCoder2K in #42286\r\n* avoiding conditional indexing in positionalencoding to avoid possibil‚Ä¶  by @ppadjinTT in #42090\r\n* ENH: Add support for LoRA hotswapping  by @BenjaminBossan in #41297\r\n* Fix Break change of AWQ FusedModules due to Attention Refactor  by @fanqiNO1 in #41909\r\n* Remove error string test that was failing  by @Rocketknight1 in #42301\r\n* Properly protect the is_compiling checks  by @Cyrilvallez in #42304\r\n* Remove outdated methods in modeling_utils.py  by @Cyrilvallez in #42302\r\n* Fix Mac mps dataloader_num_workers > 1 causes RuntimeError: _share_filename_: only available on CPU  by @AmitMY in #38819\r\n* Fix the init_weights for the MoE models  by @Cyrilvallez in #42306\r\n* Update link to generation strategies documentation  by @omkar-334 in #42252\r\n* Update conversion mapping to separate renaming from converting  by @ArthurZucker in #42254\r\n* fix(granitemoe*): Only create block_sparse_moe if num_local_experts > 0  by @gabe-l-hart in #42036\r\n* [SAM3 Video] Add support for multi prompts   by @yonigozlan in #42293\r\n* Add Pix2Struct fast image processor  by @yonigozlan in #42020\r\n* Fix post processing methods in  keypoints matching models  by @yonigozlan in #42018\r\n* fix tests/models/xcodec/test_modeling_xcodec.py::XcodecIntegrationTest  by @sywangyi in #42272\r\n* [loading] Fix device detection  by @Cyrilvallez in #42323\r\n* Fix typo from side_dict to size_dict  by @nihui in #42319\r\n* HF Trainer: ALST/Ulysses sequence parallelism integration via HF Accelerate  by @stas00 in #41832\r\n* Fix gpt2 modeling tests  by @Abdennacer-Badaoui in #42321\r\n* [loading] Use fewer threads by default for much better performances  by @Cyrilvallez in #42324\r\n* Allow LayoutLMV3Processor to accept rescale_factor  by @Rocketknight1 in #42305\r\n* Correctly create tied key mapping in post_init, and dynamic tie weight  by @Cyrilvallez in #42270\r\n* [`CI`] Skip `EfficientLoFTR` test  by @vasqu in #42327\r\n* [XPU] Add flash_attn2 support for XPU  by @YangKai0616 in #41956\r\n* [`Attn Masks`] Lift bidirectional mask restriction on eager  by @vasqu in #42325\r\n* fix bug when gemma3n model run on multiple device  by @kaixuanliu in #42303\r\n* Fix ChineseCLIPModel.get_text_features  by @JiangJQ2000 in #42351\r\n* Gemma3 hybrid fix  by @remi-or in #42287\r\n* fix(benchmarks): correct sdpa_backend inconsistency and attn_implementation for continuous batching  by @engmohamedsalah in #42339\r\n* Auto convert tekken.json  by @ArthurZucker in #42299\r\n* [loading] Re-add and improve disk offloading support  by @Cyrilvallez in #42242\r\n* Fix typo - indentation in JSON dump example  by @anthropikos in #42332\r\n* Fix tied weight for Bart (for BC)  by @Cyrilvallez in #42355\r\n* Fix reference to yelp dataset  by @JuanFKurucz in #42349\r\n* Fix documentation reference to pytorch max memory allocated  by @JuanFKurucz in #42350\r\n* Fix reference to imagenet 1k dataset  by @JuanFKurucz in #42348\r\n* Fix typos  by @omahs in #42354\r\n* Protect `torch.distributed` imports  by @Cyrilvallez in #42361\r\n* Expand npu device for KernelConfig  by @zheliuyu in #42358\r\n* Replace Optional and Union typing with | in some source files  by @cyyever in #42294\r\n* Fix code examples to load gpt 1 openai community model  by @JuanFKurucz in #42347\r\n* fix tekken pattern matching  by @ArthurZucker in #42363\r\n* Fixed-wrong-ZeRO3-json-snippet-found-in-deepspeed-markdown-file  by @Yacklin in #42346\r\n* Make benchmarking lighter: clean-up result files and remove non-needed arguments  by @remi-or in #42357\r\n* Add image processor fast vitpose  by @yonigozlan in #42021\r\n* Small tp fix  by @ArthurZucker in #42366\r\n* Remove test inheritance for EfficientLoftr, rename KeypointMatchingOutput to model specific name  by @yonigozlan in #42365\r\n* Tiny doc fix  by @molbap in #42296\r\n* Fix TimesFM patch normalization instability  by @AnMakc in #42099\r\n* [core] Fix torchao   by @MekkCyber in #42289\r\n* Fix tp  by @ArthurZucker in #42368\r\n* [`Attn Masks`] Add skip option for non-packed sequences  by @vasqu in #42367\r\n* üìö docs(granite-speech): add comprehensive usage examples  by @gorkachea in #42125\r\n* Xcodec fix  by @eustlb in #42095\r\n* Replace Optional and Union typing with | in some source files  by @cyyever in #42372\r\n* [`Mistral Tokenizers`] Fix tokenizer detection  by @vasqu in #42389\r\n* misc don't recreate it  by @ArthurZucker in #42394\r\n* [SAM3] Fix precompute vision_embeds or text_embeds for inference  by @yonigozlan in #42407\r\n* üö® Image-text pipeline expects correctly formatted chat  by @zucchini-nlp in #42359\r\n* Many small fixes for the CI  by @remi-or in #42364\r\n* [core] fix mxfp4  by @MekkCyber in #42382\r\n* fixed json syntax error for zero2 configuration file found in deepspeed.md  by @Yacklin in #42406\r\n* GLM4V - delete duplicate config attribute  by @zucchini-nlp in #42416\r\n* üö® Remove generic output_attentions warning  by @Aravind-11 in #42334\r\n* Bart config doesn't need generation parameters  by @zucchini-nlp in #42337\r\n* Simplify and standardize processor tests  by @yonigozlan in #41773\r\n* Clean bnb integration using weight converter  by @SunMarc in #42426\r\n* Any to any pipeline and auto-mapping  by @zucchini-nlp in #40884\r\n* Fix processor usage + add chat_template support to TTS pipeline, and shift common chat template logic to base class.  by @ebezzam in #42326\r\n* [fp8] fix scales param name  by @MekkCyber in #42434\r\n* Fix an edge case for `get_encoder()`  by @zucchini-nlp in #42295\r\n* Disable loss rounding in training stats log  by @AnMakc in #42104\r\n* Benchmark simplification  by @remi-or in #42408\r\n* Future annotations break FastAPI  by @LysandreJik in #42450\r\n* [cleanup] Don't use Repository in create_dummy_models.py script  by @Wauplin in #42380\r\n* [cleanup] Remove deprecated load config from file  by @Wauplin in #42383\r\n* [`FA`] Cleanup loading logic  by @vasqu in #41427\r\n* tiny fix for deepseekocr support [vllm]  by @molbap in #42423\r\n* fix: Restore explicit .keys() calls for TensorDict compatibility  by @pankajbaid567 in #42373\r\n* Transformers serve -> list all generative models from the cache   by @LysandreJik in #42146\r\n* üö® [v5][PEFT] Bump min version requirement of PEFT to  0.18.0  by @BenjaminBossan in #41889\r\n* [cleanup] Offline mode and cache dir from `huggingface_hub` constants + cleanup in `PushToHubMixin`  by @Wauplin in #42391\r\n* Correctly return finish reason length when finished  by @LysandreJik in #42157\r\n* FIX: Minimal fix for loading PEFT weights  by @BenjaminBossan in #42387\r\n* Let's break Qwen-VL üö®    by @zucchini-nlp in #42420\r\n* [`CI`] Add to run slow  by @vasqu in #42459\r\n* Fix the \"test_offline\" test  by @LysandreJik in #42458\r\n* `transformers chat` launched without base_url has a direct tie to localhost:8000  by @LysandreJik in #42463\r\n* update with more recent tts models  by @Deep-unlearning in #42328\r\n* rm slow tokenizers  by @itazap in #40936\r\n* [loading/saving] Reverse all loading operations when saving  by @Cyrilvallez in #42396\r\n* Fix T5 tests: use generation_config for generation parameters  by @Abdennacer-Badaoui in #42419\r\n* remove reference to TF models from docs  by @zucchini-nlp in #42443\r\n* [Trainer] use output.loss when using liger-kernel  by @kashif in #42444\r\n* replace source_keys and target_keys  by @SunMarc in #42471\r\n* Update migration guide - generation config  by @zucchini-nlp in #42470\r\n* üö® Move `rotary_partial_emb` to RopeParams and delete unnecessary code üî™   by @zucchini-nlp in #42255\r\n* Fix doc builds  by @Rocketknight1 in #42478\r\n* extend CwmIntegrationTest to xpu  by @sywangyi in #42314\r\n* add require_deterministic_for_xpu to make the case pass in xpu  by @sywangyi in #42439\r\n* Skip failing irrelevant test for ColQwen2  by @Rocketknight1 in #42480\r\n* [quantization] make torchao tests slow  by @MekkCyber in #42482\r\n* Fix gpt2 tokenizer `add_prefix_space` default value   by @SunMarc in #42481\r\n\r\n## Significant community contributions\r\n\r\nThe following contributors have made significant changes to the library over the last release:\r\n\r\n* @ArthurZucker\r\n    * `JetMoe` Fix jetmoe after #40132 (#41324)\r\n    * [`ModularChecker`] QOL for the modular checker (#41361)\r\n    * [`CB`] Refactors the way we access paged (#41370)\r\n    * Update from pretrained error when loading (#33380)\r\n    * :facepalm: CB nit!  (#41413)\r\n    * [`from_pretrained`] Small refactor `from_pretrained`: move around unrelated stuff (#41445)\r\n    * update deps table (#42120)\r\n    * Refactor weight loading (#41580)\r\n    * Update conversion mapping to separate renaming from converting (#42254)\r\n    * Auto convert tekken.json (#42299)\r\n    * fix tekken pattern matching (#42363)\r\n    * Small tp fix (#42366)\r\n    * Fix tp (#42368)\r\n    * misc don't recreate it (#42394)\r\n* @vasqu\r\n    * :rotating_light: [`v5`] Remove relative position embeddings (for bert like models) (#41170)\r\n    * [`v5`] Sync Bert and Bart eager attention (#41248)\r\n    * [`JetMoe`] Fix KV head repetition and padding free (#41423)\r\n    * :rotating_light: [`Attention Masks`] Bidirectional masks for encoder and encoder-decoder models (#41265)\r\n    * [`CI`] Fix copies on main (#41486)\r\n    * [`Docs`] Fix changed references (#41614)\r\n    * [`Executorch`] Simplify for encoder models (#41627)\r\n    * [`Ernie 4.5 Moe`] Fix Moe and offloading (#41385)\r\n    * [`Masks`] Fix mask handling in eager for vision models (#41625)\r\n    * [`Attn`] Allow dynamic causality in SDPA via Kwargs (#41692)\r\n    * [`Onnx docs`] Remove some traces (#41791)\r\n    * :rotating_light: [`Clip`] Fix masking and enable flash attention on all model types (#41750)\r\n    * [`Attn Masks`] Non-vmap default for attention masks (#41852)\r\n    * [`T5Gemma`] Fix cross attention cache (#41890)\r\n    * [`Pop2Piano`] Fix cache usage (#42170)\r\n    * [`PEFT`] Fix prefix tuning (#41696)\r\n    * [`PEFT`] Fix the general test for prefix tuning (#42185)\r\n    * [`Pop2Piano`] Fix tied weights (#42193)\r\n    * [`BLT`] Fix cache usage (#42188)\r\n    * [`CI`] Skip `EfficientLoFTR` test (#42327)\r\n    * [`Attn Masks`] Lift bidirectional mask restriction on eager (#42325)\r\n    * [`Attn Masks`] Add skip option for non-packed sequences (#42367)\r\n    * [`Mistral Tokenizers`] Fix tokenizer detection (#42389)\r\n    * [`FA`] Cleanup loading logic (#41427)\r\n    * [`CI`] Add to run slow (#42459)\r\n* @ydshieh\r\n    * [testing] update `test_longcat_generation_cpu` (#41368)\r\n    * [testing] Fix `JetMoeIntegrationTest` (#41377)\r\n    * Pickle - part 2 (#41476)\r\n    * Try to remove `pickle` - `BloomTokenizerFast` (#41466)\r\n    * [testing] reduce runtime of `HunYuanMoEV1IntegrationTest:test_model_generation` (#41373)\r\n    * delete some tokenizer tests using pickle (#41514)\r\n    * torch 2.9 don't ‚ù§Ô∏è torchcodec üíî  (#41610)\r\n    * Update a dataset reop link (#41618)\r\n    * Remove the head masking block in some vision models (#41620)\r\n    * improve `utils/check_bad_commit.py` (#41658)\r\n    * torch 2.9 still don't ‚ù§Ô∏è torchcodec 0.8 üíî (#41686)\r\n    * path validation for security reason (#41256)\r\n    * pin torchcodec on CI docker image (#41703)\r\n    * further improve `utils/check_bad_commit.py` (#41658) (#41690)\r\n    * Revert \"Remove upper version bound of pandas\" (#41744)\r\n    * Fix bark after #41445 (#41645)\r\n    * flash attn pytest marker (#41781)\r\n    * unpin torch/torchcodec for CircleCI (#41839)\r\n    * further reducing flakiness in `utils/check_bad_commit.py` (#41658)  (#41815)\r\n    * CI workflow for Flash Attn (#41857)\r\n    * Update some workflow files (#41892)\r\n    * Minor fix in docker image build workflow (#41949)\r\n    * Run slow v2 (#41914)\r\n    * Fix `detectron2` installation in docker files (#41975)\r\n    * Fix `autoawq[kernels]` installation in quantization docker file (#41978)\r\n    * Fix `torchcodec` version in quantization docker file (#41988)\r\n    * Fix `run slow v2`: empty report when there is only one model (#42002)\r\n    * Fix `torch+deepspeed` docker file (#41985)\r\n    * fix `deeepspeed` in AMD docker file (#42025)\r\n    * Change trigger time for AMD CI (#42034)\r\n    * Remove some custom datasets defined in codebase (#41511)\r\n    * Cleanup workflow - part 1 (#42023)\r\n    * Fix `pr_slow_ci_suggestion.yml` after #42023 (#42049)\r\n    * Avoid explicit checkout in workflow (#42057)\r\n    * Be careful at explicit checkout actions (#42060)\r\n    * Fix another `Argument list too long` in `pr_slow_ci_suggestion.yml` (#42061)\r\n    * Revert back to use GitHub context  (#42066)\r\n    * Fix inconsistency of commit sha during the workflow run (#42074)\r\n    * Revert \"permissions worflows fix\" (#42110)\r\n    * pin `pytest<9` for now (#42162)\r\n    * Update `test_dynamic_cache_exportability_multiple_run` (failing on torch 2.10 nightly) (#42212)\r\n    * Reduce timing on CircleCI - part 1 (Use @slow for IntegrationTests) (#42206)\r\n    * Make tests run in less time by reducing `batch_size` (#42213)\r\n    * Revert \"Make tests run in less time by reducing `batch_size`\" (#42258)\r\n    * delete already deprecated models (#42235)\r\n    * Remove doc files of other langs for deleted models (#42276)\r\n    * [testing] fix `cwm` (#42261)\r\n* @cyyever\r\n    * Remove unnecessary list comprehension (#41305)\r\n    * Remove unused function patameters (#41358)\r\n    * Use accelerator API to free device memory (#41195)\r\n    * Remove Python 3.9 classifier (#41410)\r\n    * Remove KERAS_NLP_IMPORT_ERROR (#41468)\r\n    * Import Callable from collections.abc (#41130)\r\n    * Remove infer_device (#41088)\r\n    * Fix Latex typesetting in documentation (#41177)\r\n    * Fix typsetting and content of llm_tutorial_optimization.md (#41172)\r\n    * More markdown file fixes (#41599)\r\n    * Format MarkDown documentation and tiny fixes (#41638)\r\n    * Fix typos in documentation (#41641)\r\n    * Fix confusing cls assignment (#41642)\r\n    * Use | for Optional and Union typing (#41646)\r\n    * Remove  require_torch_bf16_gpu (#40979)\r\n    * Fix MarkDown syntax (#41676)\r\n    * Use | for Optional and Union typing  (#41675)\r\n    * Enable faiss-cpu on Windows (#41678)\r\n    * Fix Pylint warnings (#41644)\r\n    * Enable  FURB rules in ruff (#41395)\r\n    * Remove upper version bound of pandas (#41677)\r\n    * Fix documentation issues (#41726)\r\n    * Apply RUFF PIE rules (#41727)\r\n    * Replace Optional and Union typing with | in some source files (#42294)\r\n    * Replace Optional and Union typing with | in some source files (#42372)\r\n* @yao-matrix\r\n    * make some ut cases pass on xpu w/ latest torch (#41337)\r\n    * fix asr ut failures (#41332)\r\n    * enable new model uts to xpu and fix some failures on xpu (#41386)\r\n    * enable some falcon-mamba uts on xpu (#41428)\r\n    * enhance patched_tearDown to support python 3.11+ (#41429)\r\n    * fix gemma3n case failure (#41426)\r\n    * upgrade xpu docker file to torch 2.8 (#41551)\r\n    * make apollo test case pass (#41805)\r\n    * extend bitnet cases to xpu, all 8 cases pass (#41831)\r\n    * extend 2 trainer test cases to xpu (#41829)\r\n    * extend 2 blip2 and falcon_h1 test cases to xpu (#41825)\r\n    * make lfm2_moe integration test pass on XPU (#41796)\r\n    * fix some ut failures on XPU w/ torch 2.9 (#41923)\r\n    * fix some ut failures on XPU w/ torch 2.9 (#41941)\r\n    * fix prepare_config_and_inputs_for_common bug in llava test (#41942)\r\n    * make recurrent_gemma and voxtral cases pass on xpu (#41958)\r\n    * extend fp_quant cases to xpu (#41833)\r\n    * fix tensor device placement issue of 2 UT cases (#41921)\r\n    * fix continuous batching issues, extend ut cases to xpu (#41830)\r\n* @MekkCyber\r\n    * [kernels] Kernel Config  (#41232)\r\n    * Fixing comments in __init__ file (#41414)\r\n    * [kernels] Cleanup deta kernel (#41470)\r\n    * Cleaning hub kernels  (#41477)\r\n    * Remove DISABLE_KERNEL_MAPPING flag (#41475)\r\n    * [kernels] Remove RWKV kernel finally ! (#41493)\r\n    * [kernels] rm yoso kernel (#41495)\r\n    * [kernels] rm mra kernels (#41507)\r\n    * Revert \"add rmsnorm kernels support for Intel XPU\" (#41579)\r\n    * [kernels] refactor function kernel calling (#41577)\r\n    * Erroring when KernelConfig is passed without use_kernels = True (#41657)\r\n    * Small Fix for imports  (#41411)\r\n    * [kernels] Add version to function mapping (#41685)\r\n    * [quantization] fix compressed_tensors tests (#41780)\r\n    * [quantization] Skip Fp8 tests when hardware capability < 8.9 (#41785)\r\n    * [quantization] fix torchao tests after 0.14.0 release (#41777)\r\n    * revert changes in _is_package_available (#41891)\r\n    * [kernels] Add Tests & CI for kernels (#41765)\r\n    * [kernels] change import time in KernelConfig (#42004)\r\n    * [kernels] Fix XPU layernorm kernel (#41583)\r\n    * [core] Fix torchao  (#42289)\r\n    * [core] fix mxfp4 (#42382)\r\n    * [fp8] fix scales param name (#42434)\r\n    * [quantization] make torchao tests slow (#42482)\r\n* @paulpak58\r\n    * [Cache] lfm2 cache: allocate empty kv layers during init (#41396)\r\n    * [Model] Lfm2Moe (#41401)\r\n* @gante\r\n    * üö® [v5] Prune `prune_heads` (#41417)\r\n    * [v5] rm `utils/tf_ops/` (#41402)\r\n    * [causallm tester] automate pipeline mappings + bloom tests (#41318)\r\n    * üö® [v5] `generate` delegates default cache initialization to the model (#41505)\r\n* @zRzRzRzRzRzRzR\r\n    * Update GLM-4.1V MMRope implementation (#41182)\r\n    * Update GLM-4.6 doc (#41471)\r\n    * Add aux loss for GLM-4.5V (#41564)\r\n    * 4.1V Model and GLM-4.5V Model Conversion Code Updates (#41784)\r\n    * GLM-V update with new processor (#42122)\r\n* @jacobkahn\r\n    * Add Code World Model (CWM) (#41199)\r\n* @molbap\r\n    * Update philosophy (#41438)\r\n    * [QoL] modular conversion shows LoC saved (#41500)\r\n    * Double router compute? (#41653)\r\n    * Add vision contribution guide (#41456)\r\n    * Modernize CLIP modeling code  (#41546)\r\n    * handle inputs from Siglip/Siglip2 non-automapped encoder layers (#41930)\r\n    * Fix processor test for glm (#42233)\r\n    * Tiny doc fix (#42296)\r\n    * tiny fix for deepseekocr support [vllm] (#42423)\r\n* @Wauplin\r\n    * Bump to hfh 1.0.0.rc5 to fix test (#41508)\r\n    * Migrate transformers cli to Typer (#41487)\r\n    * Remove deprecated `use_auth_token` parameter (#41666)\r\n    * added more breaking changes\r\n    * [cleanup] Don't use Repository in create_dummy_models.py script (#42380)\r\n    * [cleanup] Remove deprecated load config from file (#42383)\r\n    * [cleanup] Offline mode and cache dir from `huggingface_hub` constants + cleanup in `PushToHubMixin` (#42391)\r\n* @remi-or\r\n    * Restore cuda graphs to continuous batching (#41421)\r\n    * Fix an import error with PreTrainModel (#41571)\r\n    * Add __iter__ to DynamicCache (#41569)\r\n    * Gemma3 fixes (#41572)\r\n    * Benchmark overhaul (#41408)\r\n    * Fix fp32_ln for various models (#41605)\r\n    * Fix EncoderDecoder cache (#41612)\r\n    * Switch to CB if cache_implementation == paged (#41655)\r\n    * Small changes to benchmarking script (#41662)\r\n    * Bump AMD docker (#41792)\r\n    * Add a safeguard around a flaky test in gemma2 (#41811)\r\n    * Use indices as position_ids in modernebert (#41789)\r\n    * Move the Mi355 to regular docker (#41989)\r\n    * More data in benchmarking (#41848)\r\n    * Reduce the number of benchmark in the CI (#42008)\r\n    * New docker from AMD (#42208)\r\n    * Add prefix sharing to continuous batching (#42094)\r\n    * Update torchcodec to match torchaudio version (#42288)\r\n    * Gemma3 hybrid fix (#42287)\r\n    * Make benchmarking lighter: clean-up result files and remove non-needed arguments (#42357)\r\n    * Many small fixes for the CI (#42364)\r\n    * Benchmark simplification (#42408)\r\n* @lkhl\r\n    * [model] Add VideoLLaMA3 implementation (#40499)\r\n* @philiproeleveld\r\n    * Add `logits_to_keep` to many older CausalLM models (#41335)\r\n* @AlphaOrOmega\r\n    * Adding superglue fast image processing (#41394)\r\n* @echarlaix\r\n    * [v5] Remove deprecated tranformers.onnx (#41700)\r\n* @Aravind-11\r\n    * Add GLPNImageProcessorFast  (#41725)\r\n    * T5 migration to new masking interface (#41804)\r\n    * üö® Remove generic output_attentions warning (#42334)\r\n* @DeXtAr47-oss\r\n    * add fuyu fast image processors (#41817)\r\n* @lashahub\r\n    * [models] Add AudioFlamingo3 integration (#40290)\r\n* @lilin-1\r\n    * Docs/i18n updates (#42006)\r\n* @burtenshaw\r\n    * [MODEL] Nanochat implementation (#41634)\r\n* @itazap\r\n    * rm slow tokenizers (#40936)\r\n",
      "publishedAt": "2025-12-01T18:14:54.000Z",
      "source": "Hugging Face GitHub",
      "sourceUrl": "https://github.com/huggingface/transformers/releases/tag/v5.0.0rc0",
      "category": "tools",
      "company": "Hugging Face",
      "imageUrl": null,
      "tags": [
        "GPT",
        "Llama",
        "LLM",
        "AI",
        "ML"
      ],
      "featured": false
    },
    {
      "id": "miy7baxbjusvw324469",
      "title": "Google: cookbook„ÅÆÊúÄÊñ∞„Ç¢„ÉÉ„Éó„Éá„Éº„Éà",
      "summary": "cookbook„É™„Éù„Ç∏„Éà„É™„Å´Êñ∞„Åó„ÅÑÊõ¥Êñ∞: grammar: fix typo in multi_spectral_remote_sensing.ipynb (#1033)...",
      "content": "grammar: fix typo in multi_spectral_remote_sensing.ipynb (#1033)",
      "publishedAt": "2025-12-01T11:03:27.000Z",
      "source": "Google GitHub",
      "sourceUrl": "https://github.com/google-gemini/cookbook",
      "category": "tools",
      "company": "Google",
      "imageUrl": null,
      "tags": [],
      "featured": false
    },
    {
      "id": "miy7bdx8xoj4wpcev4",
      "title": "Hugging Face: Patch release v4.57.3",
      "summary": "There was a hidden bug when loading models with `local_files_only=True` and a typo related to the recent patch. \r\n\r\nThe main fix is: https://github.com/huggingface/transformers/commit/b6055550a15a8fab367cf983b743ff68cc58d81a.\r\n\r\nWe are really sorry that this slipped through, our CIs just did not cat...",
      "content": "There was a hidden bug when loading models with `local_files_only=True` and a typo related to the recent patch. \r\n\r\nThe main fix is: https://github.com/huggingface/transformers/commit/b6055550a15a8fab367cf983b743ff68cc58d81a.\r\n\r\nWe are really sorry that this slipped through, our CIs just did not catch it.\r\n\r\nAs it affects a lot of users we are gonna yank the previous release",
      "publishedAt": "2025-11-25T15:51:36.000Z",
      "source": "Hugging Face GitHub",
      "sourceUrl": "https://github.com/huggingface/transformers/releases/tag/v4.57.3",
      "category": "tools",
      "company": "Hugging Face",
      "imageUrl": null,
      "tags": [
        "AI",
        "Transformer"
      ],
      "featured": false
    },
    {
      "id": "miy7b5n2thkrq0porag",
      "title": "Anthropic: v0.75.0",
      "summary": "## 0.75.0 (2025-11-24)\n\nFull Changelog: [v0.74.1...v0.75.0](https://github.com/anthropics/anthropic-sdk-python/compare/v0.74.1...v0.75.0)\n\n### Features\n\n* **api:** adds support for Claude Opus 4.5, Effort, Advance Tool Use Features, Autocompaction, and Computer Use v5 ([5c3e633](https://github.com/a...",
      "content": "## 0.75.0 (2025-11-24)\n\nFull Changelog: [v0.74.1...v0.75.0](https://github.com/anthropics/anthropic-sdk-python/compare/v0.74.1...v0.75.0)\n\n### Features\n\n* **api:** adds support for Claude Opus 4.5, Effort, Advance Tool Use Features, Autocompaction, and Computer Use v5 ([5c3e633](https://github.com/anthropics/anthropic-sdk-python/commit/5c3e633395acc100c45e8a403f1b5be4d17a3b32))\n\n\n### Bug Fixes\n\n* **internal:** small fixes ([36c82f7](https://github.com/anthropics/anthropic-sdk-python/commit/36c82f72b588bd549770a89072dd52d6f9e5b6a5))\n\n\n### Chores\n\n* fix lint issues ([4f1fd54](https://github.com/anthropics/anthropic-sdk-python/commit/4f1fd54143b79a4a7976dfb332ec7bfc666d1598))",
      "publishedAt": "2025-11-24T20:41:18.000Z",
      "source": "Anthropic GitHub",
      "sourceUrl": "https://github.com/anthropics/anthropic-sdk-python/releases/tag/v0.75.0",
      "category": "tools",
      "company": "Anthropic",
      "imageUrl": null,
      "tags": [
        "Claude",
        "Anthropic"
      ],
      "featured": false
    },
    {
      "id": "miy7b5n24mreer9xy6u",
      "title": "Anthropic: v0.74.1",
      "summary": "## 0.74.1 (2025-11-19)\n\nFull Changelog: [v0.74.0...v0.74.1](https://github.com/anthropics/anthropic-sdk-python/compare/v0.74.0...v0.74.1)\n\n### Bug Fixes\n\n* **structured outputs:** use correct beta header ([e90d347](https://github.com/anthropics/anthropic-sdk-python/commit/e90d347dfd80adc5ae2a412ebfe...",
      "content": "## 0.74.1 (2025-11-19)\n\nFull Changelog: [v0.74.0...v0.74.1](https://github.com/anthropics/anthropic-sdk-python/compare/v0.74.0...v0.74.1)\n\n### Bug Fixes\n\n* **structured outputs:** use correct beta header ([e90d347](https://github.com/anthropics/anthropic-sdk-python/commit/e90d347dfd80adc5ae2a412ebfe2eb55351ce08e))\n\n\n### Chores\n\n* **examples:** update model references ([e09461d](https://github.com/anthropics/anthropic-sdk-python/commit/e09461da36405bbb1a7ef616b9281457b2a6e20f))",
      "publishedAt": "2025-11-19T22:17:03.000Z",
      "source": "Anthropic GitHub",
      "sourceUrl": "https://github.com/anthropics/anthropic-sdk-python/releases/tag/v0.74.1",
      "category": "tools",
      "company": "Anthropic",
      "imageUrl": null,
      "tags": [
        "Anthropic"
      ],
      "featured": false
    },
    {
      "id": "miy7bnou2hjr7tf9l95",
      "title": "Show HN: I built Solveig, it turns any LLM into an assistant in your terminal",
      "summary": "Solveig (<a href=\"https:&#x2F;&#x2F;github.com&#x2F;FSilveiraa&#x2F;solveig\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;FSilveiraa&#x2F;solveig</a>) can plan tasks, read files, list directory trees, edit your code, run commands and more.<p>Watch 45s demo: <a href=\"https:&#x2F;&#x2F;asciinema.o...",
      "content": "Solveig (<a href=\"https:&#x2F;&#x2F;github.com&#x2F;FSilveiraa&#x2F;solveig\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;FSilveiraa&#x2F;solveig</a>) can plan tasks, read files, list directory trees, edit your code, run commands and more.<p>Watch 45s demo: <a href=\"https:&#x2F;&#x2F;asciinema.org&#x2F;a&#x2F;p5mzDGAoHTUHNEaVeROHpFibx\" rel=\"nofollow\">https:&#x2F;&#x2F;asciinema.org&#x2F;a&#x2F;p5mzDGAoHTUHNEaVeROHpFibx</a><p>---<p>QUICK START<p><pre><code>  # Install\n  pip install solveig\n\n  # Run from local models or remote APIs\n  solveig -u &quot;http:&#x2F;&#x2F;localhost:5001&#x2F;v1&quot; &quot;Create a demo BlackSheep webapp&quot;\n\n  # Mix config files and CLI args\n  solveig -c solveig.config -k &quot;&lt;API_KEY&gt;&quot; -m &quot;gpt-5&quot;\n</code></pre>\nSee Usage for more: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;FSilveiraa&#x2F;solveig&#x2F;blob&#x2F;main&#x2F;docs&#x2F;usage.md\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;FSilveiraa&#x2F;solveig&#x2F;blob&#x2F;main&#x2F;docs&#x2F;usage.m...</a><p>---<p>FEATURES<p>AI Terminal Assistant - Automate task planning, file management, code analysis and system management using natural language in your terminal.<p>Safe by Design - Granular controls with pattern-based permissions. File operations prioritized, and shell commands can be disabled.<p>Plugin Architecture - Extend capabilities through drop-in plugins. Add SQL queries, web scraping or block dangerous commands with 100 lines of Python.<p>Modern CLI - Clear interface with task planning and listing, file content previews, diff editing, API usage tracking, code linting, waiting animations and rich tree displays for informed user decisions.<p>Provider Independence - Works with any OpenAI-compatible API, including local models.<p>tl;dr: similar idea to Claude Code (<a href=\"https:&#x2F;&#x2F;claude.com&#x2F;product&#x2F;claude-code\" rel=\"nofollow\">https:&#x2F;&#x2F;claude.com&#x2F;product&#x2F;claude-code</a>) or Aider (<a href=\"https:&#x2F;&#x2F;aider.chat&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;aider.chat&#x2F;</a>), focusing on providing explicit user consent, granular configuration, drop-in plugins and the ability to integrate any model, backend or API.<p>See the Features for more: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;FSilveiraa&#x2F;solveig&#x2F;blob&#x2F;main&#x2F;docs&#x2F;about.md#features-and-principles\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;FSilveiraa&#x2F;solveig&#x2F;blob&#x2F;main&#x2F;docs&#x2F;about.m...</a><p>---<p>TYPICAL TASKS<p>- &quot;Find and list all the duplicate files inside ~&#x2F;Documents&#x2F;&quot;\n- &quot;Check my essay Final.docx for spelling, syntax or factual errors while maintaining the tone&quot;\n- &quot;Refactor my test_database.ts suite to be more concise&quot;\n- &quot;Try and find out why my computer is slow&quot;\n- &quot;Create a dockerized BlackSheep webapp with a test suite, then build the image and run it locally&quot;<p>---<p>So it&#x27;s a coding assistant?<p>You can use Solveig for analyzing, editing and testing your code, and all of these scenarios have received significant support through development features like code linting. But I didn&#x27;t build Solveig with a single kind of use case in mind.<p>---<p>So it&#x27;s yet another LLM-in-my-terminal?<p>Sort of. Solveig tries to do a few things that other tools don&#x27;t, and to do the shared features with clearer UX, explicit consent, and deeper configuration. It&#x27;s not an IDE extension, doesn&#x27;t require a GUI, and it&#x27;s not built for a specific user type or scenario.<p>At the same time, Solveig&#x27;s competitors are mature projects with real user testing that you should check out. I&#x27;ve written a detailed comparison (<a href=\"https:&#x2F;&#x2F;github.com&#x2F;FSilveiraa&#x2F;solveig&#x2F;blob&#x2F;main&#x2F;docs&#x2F;comparison.md\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;FSilveiraa&#x2F;solveig&#x2F;blob&#x2F;main&#x2F;docs&#x2F;compari...</a>) to similar tools in the market in the docs.<p>---<p>UPCOMING<p>I have a Roadmap (<a href=\"https:&#x2F;&#x2F;github.com&#x2F;FSilveiraa&#x2F;solveig&#x2F;discussions&#x2F;2\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;FSilveiraa&#x2F;solveig&#x2F;discussions&#x2F;2</a>) available, and feel free to suggest new features or improvements. I&#x27;ve recently added user-defined system prompt templates, and now I&#x27;m working on adding token counting from API messages instead of relying on encoders.<p>---<p>A cool aspect of this project is that I can use Solveig to analyze and improve its own code itself, which also gives me a lot of exposure to its actual usability.<p>I appreciate any feedback or comment, especially anyone who can try out Solveig using Anthropic or Gemini APIs. Tell me if it helped you do something or what stopped you from using it properly in your specific case. Even if you can&#x27;t see how Solveig could help you let me know, that&#x27;s an issue with me communicating value that I need to fix.<p>Leaving a star on the repository (<a href=\"https:&#x2F;&#x2F;github.com&#x2F;FSilveiraa&#x2F;solveig\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;FSilveiraa&#x2F;solveig</a>) is also very much appreciated.",
      "publishedAt": "2025-11-13T18:29:28.000Z",
      "source": "Hacker News Claude/Anthropic",
      "sourceUrl": "https://github.com/FSilveiraa/solveig",
      "category": "companies",
      "company": "Anthropic",
      "imageUrl": null,
      "tags": [
        "GPT",
        "Claude",
        "Gemini",
        "LLM",
        "AI"
      ],
      "featured": false
    },
    {
      "id": "miy7bnou78dav01l3tj",
      "title": "Show HN: MCP Agent Mail, Like Gmail for Coding Agents",
      "summary": "I finally got around to making a tool I&#x27;ve wanted for a long time: you can basically think of it as being &quot;like Gmail for coding agents.&quot;<p>If you&#x27;ve ever tried to use a bunch of instances of Claude Code or Codex at once across the same project, you&#x27;ve probably noticed how a...",
      "content": "I finally got around to making a tool I&#x27;ve wanted for a long time: you can basically think of it as being &quot;like Gmail for coding agents.&quot;<p>If you&#x27;ve ever tried to use a bunch of instances of Claude Code or Codex at once across the same project, you&#x27;ve probably noticed how annoying it can be when they freak out about the other agent changing the files they&#x27;re working on.<p>Then they start doing annoying things, like restoring files from git, in the process wiping out another agent&#x27;s work without a backup.<p>Or if you&#x27;ve tried to have agents coordinate on two separate repos, like a Python backend and a Nextjs frontend for the same project, you may have found yourself acting as the go-between and liaison between two or three different agents, passing messages between them or having them communicate by means of markdown files or some other workaround.<p>I always knew there had to be a better way. But it&#x27;s hard to get the big providers to offer something like that in a way that&#x27;s universal, because Anthropic doesn&#x27;t want to integrate with OpenAI&#x27;s competitive coding tool, and neither wants to deal with Cursor or Gemini-CLI.<p>So a few days ago, I started working on it, and it&#x27;s now ready to share with the world. Introducing the 100% open-source MCP Agent Mail tool. This can be set up very quickly and easily on your machine and automatically detects all the most common coding agents and configures everything for you.<p>I also include a ready-made blurb (see the README file in the repo) that you can add to your existing AGENTS dot md or CLAUDE dot md file to help the agents better leverage the system straight out of the gate.<p>It&#x27;s almost comical how quickly the agents take to this system like a fish to water. They seem to relish in it, sending very detailed messages to each other just like humans do, and start coordinating in a natural, powerful way. They even give each other good ideas and pushback on bad ideas.<p>They can also reserve access to certain files to avoid the &quot;too many cooks&quot; problems associated with having too many agents all working on the same project at the same time, all without dealing with git worktrees and &quot;merge hell.&quot;<p>This also introduces a natural and powerful way to do something I&#x27;ve also long wanted, which is to automatically have multiple different frontier models working together in a collaborative, complementary way without me needing to be in the middle coordinating everything like a parent setting up playdates for their kids.<p>And for the human in the loop, I made a really slick web frontend that you can view and see all the messages your agents are sending each other in a nice, Gmail-like interface, so you can monitor the process. You can even send a special message to some or all your agents as the &quot;Human Overseer&quot; to give them a directive (of course, you can also just type that in manually into each coding agent, too.)<p>I made this for myself and know that I&#x27;m going to be getting a ton of usage out of it going forward. It really lets you unleash a massive number of agents using a bunch of different tools&#x2F;models, and they just naturally coordinate and work with each other without stepping on each other&#x27;s toes. It lets you as the human overseer relax a bit more as you no longer have to be the one responsible for coordinating things, and also because the agents watch each other and push back when they see mistakes and errors happening. Obviously, the greater the variety of models and agent tools you use, the more valuable that emergent peer review process will be.<p>Anyway, give it a try and let me know what you think. I&#x27;m sure there are a bunch of bugs that I&#x27;ll have to iron out over the next couple days, but I&#x27;ve already been productively using it today to work on another project and it is pretty amazingly functional already!",
      "publishedAt": "2025-10-27T12:46:13.000Z",
      "source": "Hacker News Claude/Anthropic",
      "sourceUrl": "https://github.com/Dicklesworthstone/mcp_agent_mail",
      "category": "companies",
      "company": "Anthropic",
      "imageUrl": null,
      "tags": [
        "Claude",
        "Gemini",
        "AI",
        "OpenAI",
        "Anthropic"
      ],
      "featured": false
    },
    {
      "id": "miy7blqlml55zk6cblj",
      "title": "Show HN: Pantheon-CLI ‚Äì Open-Source Python Claude Code and Smart Notebook",
      "summary": "Hi, We‚Äôve built Pantheon-CLI, an fully open-source project that aims to be the ‚ÄúPython Claude Code + Notebook‚Äù ‚Äî but designed for data analysis instead of just coding.<p>Unlike most AI coding assistants, Pantheon-CLI runs entirely on your machine (or server). No data upload required. It blends natur...",
      "content": "Hi, We‚Äôve built Pantheon-CLI, an fully open-source project that aims to be the ‚ÄúPython Claude Code + Notebook‚Äù ‚Äî but designed for data analysis instead of just coding.<p>Unlike most AI coding assistants, Pantheon-CLI runs entirely on your machine (or server). No data upload required. It blends natural language and code in a single workflow, keeping variables in memory and letting you switch seamlessly between typing code and asking in plain English.<p>What it does:\n1. Chat with your data: Directly process CSV, Excel, AnnData, Pickle, Torch tensors, or any format supported by Python&#x2F;R&#x2F;Julia.\n2. Mixed programming: Variables persist across natural language and code; the CLI auto-generates and runs code for you.\n3. MCP-like agent integration: Read&#x2F;create files, run commands, fetch web pages, generate&#x2F;revise code.\n4. Human-like learning: Feed it a PDF paper or tutorial‚ÄîPantheon-CLI reads it, plans steps, and replicates methods before analysis.\n5. Task planning: Builds scientific agents by learning from papers&#x2F;tutorials (not just fixed, human-predefined steps).\n6. Multi-model support: Works with OpenAI, Anthropic, Gemini, DeepSeek, Qwen, etc. + offline local LLMs (ollama, deepseek, gpt-oss).\n7. Multi-RAG support: Pre-learns from docs&#x2F;web into a local ‚Äúbrain‚Äù for more credible outputs without massive token costs.\n8. Built-in biology toolsets: For omics analysis (alignment, annotation, differential expression, full paper reproduction).\n9. Notebook mode: Brings the same agentic workflow into Jupyter‚Äîautomatically runs and revises code, operates on files, and learns from tutorials&#x2F;papers.<p>Pantheon-CLI is our attempt to push beyond ‚ÄúAI writes code for you.‚Äù Instead, it‚Äôs an agentic operating system for data analysis, spanning both terminal and notebook.<p>Repo: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;aristoteleo&#x2F;pantheon-cli\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;aristoteleo&#x2F;pantheon-cli</a><p>Tutorial: <a href=\"https:&#x2F;&#x2F;pantheonos.stanford.edu&#x2F;cli&#x2F;docs&#x2F;intro&#x2F;getting-started\" rel=\"nofollow\">https:&#x2F;&#x2F;pantheonos.stanford.edu&#x2F;cli&#x2F;docs&#x2F;intro&#x2F;getting-start...</a><p>Home page: <a href=\"https:&#x2F;&#x2F;pantheonos.stanford.edu&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;pantheonos.stanford.edu&#x2F;</a><p>Would love to hear feedback from the HN community‚Äîwhat use cases would you try this for, and what features would make it more useful to you?",
      "publishedAt": "2025-08-26T16:58:33.000Z",
      "source": "Hacker News AI",
      "sourceUrl": "https://github.com/aristoteleo/pantheon-cli",
      "category": "industry",
      "company": "Community",
      "imageUrl": null,
      "tags": [
        "GPT",
        "Claude",
        "Gemini",
        "Llama",
        "LLM"
      ],
      "featured": false
    },
    {
      "id": "miy7b8qs9swjxxlliwi",
      "title": "Meta: v0.2.0",
      "summary": "Llama 4 Support ( https://www.llama.com ) \r\n\r\n...",
      "content": "Llama 4 Support ( https://www.llama.com ) \r\n\r\n",
      "publishedAt": "2025-04-05T19:02:56.000Z",
      "source": "Meta GitHub",
      "sourceUrl": "https://github.com/meta-llama/llama-models/releases/tag/v0.2.0",
      "category": "research",
      "company": "Meta",
      "imageUrl": null,
      "tags": [
        "Llama"
      ],
      "featured": false
    }
  ],
  "featuredCount": 11
}