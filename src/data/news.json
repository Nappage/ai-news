{
  "lastUpdated": "2025-06-30T01:27:15.968Z",
  "totalArticles": 32,
  "articles": [
    {
      "id": "mcif4rg2xqkj3npaejh",
      "title": "Simulating a neural operating system with Gemini 2.5 Flash-Lite",
      "summary": "A research prototype simulating a neural operating system generates UI in real-time adapting to user interactions with Gemini 2.5 Flash-Lite, using interaction tracing for contextual awareness, streaming the UI for responsiveness, and achieving statefulness with an in-memory UI graph.",
      "content": "A research prototype simulating a neural operating system generates UI in real-time adapting to user interactions with Gemini 2.5 Flash-Lite, using interaction tracing for contextual awareness, streaming the UI for responsiveness, and achieving statefulness with an in-memory UI graph.",
      "publishedAt": "2025-06-29T01:26:45.746Z",
      "source": "Google Developers Blog",
      "sourceUrl": "https://developers.googleblog.com/en/simulating-a-neural-operating-system-with-gemini-2-5-flash-lite/",
      "category": "tools",
      "company": "Google",
      "imageUrl": null,
      "tags": [
        "Gemini"
      ],
      "featured": true
    },
    {
      "id": "mcif4olv17e05qy95ub",
      "title": "Customizable, no-code voice agent automation with GPT-4o",
      "summary": "Retell AI is transforming the call center with AI voice automation powered by GPT-4o and GPT-4.1. Its no-code platform enables businesses to launch natural, real-time voice agents that cut call costs, boost CSAT, and automate customer conversations—without scripts or hold times.",
      "content": "Retell AI is transforming the call center with AI voice automation powered by GPT-4o and GPT-4.1. Its no-code platform enables businesses to launch natural, real-time voice agents that cut call costs, boost CSAT, and automate customer conversations—without scripts or hold times.",
      "publishedAt": "2025-06-26T10:00:00.000Z",
      "source": "OpenAI Blog",
      "sourceUrl": "https://openai.com/index/retell-ai",
      "category": "companies",
      "company": "OpenAI",
      "imageUrl": null,
      "tags": [
        "GPT",
        "AI"
      ],
      "featured": true
    },
    {
      "id": "mcif4sd6n2bxvs2ad19",
      "title": "Gemini CLI: your open-source AI agent",
      "summary": "Free and open source, Gemini CLI brings Gemini directly into developers’ terminals — with unmatched access for individuals.",
      "content": "<img src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Gemini_CLI_Hero_Final.max-600x600.format-webp.webp\">Free and open source, Gemini CLI brings Gemini directly into developers’ terminals — with unmatched access for individuals.",
      "publishedAt": "2025-06-25T13:00:00.000Z",
      "source": "Google Technology Blog",
      "sourceUrl": "https://blog.google/technology/developers/introducing-gemini-cli-open-source-ai-agent/",
      "category": "companies",
      "company": "Google",
      "imageUrl": null,
      "tags": [
        "Gemini",
        "AI"
      ],
      "featured": true
    },
    {
      "id": "mcif4rg2u9ygps0pqn",
      "title": "Introducing Gemma 3n: The developer guide",
      "summary": "The Gemma 3n model has been fully released, building on the success of previous Gemma models and bringing advanced on-device multimodal capabilities to edge devices with unprecedented performance. Explore Gemma 3n's innovations, including its mobile-first architecture, MatFormer technology, Per-Laye...",
      "content": "The Gemma 3n model has been fully released, building on the success of previous Gemma models and bringing advanced on-device multimodal capabilities to edge devices with unprecedented performance. Explore Gemma 3n's innovations, including its mobile-first architecture, MatFormer technology, Per-Layer Embeddings, KV Cache Sharing, and new audio and MobileNet-V5 vision encoders, and how developers can start building with it today.",
      "publishedAt": "2025-06-24T01:26:45.746Z",
      "source": "Google Developers Blog",
      "sourceUrl": "https://developers.googleblog.com/en/introducing-gemma-3n-developer-guide/",
      "category": "tools",
      "company": "Google",
      "imageUrl": null,
      "tags": [],
      "featured": true
    },
    {
      "id": "mcif4sd6h3z4kmbkh4c",
      "title": "Ask a Techspert: What is inference?",
      "summary": "Learn more about AI and inference from Google experts.",
      "content": "<img src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/InferenceHero_v3.max-600x600.format-webp.webp\">Learn more about AI and inference from Google experts.",
      "publishedAt": "2025-06-23T17:30:00.000Z",
      "source": "Google Technology Blog",
      "sourceUrl": "https://blog.google/technology/ai/ask-a-techspert-what-is-inference/",
      "category": "companies",
      "company": "Google",
      "imageUrl": null,
      "tags": [
        "AI",
        "Google"
      ],
      "featured": true
    },
    {
      "id": "mcif4sd633f1djk1ssa",
      "title": "Search Live: Talk, listen and explore in real time with AI Mode",
      "summary": "Search Live with voice facilitates back-and-forth conversations in AI Mode.",
      "content": "<img src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/SearchLive_Hero.max-600x600.format-webp.webp\">Search Live with voice facilitates back-and-forth conversations in AI Mode.",
      "publishedAt": "2025-06-18T16:00:00.000Z",
      "source": "Google Technology Blog",
      "sourceUrl": "https://blog.google/products/search/search-live-ai-mode/",
      "category": "companies",
      "company": "Google",
      "imageUrl": null,
      "tags": [
        "AI"
      ],
      "featured": true
    },
    {
      "id": "mcif4sd6indkfps4y7",
      "title": "Hear a podcast discussion about Gemini’s coding capabilities.",
      "summary": "The latest episode of the Google AI: Release Notes podcast focuses on how the Gemini team built one of the world’s leading AI coding models.Host Logan Kilpatrick chats w…",
      "content": "<img src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/ep8_thumbnail.max-600x600.format-webp.webp\">The latest episode of the Google AI: Release Notes podcast focuses on how the Gemini team built one of the world’s leading AI coding models.Host Logan Kilpatrick chats w…",
      "publishedAt": "2025-06-18T10:28:00.000Z",
      "source": "Google Technology Blog",
      "sourceUrl": "https://blog.google/products/gemini/gemini-coding-podcast/",
      "category": "companies",
      "company": "Google",
      "imageUrl": null,
      "tags": [
        "Gemini",
        "AI",
        "Google"
      ],
      "featured": true
    },
    {
      "id": "mcif4olvs7x796v6iwj",
      "title": "Preparing for future AI risks in biology",
      "summary": "Advanced AI can transform biology and medicine—but also raises biosecurity risks. We’re proactively assessing capabilities and implementing safeguards to prevent misuse.",
      "content": "Advanced AI can transform biology and medicine—but also raises biosecurity risks. We’re proactively assessing capabilities and implementing safeguards to prevent misuse.",
      "publishedAt": "2025-06-18T10:00:00.000Z",
      "source": "OpenAI Blog",
      "sourceUrl": "https://openai.com/index/preparing-for-future-ai-capabilities-in-biology",
      "category": "companies",
      "company": "OpenAI",
      "imageUrl": null,
      "tags": [
        "AI"
      ],
      "featured": true
    },
    {
      "id": "mcif5d3nujrmgkx14je",
      "title": "Show HN: m(ctf)p – A semi-automated environment for solving CTF challenges",
      "summary": "Hi folks!<p>I built this over the past few weeks for a cross-company CTF I was participating in. It was mostly an experiment to learn about CTFs, MCP servers, Kali Linux, Claude Code, and really just how far LLMs can go in a given domain.<p>It&#x27;s basically just an MCP server (for integrating wit...",
      "content": "Hi folks!<p>I built this over the past few weeks for a cross-company CTF I was participating in. It was mostly an experiment to learn about CTFs, MCP servers, Kali Linux, Claude Code, and really just how far LLMs can go in a given domain.<p>It&#x27;s basically just an MCP server (for integrating with the CTF server APIs, providing notes, and a few other niceties) paired with a Kali Linux-based Docker image that has Claude Code installed, plus a custom slash command [1] to tie it all together.<p>It performed admirably during the CTF I tried it on, it was able to zero-shot solve maybe 10 or so of the simpler challenges, and provided substantial assistance on another 5 or 6 before getting stuck. It didn&#x27;t stand a chance against the hardest challenges.<p>This was the first CTF I&#x27;ve participated in, and it was an absolute blast. I can imagine some people feeling that LLMs take the fun out of CTFs, but I think the &quot;centaur&quot; [2] aspect of human-LLM interactions is both powerful and effective, given the right infrastructure and UX.<p>Happy to answer any questions people have about the project!<p>[1] <a href=\"https:&#x2F;&#x2F;docs.anthropic.com&#x2F;en&#x2F;docs&#x2F;claude-code&#x2F;slash-commands#custom-slash-commands\" rel=\"nofollow\">https:&#x2F;&#x2F;docs.anthropic.com&#x2F;en&#x2F;docs&#x2F;claude-code&#x2F;slash-command...</a><p>[2] <a href=\"https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Advanced_chess\" rel=\"nofollow\">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Advanced_chess</a>",
      "publishedAt": "2025-06-28T01:53:52.000Z",
      "source": "Hacker News Claude/Anthropic",
      "sourceUrl": "https://git.sr.ht/~bsprague/mctfp",
      "category": "companies",
      "company": "Anthropic",
      "imageUrl": null,
      "tags": [
        "Claude",
        "LLM",
        "AI",
        "Anthropic"
      ],
      "featured": false
    },
    {
      "id": "mcif4rg3jsacm24a6qg",
      "title": "Supercharge your notebooks: The new AI-first Google Colab is now available to everyone",
      "summary": "The new AI-first Google Colab enhances productivity with improvements powered by features like iterative querying for conversational coding, a next-generation Data Science Agent for autonomous workflows, and effortless code transformation. Early adopters report a dramatic productivity boost, acceler...",
      "content": "The new AI-first Google Colab enhances productivity with improvements powered by features like iterative querying for conversational coding, a next-generation Data Science Agent for autonomous workflows, and effortless code transformation. Early adopters report a dramatic productivity boost, accelerating ML projects, debugging code faster, and effortlessly creating high-quality visualizations.",
      "publishedAt": "2025-06-27T01:26:45.747Z",
      "source": "Google Developers Blog",
      "sourceUrl": "https://developers.googleblog.com/en/new-ai-first-google-colab-now-available-to-everyone/",
      "category": "tools",
      "company": "Google",
      "imageUrl": null,
      "tags": [
        "AI",
        "ML",
        "Google"
      ],
      "featured": false
    },
    {
      "id": "mcif4sd6j62mo3sd7sf",
      "title": "We’re improving Ask Photos and bringing it to more Google Photos users.",
      "summary": "We love seeing how you’re using Ask Photos in early access, like asking \"suggest photos that'd make great phone backgrounds\" or \"what did I eat on my trip to Barcelona?\"…",
      "content": "<img src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/original_videos/wagtailvideo-mbcag8qi_thumb.jpg\">We love seeing how you’re using Ask Photos in early access, like asking \"suggest photos that'd make great phone backgrounds\" or \"what did I eat on my trip to Barcelona?\"…",
      "publishedAt": "2025-06-26T17:00:00.000Z",
      "source": "Google Technology Blog",
      "sourceUrl": "https://blog.google/products/photos/updates-ask-photos-search/",
      "category": "companies",
      "company": "Google",
      "imageUrl": null,
      "tags": [
        "Google"
      ],
      "featured": false
    },
    {
      "id": "mcif4sd6jpdw6rejaaa",
      "title": "Try on looks and discover your style with Doppl",
      "summary": "Doppl, a new Google Labs app, uses AI to create personalized outfit try-on images and videos.",
      "content": "<img src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Doppl_-_YouTube_Thumbnail_-_120.max-600x600.format-webp.webp\">Doppl, a new Google Labs app, uses AI to create personalized outfit try-on images and videos.",
      "publishedAt": "2025-06-26T16:16:00.000Z",
      "source": "Google Technology Blog",
      "sourceUrl": "https://blog.google/technology/google-labs/doppl/",
      "category": "companies",
      "company": "Google",
      "imageUrl": null,
      "tags": [
        "AI",
        "Google"
      ],
      "featured": false
    },
    {
      "id": "mcif4sd6dtv7h8ysjl6",
      "title": "The Google for Startups Gemini kit is here",
      "summary": "Learn more about how startups can use Gemini models and other AI resources from Google.",
      "content": "<img src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Paige_Thumbnail.max-600x600.format-webp.webp\">Learn more about how startups can use Gemini models and other AI resources from Google.",
      "publishedAt": "2025-06-26T12:00:00.000Z",
      "source": "Google Technology Blog",
      "sourceUrl": "https://blog.google/outreach-initiatives/entrepreneurs/google-for-startups-gemini-ai-kit/",
      "category": "companies",
      "company": "Google",
      "imageUrl": null,
      "tags": [
        "Gemini",
        "AI",
        "Google"
      ],
      "featured": false
    },
    {
      "id": "mcif4pii3eskhcho2m7",
      "title": "Gemma 3n fully available in the open-source ecosystem!",
      "summary": "",
      "content": "",
      "publishedAt": "2025-06-26T00:00:00.000Z",
      "source": "Hugging Face Blog",
      "sourceUrl": "https://huggingface.co/blog/gemma3n",
      "category": "tools",
      "company": "Hugging Face",
      "imageUrl": null,
      "tags": [
        "AI"
      ],
      "featured": false
    },
    {
      "id": "mcif4sd6ze80odmdkc",
      "title": "5 tips for getting started with Flow",
      "summary": "Here are five tips for making videos with Flow, Google’s new AI filmmaking tool.",
      "content": "<img src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/GetStartedwithFlow_Hero.max-600x600.format-webp.webp\">Here are five tips for making videos with Flow, Google’s new AI filmmaking tool.",
      "publishedAt": "2025-06-25T22:45:00.000Z",
      "source": "Google Technology Blog",
      "sourceUrl": "https://blog.google/technology/ai/flow-video-tips/",
      "category": "companies",
      "company": "Google",
      "imageUrl": null,
      "tags": [
        "AI",
        "Google"
      ],
      "featured": false
    },
    {
      "id": "mcif4w1wdjt01hiua16",
      "title": "AlphaGenome: AI for better understanding the genome",
      "summary": "Introducing a new, unifying DNA sequence model that advances regulatory variant-effect prediction and promises to shed new light on genome function — now available via API.",
      "content": "Introducing a new, unifying DNA sequence model that advances regulatory variant-effect prediction and promises to shed new light on genome function — now available via API.",
      "publishedAt": "2025-06-25T13:59:51.000Z",
      "source": "DeepMind Blog",
      "sourceUrl": "https://deepmind.google/discover/blog/alphagenome-ai-for-better-understanding-the-genome/",
      "category": "research",
      "company": "Google",
      "imageUrl": null,
      "tags": [
        "AI"
      ],
      "featured": false
    },
    {
      "id": "mcif4rg3ab2rdkth2h",
      "title": "Using KerasHub for easy end-to-end machine learning workflows with Hugging Face",
      "summary": "KerasHub enables users to mix and match model architectures and weights across different machine learning frameworks, allowing checkpoints from sources like Hugging Face Hub (including those created with PyTorch) to be loaded into Keras models for use with JAX, PyTorch, or TensorFlow. This flexibili...",
      "content": "KerasHub enables users to mix and match model architectures and weights across different machine learning frameworks, allowing checkpoints from sources like Hugging Face Hub (including those created with PyTorch) to be loaded into Keras models for use with JAX, PyTorch, or TensorFlow. This flexibility means you can leverage a vast array of community fine-tuned models while maintaining full control over your chosen backend framework.",
      "publishedAt": "2025-06-25T01:26:45.747Z",
      "source": "Google Developers Blog",
      "sourceUrl": "https://developers.googleblog.com/en/load-model-weights-from-safetensors-into-kerashub-multi-framework-machine-learning/",
      "category": "tools",
      "company": "Google",
      "imageUrl": null,
      "tags": [
        "AI"
      ],
      "featured": false
    },
    {
      "id": "mcif4w1wevpfuf5zd6e",
      "title": "Gemini Robotics On-Device brings AI to local robotic devices",
      "summary": "We’re introducing an efficient, on-device robotics model with general-purpose dexterity and fast task adaptation.",
      "content": "We’re introducing an efficient, on-device robotics model with general-purpose dexterity and fast task adaptation.",
      "publishedAt": "2025-06-24T14:00:00.000Z",
      "source": "DeepMind Blog",
      "sourceUrl": "https://deepmind.google/discover/blog/gemini-robotics-on-device-brings-ai-to-local-robotic-devices/",
      "category": "research",
      "company": "Google",
      "imageUrl": null,
      "tags": [
        "Gemini",
        "AI"
      ],
      "featured": false
    },
    {
      "id": "mcif4rg31j439mek09w",
      "title": "Gemini 2.5 for robotics and embodied intelligence",
      "summary": "Gemini 2.5 Pro and Flash are transforming robotics by enhancing coding, reasoning, and multimodal capabilities, including spatial understanding. These models are used for semantic scene understanding, code generation for robot control, and building interactive applications with the Live API, with a ...",
      "content": "Gemini 2.5 Pro and Flash are transforming robotics by enhancing coding, reasoning, and multimodal capabilities, including spatial understanding. These models are used for semantic scene understanding, code generation for robot control, and building interactive applications with the Live API, with a strong emphasis on safety improvements and community applications.",
      "publishedAt": "2025-06-24T01:26:45.747Z",
      "source": "Google Developers Blog",
      "sourceUrl": "https://developers.googleblog.com/en/gemini-25-for-robotics-and-embodied-intelligence/",
      "category": "tools",
      "company": "Google",
      "imageUrl": null,
      "tags": [
        "Gemini"
      ],
      "featured": false
    },
    {
      "id": "mcif4olv2hl8cbdw0yi",
      "title": "Driving scalable growth with OpenAI o3, GPT-4.1, and CUA",
      "summary": "Unify, an AI-powered GTM platform, uses OpenAI’s o3, GPT-4.1, and CUA to automate prospecting, research, and outreach. With hyper-personalized messaging and an always-on workflow, Unify helps teams generate pipeline at scale while focusing on high-impact customer interactions.",
      "content": "Unify, an AI-powered GTM platform, uses OpenAI’s o3, GPT-4.1, and CUA to automate prospecting, research, and outreach. With hyper-personalized messaging and an always-on workflow, Unify helps teams generate pipeline at scale while focusing on high-impact customer interactions.",
      "publishedAt": "2025-06-24T00:00:00.000Z",
      "source": "OpenAI Blog",
      "sourceUrl": "https://openai.com/index/unify",
      "category": "companies",
      "company": "OpenAI",
      "imageUrl": null,
      "tags": [
        "GPT",
        "AI",
        "OpenAI"
      ],
      "featured": false
    },
    {
      "id": "mcif4rg3v24emxi989h",
      "title": "Imagen 4 is now available in the Gemini API and Google AI Studio",
      "summary": "Imagen 4, Google's advanced text-to-image model, is now available in paid preview via the Gemini API and Google AI Studio, offering significant quality improvements, especially for text generation within images. The Imagen 4 family includes Imagen 4 for general tasks and Imagen 4 Ultra for high-prec...",
      "content": "Imagen 4, Google's advanced text-to-image model, is now available in paid preview via the Gemini API and Google AI Studio, offering significant quality improvements, especially for text generation within images. The Imagen 4 family includes Imagen 4 for general tasks and Imagen 4 Ultra for high-precision prompt adherence, with all generated images featuring a non-visible SynthID watermark.",
      "publishedAt": "2025-06-23T01:26:45.747Z",
      "source": "Google Developers Blog",
      "sourceUrl": "https://developers.googleblog.com/en/imagen-4-now-available-in-the-gemini-api-and-google-ai-studio/",
      "category": "tools",
      "company": "Google",
      "imageUrl": null,
      "tags": [
        "Gemini",
        "AI",
        "Google"
      ],
      "featured": false
    },
    {
      "id": "mcif4rg32638woq1e32",
      "title": "Google Cloud donates A2A to Linux Foundation",
      "summary": "Google, along with Amazon and Cisco, announces the formation of the Agent2Agent Foundation under the Linux Foundation, establishing A2A as an industry standard for AI agent interoperability, fostering a diverse ecosystem, ensuring neutral governance, and accelerating secure innovation in AI applicat...",
      "content": "Google, along with Amazon and Cisco, announces the formation of the Agent2Agent Foundation under the Linux Foundation, establishing A2A as an industry standard for AI agent interoperability, fostering a diverse ecosystem, ensuring neutral governance, and accelerating secure innovation in AI applications.",
      "publishedAt": "2025-06-23T01:26:45.747Z",
      "source": "Google Developers Blog",
      "sourceUrl": "https://developers.googleblog.com/en/google-cloud-donates-a2a-to-linux-foundation/",
      "category": "tools",
      "company": "Google",
      "imageUrl": null,
      "tags": [
        "AI",
        "Google"
      ],
      "featured": false
    },
    {
      "id": "mcif4rg2hks6s1pkfs",
      "title": "Unlock deeper insights with the new Python client library for Data Commons",
      "summary": "Google has released a new Python client library for Data Commons – an open-source knowledge graph that unifies public statistical data, and enhances how data developers can leverage Data Commons by offering improved features, support for custom instances, and easier access to a vast array of statist...",
      "content": "Google has released a new Python client library for Data Commons – an open-source knowledge graph that unifies public statistical data, and enhances how data developers can leverage Data Commons by offering improved features, support for custom instances, and easier access to a vast array of statistical variables – developed with contributions from The ONE Campaign.",
      "publishedAt": "2025-06-23T01:26:45.746Z",
      "source": "Google Developers Blog",
      "sourceUrl": "https://developers.googleblog.com/en/pythondatacommons/",
      "category": "tools",
      "company": "Google",
      "imageUrl": null,
      "tags": [
        "AI",
        "Google"
      ],
      "featured": false
    },
    {
      "id": "mcif4piinq5mbjtb68d",
      "title": "Transformers backend integration in SGLang",
      "summary": "",
      "content": "",
      "publishedAt": "2025-06-23T00:00:00.000Z",
      "source": "Hugging Face Blog",
      "sourceUrl": "https://huggingface.co/blog/transformers-backend-sglang",
      "category": "tools",
      "company": "Hugging Face",
      "imageUrl": null,
      "tags": [
        "Transformer"
      ],
      "featured": false
    },
    {
      "id": "mcif4piiz2jsd1ezda",
      "title": "(LoRA) Fine-Tuning FLUX.1-dev on Consumer Hardware",
      "summary": "",
      "content": "",
      "publishedAt": "2025-06-19T00:00:00.000Z",
      "source": "Hugging Face Blog",
      "sourceUrl": "https://huggingface.co/blog/flux-qlora",
      "category": "tools",
      "company": "Hugging Face",
      "imageUrl": null,
      "tags": [],
      "featured": false
    },
    {
      "id": "mcif4olvrrnp6k129rh",
      "title": "Toward understanding and preventing misalignment generalization",
      "summary": "We study how training on incorrect responses can cause broader misalignment in language models and identify an internal feature driving this behavior—one that can be reversed with minimal fine-tuning.",
      "content": "We study how training on incorrect responses can cause broader misalignment in language models and identify an internal feature driving this behavior—one that can be reversed with minimal fine-tuning.",
      "publishedAt": "2025-06-18T10:00:00.000Z",
      "source": "OpenAI Blog",
      "sourceUrl": "https://openai.com/index/emergent-misalignment",
      "category": "companies",
      "company": "OpenAI",
      "imageUrl": null,
      "tags": [
        "AI"
      ],
      "featured": false
    },
    {
      "id": "mcif4w1wd5wjs964y8",
      "title": "Gemini 2.5: Updates to our family of thinking models",
      "summary": "Explore the latest Gemini 2.5 model updates with enhanced performance and accuracy: Gemini 2.5 Pro now stable, Flash generally available, and the new Flash-Lite in preview.",
      "content": "Explore the latest Gemini 2.5 model updates with enhanced performance and accuracy: Gemini 2.5 Pro now stable, Flash generally available, and the new Flash-Lite in preview.",
      "publishedAt": "2025-06-17T16:03:39.000Z",
      "source": "DeepMind Blog",
      "sourceUrl": "https://deepmind.google/discover/blog/gemini-25-updates-to-our-family-of-thinking-models/",
      "category": "research",
      "company": "Google",
      "imageUrl": null,
      "tags": [
        "Gemini",
        "AI"
      ],
      "featured": false
    },
    {
      "id": "mcif4w1w6ndpbwh1ovd",
      "title": "We’re expanding our Gemini 2.5 family of models",
      "summary": "Gemini 2.5 Flash and Pro are now generally available, and we’re introducing 2.5 Flash-Lite, our most cost-efficient and fastest 2.5 model yet.",
      "content": "Gemini 2.5 Flash and Pro are now generally available, and we’re introducing 2.5 Flash-Lite, our most cost-efficient and fastest 2.5 model yet.",
      "publishedAt": "2025-06-17T16:01:00.000Z",
      "source": "DeepMind Blog",
      "sourceUrl": "https://deepmind.google/discover/blog/were-expanding-our-gemini-25-family-of-models/",
      "category": "research",
      "company": "Google",
      "imageUrl": null,
      "tags": [
        "Gemini",
        "AI"
      ],
      "featured": false
    },
    {
      "id": "mcif4olvqtxit3acskk",
      "title": "Introducing OpenAI for Government",
      "summary": "We’re launching OpenAI for Government, a new initiative focused on bringing our most advanced AI tools to public servants across the United States. We're supporting the U.S. government's efforts in adopting best-in-class technology and deploying these tools in service of the public good.",
      "content": "We’re launching OpenAI for Government, a new initiative focused on bringing our most advanced AI tools to public servants across the United States. We're supporting the U.S. government's efforts in adopting best-in-class technology and deploying these tools in service of the public good.",
      "publishedAt": "2025-06-16T00:00:00.000Z",
      "source": "OpenAI Blog",
      "sourceUrl": "https://openai.com/global-affairs/introducing-openai-for-government",
      "category": "companies",
      "company": "OpenAI",
      "imageUrl": null,
      "tags": [
        "AI",
        "OpenAI"
      ],
      "featured": false
    },
    {
      "id": "mcif4piii5dwmwsz6rl",
      "title": "Groq on Hugging Face Inference Providers 🔥",
      "summary": "",
      "content": "",
      "publishedAt": "2025-06-16T00:00:00.000Z",
      "source": "Hugging Face Blog",
      "sourceUrl": "https://huggingface.co/blog/inference-providers-groq",
      "category": "tools",
      "company": "Hugging Face",
      "imageUrl": null,
      "tags": [],
      "featured": false
    },
    {
      "id": "mcif4w1wprr02mkcy7",
      "title": "Behind “ANCESTRA”: combining Veo with live-action filmmaking",
      "summary": "We partnered with Darren Aronofsky, Eliza McNitt and a team of more than 200 people to make a film using Veo and live-action filmmaking.",
      "content": "We partnered with Darren Aronofsky, Eliza McNitt and a team of more than 200 people to make a film using Veo and live-action filmmaking.",
      "publishedAt": "2025-06-13T13:30:00.000Z",
      "source": "DeepMind Blog",
      "sourceUrl": "https://deepmind.google/discover/blog/behind-ancestra-combining-veo-with-live-action-filmmaking/",
      "category": "research",
      "company": "Google",
      "imageUrl": null,
      "tags": [],
      "featured": false
    },
    {
      "id": "mcif4piifbhkji9pbr4",
      "title": "Enhance Your Models in 5 Minutes with the Hugging Face Kernel Hub",
      "summary": "",
      "content": "",
      "publishedAt": "2025-06-12T00:00:00.000Z",
      "source": "Hugging Face Blog",
      "sourceUrl": "https://huggingface.co/blog/hello-hf-kernels",
      "category": "tools",
      "company": "Hugging Face",
      "imageUrl": null,
      "tags": [],
      "featured": false
    }
  ],
  "communityArticles": [
    {
      "id": "mcif551ipzpm9b4rz2",
      "title": "発見: garylab/EarnWithAI - A list of open-source AI projects you can use to generate income easily.",
      "summary": "新しいプロジェクトを発見: A list of open-source AI projects you can use to generate income easily. (⭐37 | 🍴6)",
      "content": "A list of open-source AI projects you can use to generate income easily.\n\n言語: Python\nスター数: 37\nフォーク数: 6",
      "publishedAt": "2025-06-30T01:26:51.000Z",
      "source": "GitHub Search",
      "sourceUrl": "https://github.com/garylab/EarnWithAI",
      "category": "community",
      "company": "Community",
      "imageUrl": null,
      "tags": [
        "AI"
      ],
      "featured": false,
      "showOnTopPage": false
    },
    {
      "id": "mcif56vef7504kfm7ae",
      "title": "発見: HovChen/Paper-List-for-Medical-Reasoning-Large-Language-Models - Paper List for Medical Reasoning Large Language Models",
      "summary": "新しいプロジェクトを発見: Paper List for Medical Reasoning Large Language Models (⭐7 | 🍴0)",
      "content": "Paper List for Medical Reasoning Large Language Models\n\n言語: N/A\nスター数: 7\nフォーク数: 0",
      "publishedAt": "2025-06-30T01:26:51.000Z",
      "source": "GitHub Search",
      "sourceUrl": "https://github.com/HovChen/Paper-List-for-Medical-Reasoning-Large-Language-Models",
      "category": "community",
      "company": "Community",
      "imageUrl": null,
      "tags": [],
      "featured": false,
      "showOnTopPage": false
    },
    {
      "id": "mcif551inf8ivjyvd9l",
      "title": "発見: sst/opencode - AI coding agent, built for the terminal.",
      "summary": "新しいプロジェクトを発見: AI coding agent, built for the terminal. (⭐7584 | 🍴365)",
      "content": "AI coding agent, built for the terminal.\n\n言語: TypeScript\nスター数: 7584\nフォーク数: 365",
      "publishedAt": "2025-06-30T01:26:48.000Z",
      "source": "GitHub Search",
      "sourceUrl": "https://github.com/sst/opencode",
      "category": "community",
      "company": "Community",
      "imageUrl": null,
      "tags": [
        "AI"
      ],
      "featured": false,
      "showOnTopPage": false
    },
    {
      "id": "mcif551ioimzwywk93",
      "title": "発見: TakanariShimbo/quickchart-mcp-server - This Model Context Protocol (MCP) server provides powerful visualization tools u",
      "summary": "新しいプロジェクトを発見: This Model Context Protocol (MCP) server provides powerful visualization tools using QuickChart.io APIs.   With this MCP, AI assistants can create charts, diagrams, barcodes, QR codes, word clouds, tables, and more. (⭐0 | 🍴0)",
      "content": "This Model Context Protocol (MCP) server provides powerful visualization tools using QuickChart.io APIs.   With this MCP, AI assistants can create charts, diagrams, barcodes, QR codes, word clouds, tables, and more.\n\n言語: TypeScript\nスター数: 0\nフォーク数: 0",
      "publishedAt": "2025-06-30T01:26:43.000Z",
      "source": "GitHub Search",
      "sourceUrl": "https://github.com/TakanariShimbo/quickchart-mcp-server",
      "category": "community",
      "company": "Community",
      "imageUrl": null,
      "tags": [
        "AI"
      ],
      "featured": false,
      "showOnTopPage": false
    },
    {
      "id": "mcif551iny4nixfhxfd",
      "title": "発見: Idea-R/OnceAI - GitHub新プロジェクト",
      "summary": "新しいプロジェクトを発見: GitHub上の注目プロジェクト (⭐0 | 🍴0)",
      "content": "\n\n言語: TypeScript\nスター数: 0\nフォーク数: 0",
      "publishedAt": "2025-06-30T01:26:30.000Z",
      "source": "GitHub Search",
      "sourceUrl": "https://github.com/Idea-R/OnceAI",
      "category": "community",
      "company": "Community",
      "imageUrl": null,
      "tags": [
        "AI"
      ],
      "featured": false,
      "showOnTopPage": false
    },
    {
      "id": "mcif55xid6j5siwxzs5",
      "title": "発見: chris-pikul/py-exfer - Python library providing clients for external inference providers such as OpenAI",
      "summary": "新しいプロジェクトを発見: Python library providing clients for external inference providers such as OpenAI, OpenRouter, Anthropic, Google, etc. (⭐0 | 🍴0)",
      "content": "Python library providing clients for external inference providers such as OpenAI, OpenRouter, Anthropic, Google, etc.\n\n言語: N/A\nスター数: 0\nフォーク数: 0",
      "publishedAt": "2025-06-30T01:26:30.000Z",
      "source": "GitHub Search",
      "sourceUrl": "https://github.com/chris-pikul/py-exfer",
      "category": "community",
      "company": "Community",
      "imageUrl": null,
      "tags": [
        "AI",
        "OpenAI",
        "Anthropic",
        "Google"
      ],
      "featured": false,
      "showOnTopPage": false
    },
    {
      "id": "mcif551j7rvjqrcur8k",
      "title": "発見: chatman-media/timeline-studio - Timeline Studio - Video Editing with AI",
      "summary": "新しいプロジェクトを発見: Timeline Studio - Video Editing with AI (⭐11 | 🍴1)",
      "content": "Timeline Studio - Video Editing with AI\n\n言語: TypeScript\nスター数: 11\nフォーク数: 1",
      "publishedAt": "2025-06-30T01:26:27.000Z",
      "source": "GitHub Search",
      "sourceUrl": "https://github.com/chatman-media/timeline-studio",
      "category": "community",
      "company": "Community",
      "imageUrl": null,
      "tags": [
        "AI"
      ],
      "featured": false,
      "showOnTopPage": false
    },
    {
      "id": "mcif56veabo11tjpirh",
      "title": "発見: Darshika-Dudhat/Equity_news_research_tool - Equity News Research Toll using Groq LLM and News API",
      "summary": "新しいプロジェクトを発見: Equity News Research Toll using Groq LLM and News API (⭐0 | 🍴0)",
      "content": "Equity News Research Toll using Groq LLM and News API\n\n言語: Python\nスター数: 0\nフォーク数: 0",
      "publishedAt": "2025-06-30T01:26:01.000Z",
      "source": "GitHub Search",
      "sourceUrl": "https://github.com/Darshika-Dudhat/Equity_news_research_tool",
      "category": "community",
      "company": "Community",
      "imageUrl": null,
      "tags": [
        "LLM"
      ],
      "featured": false,
      "showOnTopPage": false
    },
    {
      "id": "mcif55xizbqlmk210y",
      "title": "発見: CARIOR123/browser-operator - Build your own AI operators like OpenAI",
      "summary": "新しいプロジェクトを発見: Build your own AI operators like OpenAI (⭐4 | 🍴0)",
      "content": "Build your own AI operators like OpenAI\n\n言語: N/A\nスター数: 4\nフォーク数: 0",
      "publishedAt": "2025-06-30T01:25:58.000Z",
      "source": "GitHub Search",
      "sourceUrl": "https://github.com/CARIOR123/browser-operator",
      "category": "community",
      "company": "Community",
      "imageUrl": null,
      "tags": [
        "AI",
        "OpenAI"
      ],
      "featured": false,
      "showOnTopPage": false
    },
    {
      "id": "mcif56vezthugpnwg6",
      "title": "発見: pedrojadir/framework_cognitivo_core4.0 - Projeto base do laboratório Core 4.0: orquestração de modelos de IA com LLMs, RA",
      "summary": "新しいプロジェクトを発見: Projeto base do laboratório Core 4.0: orquestração de modelos de IA com LLMs, RAG, agentes autônomos e MLOps local. (⭐0 | 🍴0)",
      "content": "Projeto base do laboratório Core 4.0: orquestração de modelos de IA com LLMs, RAG, agentes autônomos e MLOps local.\n\n言語: N/A\nスター数: 0\nフォーク数: 0",
      "publishedAt": "2025-06-30T01:25:54.000Z",
      "source": "GitHub Search",
      "sourceUrl": "https://github.com/pedrojadir/framework_cognitivo_core4.0",
      "category": "community",
      "company": "Community",
      "imageUrl": null,
      "tags": [
        "LLM",
        "ML"
      ],
      "featured": false,
      "showOnTopPage": false
    },
    {
      "id": "mcif56ved3w9kfr61db",
      "title": "発見: paulinhok14/fine-tuning-llms - A lightweight toolkit for efficient quantization and LoRA‑based fine‑tuning of L",
      "summary": "新しいプロジェクトを発見: A lightweight toolkit for efficient quantization and LoRA‑based fine‑tuning of LLMs (Large Language Models). (⭐0 | 🍴0)",
      "content": "A lightweight toolkit for efficient quantization and LoRA‑based fine‑tuning of LLMs (Large Language Models).\n\n言語: Jupyter Notebook\nスター数: 0\nフォーク数: 0",
      "publishedAt": "2025-06-30T01:25:41.000Z",
      "source": "GitHub Search",
      "sourceUrl": "https://github.com/paulinhok14/fine-tuning-llms",
      "category": "community",
      "company": "Community",
      "imageUrl": null,
      "tags": [
        "LLM"
      ],
      "featured": false,
      "showOnTopPage": false
    },
    {
      "id": "mcif56veij8k58qblle",
      "title": "発見: JasonFantl/LLMSwarmControl - GitHub新プロジェクト",
      "summary": "新しいプロジェクトを発見: GitHub上の注目プロジェクト (⭐0 | 🍴0)",
      "content": "\n\n言語: JavaScript\nスター数: 0\nフォーク数: 0",
      "publishedAt": "2025-06-30T01:24:46.000Z",
      "source": "GitHub Search",
      "sourceUrl": "https://github.com/JasonFantl/LLMSwarmControl",
      "category": "community",
      "company": "Community",
      "imageUrl": null,
      "tags": [
        "LLM"
      ],
      "featured": false,
      "showOnTopPage": false
    }
  ],
  "githubArticles": [
    {
      "id": "mcif551ipzpm9b4rz2",
      "title": "発見: garylab/EarnWithAI - A list of open-source AI projects you can use to generate income easily.",
      "summary": "新しいプロジェクトを発見: A list of open-source AI projects you can use to generate income easily. (⭐37 | 🍴6)",
      "content": "A list of open-source AI projects you can use to generate income easily.\n\n言語: Python\nスター数: 37\nフォーク数: 6",
      "publishedAt": "2025-06-30T01:26:51.000Z",
      "source": "GitHub Search",
      "sourceUrl": "https://github.com/garylab/EarnWithAI",
      "category": "community",
      "company": "Community",
      "imageUrl": null,
      "tags": [
        "AI"
      ],
      "featured": false,
      "showOnTopPage": false
    },
    {
      "id": "mcif56vef7504kfm7ae",
      "title": "発見: HovChen/Paper-List-for-Medical-Reasoning-Large-Language-Models - Paper List for Medical Reasoning Large Language Models",
      "summary": "新しいプロジェクトを発見: Paper List for Medical Reasoning Large Language Models (⭐7 | 🍴0)",
      "content": "Paper List for Medical Reasoning Large Language Models\n\n言語: N/A\nスター数: 7\nフォーク数: 0",
      "publishedAt": "2025-06-30T01:26:51.000Z",
      "source": "GitHub Search",
      "sourceUrl": "https://github.com/HovChen/Paper-List-for-Medical-Reasoning-Large-Language-Models",
      "category": "community",
      "company": "Community",
      "imageUrl": null,
      "tags": [],
      "featured": false,
      "showOnTopPage": false
    },
    {
      "id": "mcif551inf8ivjyvd9l",
      "title": "発見: sst/opencode - AI coding agent, built for the terminal.",
      "summary": "新しいプロジェクトを発見: AI coding agent, built for the terminal. (⭐7584 | 🍴365)",
      "content": "AI coding agent, built for the terminal.\n\n言語: TypeScript\nスター数: 7584\nフォーク数: 365",
      "publishedAt": "2025-06-30T01:26:48.000Z",
      "source": "GitHub Search",
      "sourceUrl": "https://github.com/sst/opencode",
      "category": "community",
      "company": "Community",
      "imageUrl": null,
      "tags": [
        "AI"
      ],
      "featured": false,
      "showOnTopPage": false
    },
    {
      "id": "mcif551ioimzwywk93",
      "title": "発見: TakanariShimbo/quickchart-mcp-server - This Model Context Protocol (MCP) server provides powerful visualization tools u",
      "summary": "新しいプロジェクトを発見: This Model Context Protocol (MCP) server provides powerful visualization tools using QuickChart.io APIs.   With this MCP, AI assistants can create charts, diagrams, barcodes, QR codes, word clouds, tables, and more. (⭐0 | 🍴0)",
      "content": "This Model Context Protocol (MCP) server provides powerful visualization tools using QuickChart.io APIs.   With this MCP, AI assistants can create charts, diagrams, barcodes, QR codes, word clouds, tables, and more.\n\n言語: TypeScript\nスター数: 0\nフォーク数: 0",
      "publishedAt": "2025-06-30T01:26:43.000Z",
      "source": "GitHub Search",
      "sourceUrl": "https://github.com/TakanariShimbo/quickchart-mcp-server",
      "category": "community",
      "company": "Community",
      "imageUrl": null,
      "tags": [
        "AI"
      ],
      "featured": false,
      "showOnTopPage": false
    },
    {
      "id": "mcif551iny4nixfhxfd",
      "title": "発見: Idea-R/OnceAI - GitHub新プロジェクト",
      "summary": "新しいプロジェクトを発見: GitHub上の注目プロジェクト (⭐0 | 🍴0)",
      "content": "\n\n言語: TypeScript\nスター数: 0\nフォーク数: 0",
      "publishedAt": "2025-06-30T01:26:30.000Z",
      "source": "GitHub Search",
      "sourceUrl": "https://github.com/Idea-R/OnceAI",
      "category": "community",
      "company": "Community",
      "imageUrl": null,
      "tags": [
        "AI"
      ],
      "featured": false,
      "showOnTopPage": false
    },
    {
      "id": "mcif55xid6j5siwxzs5",
      "title": "発見: chris-pikul/py-exfer - Python library providing clients for external inference providers such as OpenAI",
      "summary": "新しいプロジェクトを発見: Python library providing clients for external inference providers such as OpenAI, OpenRouter, Anthropic, Google, etc. (⭐0 | 🍴0)",
      "content": "Python library providing clients for external inference providers such as OpenAI, OpenRouter, Anthropic, Google, etc.\n\n言語: N/A\nスター数: 0\nフォーク数: 0",
      "publishedAt": "2025-06-30T01:26:30.000Z",
      "source": "GitHub Search",
      "sourceUrl": "https://github.com/chris-pikul/py-exfer",
      "category": "community",
      "company": "Community",
      "imageUrl": null,
      "tags": [
        "AI",
        "OpenAI",
        "Anthropic",
        "Google"
      ],
      "featured": false,
      "showOnTopPage": false
    },
    {
      "id": "mcif551j7rvjqrcur8k",
      "title": "発見: chatman-media/timeline-studio - Timeline Studio - Video Editing with AI",
      "summary": "新しいプロジェクトを発見: Timeline Studio - Video Editing with AI (⭐11 | 🍴1)",
      "content": "Timeline Studio - Video Editing with AI\n\n言語: TypeScript\nスター数: 11\nフォーク数: 1",
      "publishedAt": "2025-06-30T01:26:27.000Z",
      "source": "GitHub Search",
      "sourceUrl": "https://github.com/chatman-media/timeline-studio",
      "category": "community",
      "company": "Community",
      "imageUrl": null,
      "tags": [
        "AI"
      ],
      "featured": false,
      "showOnTopPage": false
    },
    {
      "id": "mcif56veabo11tjpirh",
      "title": "発見: Darshika-Dudhat/Equity_news_research_tool - Equity News Research Toll using Groq LLM and News API",
      "summary": "新しいプロジェクトを発見: Equity News Research Toll using Groq LLM and News API (⭐0 | 🍴0)",
      "content": "Equity News Research Toll using Groq LLM and News API\n\n言語: Python\nスター数: 0\nフォーク数: 0",
      "publishedAt": "2025-06-30T01:26:01.000Z",
      "source": "GitHub Search",
      "sourceUrl": "https://github.com/Darshika-Dudhat/Equity_news_research_tool",
      "category": "community",
      "company": "Community",
      "imageUrl": null,
      "tags": [
        "LLM"
      ],
      "featured": false,
      "showOnTopPage": false
    },
    {
      "id": "mcif55xizbqlmk210y",
      "title": "発見: CARIOR123/browser-operator - Build your own AI operators like OpenAI",
      "summary": "新しいプロジェクトを発見: Build your own AI operators like OpenAI (⭐4 | 🍴0)",
      "content": "Build your own AI operators like OpenAI\n\n言語: N/A\nスター数: 4\nフォーク数: 0",
      "publishedAt": "2025-06-30T01:25:58.000Z",
      "source": "GitHub Search",
      "sourceUrl": "https://github.com/CARIOR123/browser-operator",
      "category": "community",
      "company": "Community",
      "imageUrl": null,
      "tags": [
        "AI",
        "OpenAI"
      ],
      "featured": false,
      "showOnTopPage": false
    },
    {
      "id": "mcif56vezthugpnwg6",
      "title": "発見: pedrojadir/framework_cognitivo_core4.0 - Projeto base do laboratório Core 4.0: orquestração de modelos de IA com LLMs, RA",
      "summary": "新しいプロジェクトを発見: Projeto base do laboratório Core 4.0: orquestração de modelos de IA com LLMs, RAG, agentes autônomos e MLOps local. (⭐0 | 🍴0)",
      "content": "Projeto base do laboratório Core 4.0: orquestração de modelos de IA com LLMs, RAG, agentes autônomos e MLOps local.\n\n言語: N/A\nスター数: 0\nフォーク数: 0",
      "publishedAt": "2025-06-30T01:25:54.000Z",
      "source": "GitHub Search",
      "sourceUrl": "https://github.com/pedrojadir/framework_cognitivo_core4.0",
      "category": "community",
      "company": "Community",
      "imageUrl": null,
      "tags": [
        "LLM",
        "ML"
      ],
      "featured": false,
      "showOnTopPage": false
    },
    {
      "id": "mcif56ved3w9kfr61db",
      "title": "発見: paulinhok14/fine-tuning-llms - A lightweight toolkit for efficient quantization and LoRA‑based fine‑tuning of L",
      "summary": "新しいプロジェクトを発見: A lightweight toolkit for efficient quantization and LoRA‑based fine‑tuning of LLMs (Large Language Models). (⭐0 | 🍴0)",
      "content": "A lightweight toolkit for efficient quantization and LoRA‑based fine‑tuning of LLMs (Large Language Models).\n\n言語: Jupyter Notebook\nスター数: 0\nフォーク数: 0",
      "publishedAt": "2025-06-30T01:25:41.000Z",
      "source": "GitHub Search",
      "sourceUrl": "https://github.com/paulinhok14/fine-tuning-llms",
      "category": "community",
      "company": "Community",
      "imageUrl": null,
      "tags": [
        "LLM"
      ],
      "featured": false,
      "showOnTopPage": false
    },
    {
      "id": "mcif56veij8k58qblle",
      "title": "発見: JasonFantl/LLMSwarmControl - GitHub新プロジェクト",
      "summary": "新しいプロジェクトを発見: GitHub上の注目プロジェクト (⭐0 | 🍴0)",
      "content": "\n\n言語: JavaScript\nスター数: 0\nフォーク数: 0",
      "publishedAt": "2025-06-30T01:24:46.000Z",
      "source": "GitHub Search",
      "sourceUrl": "https://github.com/JasonFantl/LLMSwarmControl",
      "category": "community",
      "company": "Community",
      "imageUrl": null,
      "tags": [
        "LLM"
      ],
      "featured": false,
      "showOnTopPage": false
    },
    {
      "id": "mcif57txptukgu3p2b",
      "title": "Anthropic: prompt-eng-interactive-tutorialの最新アップデート",
      "summary": "Anthropic's Interactive Prompt Engineering Tutorialが更新されました (⭐13878)",
      "content": "Anthropic's Interactive Prompt Engineering Tutorial\n\n最終更新: 6/30/2025\nスター数: 13878",
      "publishedAt": "2025-06-30T01:19:32.000Z",
      "source": "Anthropic GitHub Org",
      "sourceUrl": "https://github.com/anthropics/prompt-eng-interactive-tutorial",
      "category": "tools",
      "company": "Anthropic",
      "imageUrl": null,
      "tags": [
        "Anthropic"
      ],
      "featured": true
    },
    {
      "id": "mcif57txhtnsnk8uc6q",
      "title": "Anthropic: dxtの最新アップデート",
      "summary": "Desktop Extensions: One-click local MCP server installation in desktop appsが更新されました (⭐459)",
      "content": "Desktop Extensions: One-click local MCP server installation in desktop apps\n\n最終更新: 6/30/2025\nスター数: 459",
      "publishedAt": "2025-06-30T01:14:18.000Z",
      "source": "Anthropic GitHub Org",
      "sourceUrl": "https://github.com/anthropics/dxt",
      "category": "tools",
      "company": "Anthropic",
      "imageUrl": null,
      "tags": [],
      "featured": true
    },
    {
      "id": "mcif57txdwoi6yu4reb",
      "title": "Anthropic: claude-code-sdk-pythonの最新アップデート",
      "summary": "claude-code-sdk-pythonが更新されました (⭐584)",
      "content": "\n\n最終更新: 6/30/2025\nスター数: 584",
      "publishedAt": "2025-06-30T01:02:51.000Z",
      "source": "Anthropic GitHub Org",
      "sourceUrl": "https://github.com/anthropics/claude-code-sdk-python",
      "category": "tools",
      "company": "Anthropic",
      "imageUrl": null,
      "tags": [
        "Claude"
      ],
      "featured": true
    },
    {
      "id": "mcif57txfk4sup3cdy",
      "title": "Anthropic: anthropic-cookbookの最新アップデート",
      "summary": "A collection of notebooks/recipes showcasing some fun and effective ways of using Claude.が更新されました (⭐17287)",
      "content": "A collection of notebooks/recipes showcasing some fun and effective ways of using Claude.\n\n最終更新: 6/30/2025\nスター数: 17287",
      "publishedAt": "2025-06-30T01:00:37.000Z",
      "source": "Anthropic GitHub Org",
      "sourceUrl": "https://github.com/anthropics/anthropic-cookbook",
      "category": "tools",
      "company": "Anthropic",
      "imageUrl": null,
      "tags": [
        "Claude",
        "Anthropic"
      ],
      "featured": true
    },
    {
      "id": "mcif57txuyh0thz92th",
      "title": "Anthropic: coursesの最新アップデート",
      "summary": "Anthropic's educational coursesが更新されました (⭐16141)",
      "content": "Anthropic's educational courses\n\n最終更新: 6/30/2025\nスター数: 16141",
      "publishedAt": "2025-06-30T00:59:12.000Z",
      "source": "Anthropic GitHub Org",
      "sourceUrl": "https://github.com/anthropics/courses",
      "category": "tools",
      "company": "Anthropic",
      "imageUrl": null,
      "tags": [
        "Anthropic"
      ],
      "featured": true
    },
    {
      "id": "mcif4ypg0tf9c7xex7nd",
      "title": "Anthropic: v0.1.0 (Initial release)",
      "summary": "**Full Changelog**: https://github.com/anthropics/dxt/commits/v0.1.0...",
      "content": "**Full Changelog**: https://github.com/anthropics/dxt/commits/v0.1.0",
      "publishedAt": "2025-06-28T23:43:01.000Z",
      "source": "Anthropic GitHub",
      "sourceUrl": "https://github.com/anthropics/dxt/releases/tag/v0.1.0",
      "category": "tools",
      "company": "Anthropic",
      "imageUrl": null,
      "tags": [
        "Anthropic"
      ],
      "featured": false
    },
    {
      "id": "mcif52cxm4gedkazhh",
      "title": "Google: v0.1.7",
      "summary": "- Fixes a bug with the sandbox container image in 0.1.6 (#2302)\r\n- Release Notes for 0.1.6 at https://github.com/google-gemini/gemini-cli/discussions/2301...",
      "content": "- Fixes a bug with the sandbox container image in 0.1.6 (#2302)\r\n- Release Notes for 0.1.6 at https://github.com/google-gemini/gemini-cli/discussions/2301",
      "publishedAt": "2025-06-28T00:28:03.000Z",
      "source": "Google GitHub",
      "sourceUrl": "https://github.com/google-gemini/gemini-cli/releases/tag/v0.1.7",
      "category": "tools",
      "company": "Google",
      "imageUrl": null,
      "tags": [
        "Gemini",
        "AI",
        "Google"
      ],
      "featured": false
    },
    {
      "id": "mcif52cygjhurk2fwfp",
      "title": "Google: v0.1.6",
      "summary": "Release notes at: https://github.com/google-gemini/gemini-cli/discussions/2301...",
      "content": "Release notes at: https://github.com/google-gemini/gemini-cli/discussions/2301",
      "publishedAt": "2025-06-28T00:22:26.000Z",
      "source": "Google GitHub",
      "sourceUrl": "https://github.com/google-gemini/gemini-cli/releases/tag/v0.1.6",
      "category": "tools",
      "company": "Google",
      "imageUrl": null,
      "tags": [
        "Gemini",
        "Google"
      ],
      "featured": false
    },
    {
      "id": "mcif50jto9by7z1hr2o",
      "title": "OpenAI: openai-cookbookの最新アップデート",
      "summary": "openai-cookbookリポジトリに新しい更新: fix images display issue for codex <> jira cookbook (#1925)...",
      "content": "fix images display issue for codex <> jira cookbook (#1925)",
      "publishedAt": "2025-06-27T13:28:01.000Z",
      "source": "OpenAI GitHub",
      "sourceUrl": "https://github.com/openai/openai-cookbook",
      "category": "tools",
      "company": "OpenAI",
      "imageUrl": null,
      "tags": [
        "AI",
        "OpenAI"
      ],
      "featured": false
    },
    {
      "id": "mcif5448bjgoit13p0c",
      "title": "Hugging Face: Release v4.53.0",
      "summary": "## Release v4.53.0\r\n\r\n### Gemma3n\r\n\r\nGemma 3n models are designed for efficient execution on low-resource devices. They are capable of multimodal input, handling text, image, video, and audio input, and generating text outputs, with open weights for pre-trained and instruction-tuned variants. These ...",
      "content": "## Release v4.53.0\r\n\r\n### Gemma3n\r\n\r\nGemma 3n models are designed for efficient execution on low-resource devices. They are capable of multimodal input, handling text, image, video, and audio input, and generating text outputs, with open weights for pre-trained and instruction-tuned variants. These models were trained with data in over 140 spoken languages.\r\n\r\nGemma 3n models use selective parameter activation technology to reduce resource requirements. This technique allows the models to operate at an effective size of 2B and 4B parameters, which is lower than the total number of parameters they contain. For more information on Gemma 3n's efficient parameter management technology, see the [Gemma 3n](https://ai.google.dev/gemma/docs/gemma-3n#parameters) page.\r\n\r\n![image](https://github.com/user-attachments/assets/858cb034-364d-4eb6-8de8-4a0b5eaff3d7)\r\n\r\n```python\r\nfrom transformers import pipeline\r\nimport torch\r\n\r\npipe = pipeline(\r\n    \"image-text-to-text\",\r\n    torch_dtype=torch.bfloat16,\r\n    model=\"google/gemma-3n-e4b\",\r\n    device=\"cuda\",\r\n)\r\noutput = pipe(\r\n    \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg\",\r\n    text=\"<image_soft_token> in this image, there is\"\r\n)\r\n\r\nprint(output)\r\n```\r\n\r\n### Dia\r\n\r\n![image](https://github.com/user-attachments/assets/bf86e887-e4f4-4222-993d-f5eac58f8040)\r\n\r\nDia is an opensource text-to-speech (TTS) model (1.6B parameters) developed by [Nari Labs](https://huggingface.co/nari-labs).\r\nIt can generate highly realistic dialogue from transcript including nonverbal communications such as laughter and coughing.\r\nFurthermore, emotion and tone control is also possible via audio conditioning (voice cloning).\r\n\r\n**Model Architecture:**\r\nDia is an encoder-decoder transformer based on the original transformer architecture. However, some more modern features such as\r\nrotational positional embeddings (RoPE) are also included. For its text portion (encoder), a byte tokenizer is utilized while\r\nfor the audio portion (decoder), a pretrained codec model [DAC](./dac.md) is used - DAC encodes speech into discrete codebook\r\ntokens and decodes them back into audio.\r\n\r\n* Add Dia model  by @buttercrab in #38405\r\n\r\n### Kyutai Speech-to-Text\r\n\r\n<img src=\"https://huggingface.co/datasets/eustlb/documentation-images/resolve/main/kyutai_stt.png\"/>\r\n\r\nKyutai STT is a speech-to-text model architecture based on the [Mimi codec](https://huggingface.co/docs/transformers/en/model_doc/mimi), which encodes audio into discrete tokens in a streaming fashion, and a [Moshi-like](https://huggingface.co/docs/transformers/en/model_doc/moshi) autoregressive decoder. Kyutai’s lab has released two model checkpoints:\r\n- [kyutai/stt-1b-en_fr](https://huggingface.co/kyutai/stt-1b-en_fr): a 1B-parameter model capable of transcribing both English and French\r\n- [kyutai/stt-2.6b-en](https://huggingface.co/kyutai/stt-2.6b-en): a 2.6B-parameter model focused solely on English, optimized for maximum transcription accuracy\r\n\r\n* Add kyutai stt  by @eustlb in #38909\r\n\r\nRead more about the model in the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/stt)\r\n\r\n### V-JEPA 2\r\n\r\n<div class=\"flex justify-center\">\r\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/vjepa.gif\" alt=\"drawing\" width=\"600\"/>\r\n</div>\r\n\r\nV-JEPA 2 is a self-supervised approach to training video encoders developed by FAIR, Meta. Using internet-scale video data, V-JEPA 2 attains state-of-the-art performance on motion understanding and human action anticipation tasks. V-JEPA 2-AC is a latent action-conditioned world model post-trained from V-JEPA 2 (using a small amount of robot trajectory interaction data) that solves robot manipulation tasks without environment-specific data collection or task-specific training or calibration.\r\n\r\n* Add V-JEPA 2  by @qubvel in #38746\r\n\r\nRead more about the model in the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/vjepa2).\r\n\r\n### Arcee\r\n\r\n![image](https://github.com/user-attachments/assets/1e7b594b-9973-4a07-b30a-cea0968b081d)\r\n\r\nArcee is a decoder-only transformer model based on the Llama architecture with a key modification: it uses ReLU² (ReLU-squared) activation in the MLP blocks instead of SiLU, following recent research showing improved training efficiency with squared activations. This architecture is designed for efficient training and inference while maintaining the proven stability of the Llama design.\r\n\r\nThe Arcee model is architecturally similar to Llama but uses x * relu(x) in MLP layers for improved gradient flow and is optimized for efficiency in both training and inference scenarios.\r\n\r\n* Add Arcee model support  by @Crystalcareai in #38621\r\n\r\nRead more about the model in the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/arcee#arcee).\r\n\r\n###  ColQwen2\r\n\r\n[ColQwen2](https://doi.org/10.48550/arXiv.2407.01449) is a variant of the [ColPali](./colpali) model designed to retrieve documents by analyzing their visual features. Unlike traditional systems that rely heavily on text extraction and OCR, ColQwen2 treats each page as an image. It uses the [Qwen2-VL](./qwen2_vl) backbone to capture not only text, but also the layout, tables, charts, and other visual elements to create detailed multi-vector embeddings that can be used for retrieval by computing pairwise late interaction similarity scores. This offers a more comprehensive understanding of documents and enables more efficient and accurate retrieval.\r\n\r\n![image](https://github.com/user-attachments/assets/eb833323-675a-4858-9aa9-834d49bcff93)\r\n\r\n* Add ColQwen2 to 🤗 transformers  by @tonywu71 in #35778\r\n\r\nRead more about the model in the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/colqwen2).\r\n\r\n### MiniMax\r\n\r\n![image](https://github.com/user-attachments/assets/0e05053b-bae9-4504-b0b3-f0d7988fe995)\r\n\r\nMiniMax is a powerful language model with 456 billion total parameters, of which 45.9 billion are activated per token. To better unlock the long context capabilities of the model, MiniMax adopts a hybrid architecture that combines Lightning Attention, Softmax Attention and Mixture-of-Experts (MoE). Leveraging advanced parallel strategies and innovative compute-communication overlap methods—such as Linear Attention Sequence Parallelism Plus (LASP+), varlen ring attention, Expert Tensor Parallel (ETP), etc., MiniMax's training context length is extended to 1 million tokens, and it can handle a context of up to 4 million tokens during the inference. On various academic benchmarks, MiniMax also demonstrates the performance of a top-tier model.\r\n\r\nThe architecture of MiniMax is briefly described as follows:\r\n\r\n- Total Parameters: 456B\r\n- Activated Parameters per Token: 45.9B\r\n- Number Layers: 80\r\n- Hybrid Attention: a softmax attention is positioned after every 7 lightning attention.\r\n    - Number of attention heads: 64\r\n    - Attention head dimension: 128\r\n- Mixture of Experts:\r\n    - Number of experts: 32\r\n    - Expert hidden dimension: 9216\r\n    - Top-2 routing strategy\r\n- Positional Encoding: Rotary Position Embedding (RoPE) applied to half of the attention head dimension with a base frequency of 10,000,000\r\n- Hidden Size: 6144\r\n- Vocab Size: 200,064\r\n\r\nFor more details refer to the [release blog post](https://www.minimaxi.com/en/news/minimax-01-series-2).\r\n\r\n* Add support for MiniMax's MiniMax-Text-01  by @geetu040 in #35831\r\n\r\nRead more about the model in the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/minimax).\r\n\r\n### Encoder-Decoder Gemma\r\n\r\n![image](https://github.com/user-attachments/assets/1780e426-435c-47e3-b872-d8b0016648ce)\r\n\r\nT5Gemma (aka encoder-decoder Gemma) was proposed in a [research paper](https://arxiv.org/abs/2504.06225) by Google. It is a family of encoder-decoder large langauge models, developed by adapting pretrained decoder-only models into encoder-decoder. T5Gemma includes pretrained and instruction-tuned variants. The architecture is based on transformer encoder-decoder design following T5, with improvements from Gemma 2: GQA, RoPE, GeGLU activation, RMSNorm, and interleaved local/global attention.\r\n\r\nT5Gemma has two groups of model sizes: 1) [Gemma 2](https://ai.google.dev/gemma/docs/core/model_card_2) sizes (2B-2B, 9B-2B, and 9B-9B), which are based on the offical Gemma 2 models (2B and 9B); and 2) [T5](https://arxiv.org/abs/1910.10683) sizes (Small, Base, Large, and XL), where are pretrained under the Gemma 2 framework following T5 configuration. In addition, we also provide a model at ML size (medium large, ~2B in total), which is in-between T5 Large and T5 XL.\r\n\r\nThe pretrained varaints are trained with two objectives: prefix language modeling with knowledge distillation (PrefixLM) and UL2, separately. We release both variants for each model size. The instruction-turned varaints was post-trained with supervised fine-tuning and reinforcement learning.\r\n\r\n* Encoder-Decoder Gemma  by @bzhangGo in #38332\r\n\r\nRead more about the model in the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/t5gemma).\r\n\r\n### GLM-4.1V\r\n\r\nThe GLM-4.1V model architecture is added to transformers; no models have yet been released with that architecture. Stay tuned for the GLM team upcoming releases!\r\n\r\n* GLM-4.1V Model support  by @zRzRzRzRzRzRzR in #38431\r\n\r\nRead more about the model in the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/glm4v).\r\n\r\n### Falcon H1\r\n\r\n![image](https://github.com/user-attachments/assets/873dd344-2566-408f-8eaf-aab149acabc1)\r\n\r\nThe FalconH1 model was developed by the TII Pretraining team. A comprehensive research paper covering the architecture, pretraining dynamics, experimental results, and conclusions is forthcoming. You can read more about this series in [this website](https://github.com/tiiuae/Falcon-H1).\r\n\r\n* [MODEL] Add Falcon H1  by @younesbelkada in #38249\r\n\r\nRead more about the model in the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/falcon_h1).\r\n\r\n### LightGlue\r\n\r\n![image](https://github.com/user-attachments/assets/45ceebaa-2216-4fcd-9fcc-f05299019c6a)\r\n\r\nThe LightGlue model was proposed in [LightGlue: Local Feature Matching at Light Speed](https://arxiv.org/abs/2306.13643)\r\nby Philipp Lindenberger, Paul-Edouard Sarlin and Marc Pollefeys.\r\n\r\nSimilar to [SuperGlue](https://huggingface.co/magic-leap-community/superglue_outdoor), this model consists of matching\r\ntwo sets of local features extracted from two images, its goal is to be faster than SuperGlue. Paired with the \r\n[SuperPoint model](https://huggingface.co/magic-leap-community/superpoint), it can be used to match two images and \r\nestimate the pose between them. This model is useful for tasks such as image matching, homography estimation, etc.\r\n\r\nThe abstract from the paper is the following:\r\n\r\n*We introduce LightGlue, a deep neural network that learns to match local features across images. We revisit multiple\r\ndesign decisions of SuperGlue, the state of the art in sparse matching, and derive simple but effective improvements. \r\nCumulatively, they make LightGlue more efficient - in terms of both memory and computation, more accurate, and much\r\neasier to train. One key property is that LightGlue is adaptive to the difficulty of the problem: the inference is much\r\nfaster on image pairs that are intuitively easy to match, for example because of a larger visual overlap or limited\r\nappearance change. This opens up exciting prospects for deploying deep matchers in latency-sensitive applications like\r\n3D reconstruction. The code and trained models are publicly available at this [https URL](https://github.com/cvg/LightGlue)*\r\n\r\n* Add LightGlue model  by @sbucaille in #31718\r\n\r\nRead more about the model in the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/lightglue).\r\n\r\n### dots.llm1\r\n\r\nThe abstract from the report is the following:\r\n\r\n*Mixture of Experts (MoE) models have emerged as a promising paradigm for scaling language models efficiently by activating only a subset of parameters for each input token. In this report, we present dots.llm1, a large-scale MoE model that activates 14B parameters out of a total of 142B parameters, delivering performance on par with state-of-the-art models while reducing training and inference costs. Leveraging our meticulously crafted and efficient data processing pipeline, dots.llm1 achieves performance comparable to Qwen2.5-72B after pretraining on high-quality corpus and post-training to fully unlock its capabilities. Notably, no synthetic data is used during pretraining. To foster further research, we open-source intermediate training checkpoints spanning the entire training process, providing valuable insights into the learning dynamics of large language models.*\r\n\r\n* [Model] add dots1  by @redmoe-moutain in #38143\r\n\r\nRead more about the model in the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/dots1).\r\n\r\n### SmolLM3\r\n\r\nSmolLM3 is a fully open, compact language model designed for efficient deployment while maintaining strong performance. It uses a Transformer decoder architecture with Grouped Query Attention (GQA) to reduce the kv cache, and no RoPE, enabling improved performance on long-context tasks. It is trained using a multi-stage training approach on high-quality public datasets across web, code, and math domains. The model is multilingual and supports very large context lengths. The instruct variant is optimized for reasoning and tool use.\r\n\r\n* Add SmolLM3  by @anton-l in #38755\r\n\r\nRead more about the model in the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/smollm3).\r\n\r\n## Performance optimizations\r\n\r\n### Kernels\r\n\r\nIn previous versions, installing the `kernels` library would **automatically activate the custom kernels** added to `transformers`, because the `@use_kernel_forward_from_the_hub` decorator directly swapped out the model’s forward method. This implicit behavior caused several issues for users — including problems with `torch.compile`, non-determinism, and inconsistent outputs.\r\n\r\nTo address this, we've introduced a new **opt-in mechanism** called `kernelize`. You can now enable kernel usage explicitly by passing `use_kernels=True` to `from_pretrained`. The `use_kernel_forward_from_the_hub` decorator now simply stores the kernel name that the user wants to use — and `kernelize` handles the rest under the hood.\r\n\r\n#### Example\r\n\r\n```python\r\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\r\nimport torch\r\n\r\nmodel = AutoModelForCausalLM.from_pretrained(\r\n    \"meta-llama/Llama-3.2-1B-Instruct\",\r\n    torch_dtype=torch.bfloat16,\r\n    device_map=\"cuda\",\r\n    use_kernels=True\r\n)\r\ntokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")\r\n\r\ninput = \"Hello\"\r\ninput_ids = tokenizer(input, return_tensors=\"pt\").to(model.device).input_ids\r\noutput = model.generate(input_ids, max_new_tokens=100)\r\n\r\nprint(tokenizer.decode(output[0], skip_special_tokens=True))\r\n```\r\nMore kernels will be added over time — this will be a collaborative, community-driven effort to make transformers lighter and faster 🤗\r\n\r\n* Add kernelize to transformers  by @MekkCyber in #38205\r\n\r\n### Flash Attention 3\r\n\r\nSupport for Flash Attention 3 is added across the most popular models.\r\n\r\n* Support for Flash Attention 3  by @EduardDurech in #38972\r\n\r\n## Notable repository maintenance & refactors\r\n\r\nSeveral efforts refactoring the repository are happening in parallel. The direction is to greatly simplify the library, removing unnecessary codepaths. Whilst the efforts are spread across the library, they're particularly visible in each individual models; where non-modeling-specific code will be simplified and eventually removed.\r\n\r\nWe take the assumption that model-agnostic utilities shouldn't be in the modeling code. Things like the output of attentions, hidden states, router logits, are important for end-users but don't need to be explicitely displayed in the modeling code. \r\n\r\n* Apply GradientCheckpointingLayer to the whole repo  by @qubvel in #38913\r\n* No more Tuple, List, Dict  by @Rocketknight1 in #38797\r\n* Deprecate TF + JAX  by @Rocketknight1 in #38758\r\n\r\n## Breaking changes\r\n\r\nSeveral minimal breaking changes aiming to bring clearer defaults while greatly simplifying the library have been merged.\r\n\r\n* 🔴 Update default `dtype` for pipelines to `auto`  by @Vaibhavs10 in #38882\r\n* 🚨🚨 Fix initialization of Mask2Former  by @Cyrilvallez in #38864\r\n* :rotating_light: :rotating_light: Inherited CausalLM Tests  by @Rocketknight1 in #37590\r\n* 🚨Early-error🚨 config will error out if `output_attentions=True` and the attn implementation is wrong  by @ArthurZucker in #38288\r\n* 🔴 [VLM] modeling updates  by @zucchini-nlp in #38317\r\n* :rotating_light: :rotating_light: Fix custom code saving  by @Rocketknight1 in #37716\r\n* 🚨🚨[core] Completely rewrite the masking logic for all attentions  by @Cyrilvallez in #37866\r\n* 🔴🔴🔴 [`Attention`] Refactor Attention Interface for Bart-based Models  by @vasqu in #38108\r\n* 🔴[`Attention`] Attention refactor for Whisper-based models  by @vasqu in #38235\r\n* Add CB  by @ArthurZucker in #38085\r\n\r\n## Bugfixes and improvements\r\n\r\n* CI reporting improvements  by @ydshieh in #38230\r\n* Revert parallelism temporarily  by @LysandreJik in #38240\r\n* tp plan should not be NONE  by @ArthurZucker in #38255\r\n* [Falcon H1] Fix Typo in Integration Test  by @dhiaEddineRhaiem in #38256\r\n* [`compile`] re-enable for Qwen-VL models  by @zucchini-nlp in #38127\r\n* fix multi-image case for llava-onevision  by @cyr0930 in #38084\r\n* Add tearDown method to Quark to solve OOM issues  by @MekkCyber in #38234\r\n* Clearer error on import failure  by @LysandreJik in #38257\r\n* [whisper] small changes for faster tests  by @gante in #38236\r\n* Simplify DTensor Check for modeling_utils.py  by @amd-xiaoyu12 in #38245\r\n* Improve typing in TrainingArgument  by @cyyever in #36944\r\n* Fix: missing else branch to handle \"--load_best_model_at_end\" in training_args.py  by @danielyxyang in #38217\r\n* assign the correct torchao data layout for xpu  by @jiqing-feng in #37781\r\n* Remove Japanese sequence_classification doc and update references  by @ritsumei-aoi in #38246\r\n* Protect ParallelInterface  by @ArthurZucker in #38262\r\n* Update Model Card for Mamba  by @ParagEkbote in #37863\r\n* docs(swin): Update Swin model card to standard format  by @BryanBradfo in #37628\r\n* add XPU info print in print_env  by @yao-matrix in #38282\r\n* [whisper] move processor test into processor test file 🧹   by @gante in #38266\r\n* [Whisper] handle deprecation of `forced_decoder_ids`  by @gante in #38232\r\n* add `liger-kernel` to docker file  by @ydshieh in #38292\r\n* Fix tp error when torch distributed is already initialized  by @SunMarc in #38294\r\n* More typing in src/transformers/training_args.py  by @cyyever in #38106\r\n* refine `transformers env` output  by @yao-matrix in #38274\r\n* Update CI Docker base image for AMD tests  by @ahadnagy in #38261\r\n* Fix HybridChunedCache & Llama4  by @Cyrilvallez in #38299\r\n* Oups typo for HybridChunkedCache  by @Cyrilvallez in #38303\r\n* [Tests] Cleanup Janus Testcase  by @yaswanth19 in #38311\r\n* [emu3] fix conversion script  by @zucchini-nlp in #38297\r\n* Fix run_slow  by @cyyever in #38314\r\n* Fix typo: change 'env' to 'environment' in .circleci/config.yml  by @AbdessamadEnabih in #38273\r\n* Adds use_repr to model_addition_debugger_context  by @RyanMullins in #37984\r\n* [tf/flax] handle `forced_decoder_ids` deletion  by @gante in #38316\r\n* [Whisper + beam search] fix usage of `beam_indices`  by @gante in #38259\r\n* Expose AutoModelForTimeSeriesPrediction for import  by @jinan-zhou in #38307\r\n* [custom_generate] don't forward `custom_generate` and `trust_remote_code`  by @gante in #38304\r\n* add `vasqu` to `self-comment-ci.yml`  by @ydshieh in #38324\r\n* Fix some tests (especially compile with fullgraph=True on Python<3.11)  by @Cyrilvallez in #38319\r\n* [performance_optim] reduce frequency of declaring attention_mask in Ascend NPU flash attention  by @FightingZhen in #38278\r\n* refactor can_save_slow_tokenizer  by @itazap in #37722\r\n* [`FlexAttention`] Reenable flex for encoder-decoder and make the test more robust  by @vasqu in #38321\r\n* Enhance Model Loading By Providing Parallelism, Uses Optional Env Flag  by @inf3rnus in #36835\r\n* Use Gradient Checkpointing Layer in Jamba & Blip Related Models  by @alex-jw-brooks in #38310\r\n* Never fallback to eager implicitly  by @Cyrilvallez in #38327\r\n* Remove duplicate docstring: resample  by @qqii in #38305\r\n* Update BioGPT model card  by @Aguedoom in #38214\r\n* docs(swinv2): Update SwinV2 model card to new standard format  by @BryanBradfo in #37942\r\n* [docs]: update roformer.md model card  by @KsuParkhamchuk in #37946\r\n* new failure CI reports for all jobs   by @ydshieh in #38298\r\n* Hot fix for AMD CI workflow  by @ydshieh in #38349\r\n* Uninstall `kernels` for AMD docker images  by @ydshieh in #38354\r\n* [VLMs] add helpers for get/set embedding  by @zucchini-nlp in #38144\r\n* switch to device agnostic device calling for test cases  by @yao-matrix in #38247\r\n* [`OPT`] Fix attention scaling  by @vasqu in #38290\r\n* Fix all import errors based on older torch versions  by @Cyrilvallez in #38370\r\n* Fix incorrect batching audio index calculation for Phi-4-Multimodal   by @Isotr0py in #38103\r\n* Protect `get_default_device` for torch<2.3  by @Cyrilvallez in #38376\r\n* [Falcon H1] Fix slow path forward pass  by @dhiaEddineRhaiem in #38320\r\n* Improved cache docs  by @manueldeprada in #38060\r\n* for now disable compile  by @ArthurZucker in #38383\r\n* Use one `utils/notification_service.py`  by @ydshieh in #38379\r\n* Better check in `initialize_weights`  by @Cyrilvallez in #38382\r\n* fix typos  by @DeVikingMark in #38336\r\n* fix typo: `tokenizer` -> `tokenize`  by @foldl in #38357\r\n* Stop TF weight rename reDOS  by @Rocketknight1 in #38325\r\n* [cli] cli usable without torch  by @gante in #38386\r\n* update gemma tests  by @ydshieh in #38384\r\n* Stop autoconverting custom code checkpoints  by @Rocketknight1 in #37751\r\n* Add AMD MI300 CI caller leveraging self-hosted runner scale set workflow in hf-workflows  by @jitesh-gupta in #38132\r\n* Fix image token mask in Gemma3  by @Cyrilvallez in #38295\r\n* [transformers x vLLM] standardize processors  by @zucchini-nlp in #37915\r\n* [paligemma] fix processor with suffix  by @zucchini-nlp in #38365\r\n* [video utils] group and reorder by number of frames  by @zucchini-nlp in #38374\r\n* [aya vision] fix processor for vLLM  by @zucchini-nlp in #38371\r\n* guard size mismatch check to only quantized models  by @SunMarc in #38397\r\n* [chat] improvements for thinking models and reduce default verbosity  by @gante in #38322\r\n* Fix convert to original state dict for VLMs  by @hiyouga in #38385\r\n* [chat] use the checkpoint's `generation_config.json` as base parameterization  by @gante in #38330\r\n* Fix Qwen2.5-VL Video Processor  by @yeliudev in #38366\r\n* [CSM] infer codec model with no_grad + audio eos label  by @eustlb in #38215\r\n* Add report_repo_id to mi300 workflow  by @ivarflakstad in #38401\r\n* [CSM] update model id  by @eustlb in #38211\r\n* [cleanup] delete deprecated kwargs in qwen2_audio 🧹   by @gante in #38404\r\n* [tests] remove overload for deleted test (`test_offloaded_cache_implementation`)  by @gante in #37896\r\n* [mllama] Allow `pixel_values` with `inputs_embeds`  by @dxoigmn in #38334\r\n* Update Model Card for Mamba-2  by @ParagEkbote in #37951\r\n* Updated Zoedepth model card  by @miniMaddy in #37898\r\n* Updated BigBird Model card as per #36979.  by @RogerSinghChugh in #37959\r\n* Updated BERTweet model card.  by @RogerSinghChugh in #37981\r\n* New bart model card  by @RogerSinghChugh in #37858\r\n* Update granite.md  by @Tanuj-rai in #37791\r\n* Falcon-H1 - Fix auto_docstring and add can_return_tuple decorator  by @yonigozlan in #38260\r\n* Updated model card for OLMo2  by @andyvu923 in #38394\r\n* Add mi300 to amd daily ci workflows definition  by @ivarflakstad in #38415\r\n* Change slack channel for mi250 CI  by @ivarflakstad in #38410\r\n* Fix an error in verify_tp_plan for keys without '.'  by @liwii in #38420\r\n* [qwen-vl] Look for vocab size in text config  by @zucchini-nlp in #38372\r\n* Update `CsmForConditionalGenerationIntegrationTest`  by @ydshieh in #38424\r\n* enable large_gpu and torchao cases on XPU  by @yao-matrix in #38355\r\n* Disable mi210 scheduled CI  by @ivarflakstad in #38411\r\n* Update error when using additional and/or masks  by @Cyrilvallez in #38429\r\n* Fix CircleCI not triggered when PR is opened from a branch of `huggingface/transformers`  by @ydshieh in #38413\r\n* make Llama4TextMoe forward more readable  by @JJJYmmm in #37529\r\n* [core] support tensor-valued _extra_state values in `from_pretrained`  by @pstjohn in #38155\r\n* Fix typo in tokenization_utils_base.py docstring  by @cwngan in #38418\r\n* Fix convert weights for InternVL  by @yonigozlan in #38233\r\n* Trigger doc-builder job after style bot  by @ydshieh in #38398\r\n* Remove redundant test_sdpa_equivalence test  by @Rocketknight1 in #38436\r\n* Fix MoE gradient test  by @Rocketknight1 in #38438\r\n* Fix `from_args_and_dict` ProcessorMixin  by @yonigozlan in #38296\r\n* Fix handling of slow/fast image processors in image_processing_auto.py  by @yonigozlan in #38161\r\n* Updated the Model docs - for the ALIGN model  by @1himan in #38072\r\n* Updated the model card for ViTMAE  by @mreraser in #38302\r\n* Model card for mobilenet v1 and v2  by @yuanjua in #37948\r\n* Merge type hints from `microsoft/python-type-stubs` (post dropping support for Python 3.8)  by @Avasam in #38335\r\n* Fix GLM4 checkpoints  by @ydshieh in #38412\r\n* feat: add cache retention for requests  by @McPatate in #38446\r\n* [Tests] Clean up test cases for few models  by @yaswanth19 in #38315\r\n* Fix TypeError in save_pretrained error handling (fixes #38422)  by @rahulrshetty45 in #38449\r\n* Cleanup `BatchFeature` and `BatchEncoding`  by @lgeiger in #38459\r\n* Fix `Gemma3IntegrationTest`  by @ydshieh in #38471\r\n* [Qwen2.5-Omni] Fix dtype of cos,sin when used with flash attention  by @HarryHsing in #38453\r\n* fix: handle no scheduler passed by user  by @McPatate in #38407\r\n* make it go brrrr  by @ArthurZucker in #38409\r\n* Fix convert_internvl_weights_to_hf.py to support local paths  by @xvyv99 in #38264\r\n* Fix incorrect bbox_embed initialization when decoder_bbox_embed_share=False in GroundingDINO  by @islemyakoubi in #38238\r\n* [Tests] Reduced model size for albert-test model  by @saqlain2204 in #38480\r\n* Align TP check  by @SunMarc in #38328\r\n* protect dtensor import   by @SunMarc in #38496\r\n* [docs] add xpu environment variable for gpu selection  by @faaany in #38194\r\n* Remove deprecated use_flash_attention_2 parameter  by @cyyever in #37131\r\n* Fix setting FLASH_ATTENTION_DETERMINISTIC after importing  by @HollowMan6 in #37185\r\n* [seamless_m4t] Skip some tests when speech is not available  by @remi-or in #38430\r\n* Update Loss Functions to Accept Tensor num_items_in_batch  by @NEREUScode in #38029\r\n* [generate] add soft deprecations on custom generation methods  by @gante in #38406\r\n* [generate] move `SinkCache` to a `custom_generate` repo  by @gante in #38399\r\n* remove unhandled parameter  by @itazap in #38145\r\n* Fix amp deprecation issue  by @SunMarc in #38100\r\n* [flax/mistral] support sliding_window: null in config  by @yiding in #37402\r\n* Num parameters in model.safetensors.index.json  by @LysandreJik in #38531\r\n* Remove type annotation in Siglip Attention Module  by @yaswanth19 in #38503\r\n* Fix `Gemma2IntegrationTest`  by @ydshieh in #38492\r\n* Fix blip2 tests  by @ydshieh in #38510\r\n* [tests] expand flex-attn test for vision models  by @zucchini-nlp in #38434\r\n* Don't use default attn if pre-set in sub-config  by @zucchini-nlp in #38526\r\n* update emu3 test  by @jiqing-feng in #38543\r\n* Update docker image to use `av`  by @ydshieh in #38548\r\n* [bugfix] [WIP] fix apply_rotary_emb error on Ascend NPU  by @FightingZhen in #38491\r\n* [TP] Change command in tests to `python3`  by @S1ro1 in #38555\r\n* Explicitly setting encoding in tokenization_utils_base.py  by @Muqi1029 in #38553\r\n* Fix `utils/notification_service.py`  by @ydshieh in #38556\r\n* Name change AOPermod -> ModuleFqn  by @drisspg in #38456\r\n* Fix hqq issue  by @SunMarc in #38551\r\n* [docs] Format fix  by @stevhliu in #38414\r\n* [janus] Fix failing tests on mi3XX  by @remi-or in #38426\r\n* Fix `chameleon` tests  by @ydshieh in #38565\r\n* update `utils/notification_service.py` for AMD vs Nvidia  by @ydshieh in #38563\r\n* Fix `deepseekv3`  by @ydshieh in #38562\r\n* [`FlexAttn`] Fix models with unique characteristics  by @vasqu in #38433\r\n* fix(attention_visualizer): add default value for image_seq_length  by @IceGiraffe in #38577\r\n* allow custom head_dim for qwen2_moe  by @bzantium in #37188\r\n* Docs: fix code formatting in torchao docs  by @Manalelaidouni in #38504\r\n* feat: add `repository` field to benchmarks table  by @McPatate in #38582\r\n* [Dinov2] Enable device_map=\"auto\" support  by @aryanchauhan31 in #38487\r\n* tests/roformer: fix couple roformer tests on gpus  by @dvrogozh in #38570\r\n* New gpt neo model card  by @RogerSinghChugh in #38505\r\n* Updated deprecated typing imports with equivalents for Python 3.9+  by @Sai-Suraj-27 in #38546\r\n* added fast image processor for ZoeDepth and expanded tests accordingly  by @henrikm11 in #38515\r\n* [qwen-omni] fix sliding window  by @zucchini-nlp in #38525\r\n* Remove custom pytest and pluggy  by @ydshieh in #38589\r\n* pin pandas  by @ydshieh in #38605\r\n* Allow `mlm_probability` to be set to `None` when `mlm=False` in DataCollatorForLanguageModeling  by @KameniAlexNea in #38522) \r\n* Avoid overwrite existing local implementation when loading remote custom model  by @Isotr0py in #38474\r\n* fix spelling errors   by @davidjsonn in #38608\r\n* Remove `isort` from dependencies  by @Sai-Suraj-27 in #38616\r\n* Fix `return_dict=False` giving errors in a few VLM models  by @ydshieh in #38519\r\n* docs: fix dark mode logo display.  by @johncaged in #38586\r\n* Fix typo in LLaVa documentation  by @mynameismon in #38618\r\n* [Nit] Add Note on SigOpt being in Public Archive Mode  by @ParagEkbote in #38610\r\n* Updated Aria model card  by @1himan in #38472\r\n* Fix `MiniMax` (docs and integration tests checkpoint)  by @geetu040 in #38575\r\n* enable more test cases on xpu  by @yao-matrix in #38572\r\n* Improve `test_initialization`  by @ydshieh in #38607\r\n* Use torch 2.7.1 on CircleCI jobs  by @ydshieh in #37856\r\n* [generation] bring back tests on vision models  by @zucchini-nlp in #38603\r\n* update `ColQwen2ModelIntegrationTest`  by @ydshieh in #38583\r\n* Improve `test_initialization` for `SwiftFormer`  by @ydshieh in #38636\r\n* fix: support grad clipping for TP through replicating non-sharded modules  by @kmehant in #36132\r\n* Don't run `AriaForConditionalGenerationModelTest` on CircleCI  by @ydshieh in #38615\r\n* fix total batch size calculation in trainer  by @inkcherry in #38286\r\n* fix torch_dtype on awq  by @jiqing-feng in #38463\r\n* Better CI  by @ydshieh in #38552\r\n* remove ipex_optimize_model usage  by @yao-matrix in #38632\r\n* Skip torchscript tests for 2 models  by @ydshieh in #38643\r\n* Fix `InternVL` integration test  by @ydshieh in #38612\r\n* Use torch 2.7.1 on daily CI  by @ydshieh in #38620\r\n* Fix qwen2-audio chat template audio placeholder insertion  by @Isotr0py in #38640\r\n* Fixed modeling_auto.py MODEL_FOR_MASK_GENERATION_MAPPING_NAMES variable  by @sbucaille in #38664\r\n* fix: \"check out\" as verb  by @DePasqualeOrg in #38678\r\n* Fix attention mask expansion when converting to executorch  by @pweglik in #38637\r\n* Fix some models import  by @nicelulu in #38694\r\n* Fix retrieve function signature and remove faiss requirement  by @Fiona-Waters in #38624\r\n* Fix TypeError: 'NoneType' object is not iterable for esm  by @dbleyl in #38667) \r\n* Docs: update bitsandbytes torch.compile compatibility  by @matthewdouglas in #38651\r\n* Drop as_target_processor from the _call_ and pad methods  by @marcndo in #38642\r\n* Created model card for XLM model  by @AshAnand34 in #38595\r\n* Update XLM-RoBERTa model documentation with enhanced usage examples and improved layout  by @AshAnand34 in #38596\r\n* Created model card for xlm-roberta-xl  by @AshAnand34 in #38597\r\n* Fix `aya_vision` test  by @ydshieh in #38674\r\n* Standardize ByT5 model card format  by @yanamis in #38699\r\n* Fix smart resize  by @rdonggroq in #38706\r\n* Update some tests for torch 2.7.1  by @ydshieh in #38701\r\n* Logging message for ``` is_bitsandbytes_available() ```   by @ved1beta in #38528\r\n* Fix `llava` tests  by @ydshieh in #38722\r\n* Use OSError  by @cyyever in #38712\r\n* [add-new-model-like] Robust search & proper outer '),' in tokenizer mapping  by @alexzms in #38703\r\n* Fix typo in Language Modeling example scripts and update TPU type  by @framoncg in #38652\r\n* Add AGENTS.md  by @Rocketknight1 in #38734\r\n* New canine model card  by @RogerSinghChugh in #38631\r\n* Fixed a multiple-devices issue in SmolVLM model  by @remi-or in #38736\r\n* [llava] fix integration tests with Siglip  by @zucchini-nlp in #38732\r\n* fix: Add method to get image features in PaliGemmaForConditionalGeneration  by @YushunXiang in #38730\r\n* from 1.11.0, torchao.prototype.low_bit_optim is promoted to torchao.optim  by @yao-matrix in #38689\r\n* fix: bf16 with TPU is allowed in configuration  by @yevvonlim in #38670\r\n* [DeepSeek-V3] implement when q_lora_rank is None  by @bzantium in #38743\r\n* Revert \"Trigger doc-builder job after style bot\"  by @ydshieh in #38735\r\n* Add z-loss to Bamba for v2  by @daviswer in #37842\r\n* Better typing for num_items_in_batch  by @SunMarc in #38728\r\n* Prepare for TF+Jax deprecation  by @Rocketknight1 in #38760\r\n* Remove IPEX requirement for bitsandbytes on CPU  by @matthewdouglas in #38594\r\n* Update repo consistency check  by @Rocketknight1 in #38763\r\n* fix(qwen3_moe): pass kwargs to self_attn  by @llllvvuu in #38691\r\n* Update pegasus model card  by @dross20 in #38675\r\n* Make style bot trigger CI after push  by @ydshieh in #38754\r\n* chore(pixtral): emit block attention mask when using flash attention  by @starcatmeow in #38741\r\n* Update altCLIP model card  by @EmileAydar in #38306\r\n* Add Qwen2 MoE model card  by @rileyafox in #38649\r\n* [masking utils] check `None` instead of try/except  by @zucchini-nlp in #38561\r\n* [Hotfix] Fix style bot   by @ydshieh in #38779\r\n* Fix masking utils  by @Cyrilvallez in #38783\r\n* [video processors] support frame sampling within processors  by @zucchini-nlp in #38105\r\n* Skip some export tests on torch 2.7  by @ydshieh in #38677\r\n* Reduce verbosity for `average_tokens_across_devices=True` and `world size = 1`  by @qgallouedec in #38785\r\n* Update PULL_REQUEST_TEMPLATE.md  by @qgallouedec in #38770\r\n* [docs] Add int4wo + 2:4 sparsity example to TorchAO README  by @jcaip in #38592\r\n* Fix `qwen_2_5 omni`  by @ydshieh in #38658\r\n* Fix `llava_onevision` tests  by @ydshieh in #38791\r\n* Reword README in light of model definitions  by @LysandreJik in #38762\r\n* Fix Typos in Comments: \"quantitation\" → \"quantization\", \"averege\" → \"average\"  by @leopardracer in #38766\r\n* Initialize flash attn flag  by @farnasirim in #38768\r\n* Fix `mllama`  by @ydshieh in #38704\r\n* build: :pushpin: Remove upper bound on PyTorch  by @KyleMylonakisProtopia in #38789\r\n* Remove all traces of `low_cpu_mem_usage`  by @Cyrilvallez in #38792\r\n* [Docs] New DiT model card  by @yushi2006 in #38721\r\n* Add missing div in Pegasus model card  by @dross20 in #38773\r\n* Updated moonshine modelcard  by @SohamPrabhu in #38711\r\n* refactor create_token_type_ids_from_sequences  by @itazap in #37681\r\n* [docs] update cache docs with new info  by @zucchini-nlp in #38775\r\n* Fix erroneous docstring for the ordering of SWA layers  by @norpadon in #38794\r\n* Fix configs and doc for the Qwens  by @Cyrilvallez in #38808\r\n* Unbreak optimum-executorch  by @guangy10 in #38646\r\n* Disable custom MRA kernels for ROCm  by @ahadnagy in #38738\r\n* Use HF papers  by @qgallouedec in #38184\r\n* Simplify and update trl examples  by @qgallouedec in #38772\r\n* Better pipeline type hints ✨  by @qubvel in #38049\r\n* Fix `llava_next` tests  by @ydshieh in #38813\r\n* Expectation fixes and added AMD expectations  by @remi-or in #38729\r\n* Use `wandb.run.url` instead of `wandb.run.get_url()` (deprecated)  by @qgallouedec in #38817\r\n* Refactor DBRX tests to use CausalLMModelTest base classes  by @Rocketknight1 in #38475\r\n* change fsdp_strategy to fsdp in TrainingArguments in accelerate doc  by @PT-10 in #38807\r\n* Fix a minor security issue  by @ydshieh in #38815\r\n* Fix trainer.py not showing signature columns  by @nenesekai in #38465\r\n* Add V-JEPA for video classification model  by @qubvel in #38788\r\n* fixed docstring in modular_qwen2_5_vl.py  by @lawrencefeng17 in #38798\r\n* [docs] Update docs moved to the course  by @stevhliu in #38800\r\n* [docs] updated roberta model card  by @allmight05 in #38777\r\n* Updated Albert model Card  by @souvikchand in #37753\r\n* [internvl] fix video inference  by @zucchini-nlp in #38811\r\n* Fix redundant code in Janus  by @yaswanth19 in #38826\r\n* bugfix: propage weight key_mapping to peft to fix 3.52 VLM renaming   by @ManuelFay in #38627\r\n* Fix peft integration  by @Cyrilvallez in #38841\r\n* Fix broken notebooks link in Italian training docs  by @VolodymyrBg in #38834\r\n* Fix broken tag in Longformer model card  by @dross20 in #38828\r\n* [BugFix] QA pipeline edge case: `align_to_words=True` in `QuestionAnsweringPipeline` can lead to duplicate answers  by @yushi2006 in #38761\r\n* GraniteMoeHybrid: Allow for only shared expert case.  by @shawntan in #38801\r\n* Updated aya_vision.md  by @1himan in #38749\r\n* Remove merge conflict artifacts in Albert model doc  by @druvdub in #38849\r\n* [video processor] fix BC when no video config if found  by @zucchini-nlp in #38840\r\n* Fix incorrect width ratio calculation in Llama4 image processor  by @Jingxiang-Zhang in #38842\r\n* Allow customization of sdpa in executorch.py  by @kimishpatel in #38827\r\n* Fix `qwen2_5_vl` tests  by @ydshieh in #38845\r\n* Improve `auxiliary_in_channels` default behavior in UperNet  by @simonreise in #37540\r\n* Fix `qwen3` tests  by @ydshieh in #38862\r\n* Update CvT documentation with improved usage examples and additional …  by @sezan92 in #38731\r\n* Update roc bert docs  by @SohamPrabhu in #38835\r\n* Post-PR fixes!  by @Rocketknight1 in #38868\r\n* enable misc test cases on XPU  by @yao-matrix in #38852\r\n* Fix `phi4_multimodal` tests  by @ydshieh in #38816\r\n* Fix `qwen3_moe` tests  by @ydshieh in #38865\r\n* Fix HQQ model param device transfer issue  by @HighCWu in #38466\r\n* Fixed markdown for BertTokenizer's '[CLS]' token.  by @eu90h in #38506\r\n* null deepspeed_plugin in args for wandb callback fake trainer  by @winglian in #38867\r\n* More PYUP fixes  by @cyyever in #38883\r\n* Fix loop var naming  by @Rocketknight1 in #38885\r\n* [bugfix] fix ATTN_MASK_NPU device mismatch error on multi-device NPU …  by @qykong in #38876\r\n* log: Add logging when using split_batches and per_device_train_batch_size  by @KeshavSingh29 in #38633\r\n* Docs: Add custom fine-tuning tutorial to TrOCR model page  by @Ashutosh-4485 in #38847\r\n* 36978 | Fast image processor for DPT model  by @samrae7 in #37481\r\n* [video processor] fix slow tests  by @zucchini-nlp in #38881\r\n* Update bamba model card  by @druvdub in #38853\r\n* Add support for specifying revisions when pushing to Hub via internal Trainer call  by @IsaacBreen in #36852\r\n* Use `raise from e` in `hub.py` utility  by @Wauplin in #37241\r\n* [phi-4] use mel filters from audio utils  by @eustlb in #36966\r\n* Fix `fsmt` tests  by @ydshieh in #38904\r\n* Fix unnecessary super calls  by @cyyever in #38897\r\n* align xpu's autocast behavior w/ cuda by using device agnostic torch APIs  by @yao-matrix in #38284\r\n* Fix `FalconMambaIntegrationTests`  by @ydshieh in #38566\r\n* Skip sdpa tests if submodule does not support sdpa  by @ivarflakstad in #38907\r\n* Fix ReDOS in tokenizer digit substitution  by @Rocketknight1 in #38844\r\n* feat: Add granite architectures to auto tokenizer name mappings  by @gabe-l-hart in #38802\r\n* Allow make-fixup on main branch, albeit slowly  by @Rocketknight1 in #38892\r\n* feat: add flexible Liger Kernel configuration to TrainingArguments  by @hamza-hcompany in #38911\r\n* Remove deprecated classes in modeling_utils.py  by @Cyrilvallez in #38919\r\n* Skip some tests for now  by @ydshieh in #38931\r\n* Modernbert fixes  by @remi-or in #38912\r\n* add pytorch-xpu Dockerfile  by @yao-matrix in #38875\r\n* Remove `ALL_LAYERNORM_LAYERS`  by @Cyrilvallez in #38922\r\n* [static cache] fix device map per layer in VLMs  by @zucchini-nlp in #38488\r\n* Add kwargs for timm.create_model in TimmWrapper  by @qubvel in #38860\r\n* Pin PyTorch extras for AMD containers  by @ahadnagy in #38941\r\n* Correctly raise error for awq quantization  by @Cyrilvallez in #38945\r\n* Fix more flaky `test_initialization`  by @ydshieh in #38932\r\n* Switch to use A10 progressively  by @ydshieh in #38936\r\n* Fix custom generate from local directory  by @manueldeprada in #38916\r\n* Update blip model card  by @devkade in #38513\r\n* Gaudi3 CI  by @IlyasMoutawwakil in #38790\r\n* Fix DTensor import compatibility for PyTorch < 2.5  by @Benoqtr in #38836\r\n* Fix(informer): Correct tensor shape for input_size=1  by @Flink-ddd in #38856\r\n* [modular] CLI allows positional arguments, and more defaults names for the optional arg  by @Cyrilvallez in #38979\r\n* Remove dead protected imports  by @Cyrilvallez in #38980\r\n* Break tie in Expectations and gemma3 fixes  by @remi-or in #38943\r\n* Add Idefics2/3 and SmolVLM Fast image processors + improvements for fast image processors  by @yonigozlan in #38157\r\n* fix: add __bool__ operator to tokenizer to avoid bloated asserts  by @kallewoof in #38899\r\n* Add support for auto_docstring with model outputs  by @yonigozlan in #38242\r\n* fix `mistral` and `mistral3` tests  by @ydshieh in #38978\r\n* [Feature] Support `is_split_into_words` in the `TokenClassificationPipeline`.  by @yushi2006 in #38818\r\n* Fix `rag`  by @ydshieh in #38585\r\n* [docs] Typos - Single GPU efficient training features  by @casinca in #38964\r\n* [qwen] refactor attentions for vision/audio  by @zucchini-nlp in #38930\r\n* Removing extra space in large command for speech-pretraining example  by @dggaytan in #38705\r\n* [`Attention`] Small fix on output attentions  by @vasqu in #38948\r\n* Fixes for Arcee model  by @Cyrilvallez in #39001\r\n* Added scikit-learn to the example image-classification requirements.txt  by @mylonjones in #37506\r\n* Update attention_visualizer.py  by @Tanuj-rai in #37860\r\n* Skip non-selected experts for qwen3_moe  by @seven-mile in #38133\r\n* Fix undeterministic order in modular dependencies  by @Cyrilvallez in #39005\r\n* Granite speech - minor fixes to support training with the HF trainer  by @avihu111 in #38833\r\n* Fix bugs in DynamicCache  by @tugsbayasgalan in #37880\r\n* Update self-comment-ci.yml user list  by @ivarflakstad in #39014\r\n* Skip sdpa dispatch on flash test due to unsupported head dims  by @ivarflakstad in #39010\r\n* [HPU][Critical Issue Fix] ThreadPool instead of Pool for parallel pre-processing  by @dsmertin in #39002\r\n* Add Hugging Face authentication procedure for IDEs (PyCharm, VS Code,…  by @marcndo in #38954\r\n* [LightGlue] Fixed attribute usage from descriptor_dim to keypoint_detector_descriptor_dim  by @sbucaille in #39021\r\n* Add zero dim tensor check when using flash_attention  by @ranzhejiang in #38280\r\n* Fix graph break in torch.compile when using FA2 with attention_mask=None and batch size > 1  by @efsotr in #37332\r\n* [AutoModelForMaskGeneration] Remove duplicate code  by @NielsRogge in #38622\r\n* [video processor] support torchcodec and decrease cuda memory usage  by @zucchini-nlp in #38880\r\n* Drop unnecessary tokens in GPT2Model generation  by @null-pointer-access in #39016\r\n* Fix the seamless_m4t cannot work on Gaudi  by @yuanwu2017 in #38363\r\n* fix: astronomical loss with ModernBERT when using gradient checkpointing  by @umarbutler in #38982) \r\n* fix gemma3 grad acc  by @SunMarc in #37208\r\n* Remove script datasets in tests  by @lhoestq in #38940\r\n* Fix grammatical error in models documentation  by @marcndo in #39019\r\n* refactor: remove custom BarkLayerNorm  by @eginhard in #39003\r\n* [Kyutai-STT] correct model type + model id  by @eustlb in #39035\r\n* Two ReDOS fixes  by @Rocketknight1 in #39013\r\n* [tests] remove TF tests (uses of `require_tf`)  by @gante in #38944\r\n* Granite speech speedup + model saving bugfix  by @avihu111 in #39028\r\n* Fix Bad Outputs in Fast Path for GraniteMoeHybrid  by @alex-jw-brooks in #39033\r\n\r\n## Significant community contributions\r\n\r\nThe following contributors have made significant changes to the library over the last release:\r\n\r\n* @ydshieh\r\n    * CI reporting improvements (#38230)\r\n    * add `liger-kernel` to docker file (#38292)\r\n    * add `vasqu` to `self-comment-ci.yml` (#38324)\r\n    * new failure CI reports for all jobs  (#38298)\r\n    * Hot fix for AMD CI workflow (#38349)\r\n    * Uninstall `kernels` for AMD docker images (#38354)\r\n    * Use one `utils/notification_service.py` (#38379)\r\n    * update gemma tests (#38384)\r\n    * Update `CsmForConditionalGenerationIntegrationTest` (#38424)\r\n    * Fix CircleCI not triggered when PR is opened from a branch of `huggingface/transformers` (#38413)\r\n    * Trigger doc-builder job after style bot (#38398)\r\n    * Fix GLM4 checkpoints (#38412)\r\n    * Fix `Gemma3IntegrationTest` (#38471)\r\n    * Fix `Gemma2IntegrationTest` (#38492)\r\n    * Fix blip2 tests (#38510)\r\n    * Update docker image to use `av` (#38548)\r\n    * Fix `utils/notification_service.py` (#38556)\r\n    * Fix `chameleon` tests (#38565)\r\n    * update `utils/notification_service.py` for AMD vs Nvidia (#38563)\r\n    * Fix `deepseekv3` (#38562)\r\n    * Remove custom pytest and pluggy (#38589)\r\n    * pin pandas (#38605)\r\n    * Fix `return_dict=False` giving errors in a few VLM models (#38519)\r\n    * Improve `test_initialization` (#38607)\r\n    * Use torch 2.7.1 on CircleCI jobs (#37856)\r\n    * update `ColQwen2ModelIntegrationTest` (#38583)\r\n    * Improve `test_initialization` for `SwiftFormer` (#38636)\r\n    * Don't run `AriaForConditionalGenerationModelTest` on CircleCI (#38615)\r\n    * Better CI (#38552)\r\n    * Skip torchscript tests for 2 models (#38643)\r\n    * Fix `InternVL` integration test (#38612)\r\n    * Use torch 2.7.1 on daily CI (#38620)\r\n    * Fix `aya_vision` test (#38674)\r\n    * Update some tests for torch 2.7.1 (#38701)\r\n    * Fix `llava` tests (#38722)\r\n    * Revert \"Trigger doc-builder job after style bot\" (#38735)\r\n    * Make style bot trigger CI after push (#38754)\r\n    * [Hotfix] Fix style bot  (#38779)\r\n    * Skip some export tests on torch 2.7 (#38677)\r\n    * Fix `qwen_2_5 omni` (#38658)\r\n    * Fix `llava_onevision` tests (#38791)\r\n    * Fix `mllama` (#38704)\r\n    * Fix `llava_next` tests (#38813)\r\n    * Fix a minor security issue (#38815)\r\n    * Fix `qwen2_5_vl` tests (#38845)\r\n    * Fix `qwen3` tests (#38862)\r\n    * Fix `phi4_multimodal` tests (#38816)\r\n    * Fix `qwen3_moe` tests (#38865)\r\n    * Fix `fsmt` tests (#38904)\r\n    * Fix `FalconMambaIntegrationTests` (#38566)\r\n    * Skip some tests for now (#38931)\r\n    * Fix more flaky `test_initialization` (#38932)\r\n    * Switch to use A10 progressively (#38936)\r\n    * fix `mistral` and `mistral3` tests (#38978)\r\n    * Fix `rag` (#38585)\r\n* @ArthurZucker\r\n    * tp plan should not be NONE (#38255)\r\n    * Protect ParallelInterface (#38262)\r\n    * Add CB (#38085)\r\n    * 🚨Early-error🚨 config will error out if `output_attentions=True` and the attn implementation is wrong (#38288)\r\n    * for now disable compile (#38383)\r\n    * make it go brrrr (#38409)\r\n* @younesbelkada\r\n    * [MODEL] Add Falcon H1 (#38249)\r\n* @cyr0930\r\n    * fix multi-image case for llava-onevision (#38084)\r\n* @cyyever\r\n    * Improve typing in TrainingArgument (#36944)\r\n    * More typing in src/transformers/training_args.py (#38106)\r\n    * Fix run_slow (#38314)\r\n    * Remove deprecated use_flash_attention_2 parameter (#37131)\r\n    * Use OSError (#38712)\r\n    * More PYUP fixes (#38883)\r\n    * Fix unnecessary super calls (#38897)\r\n* @ritsumei-aoi\r\n    * Remove Japanese sequence_classification doc and update references (#38246)\r\n* @yao-matrix\r\n    * add XPU info print in print_env (#38282)\r\n    * refine `transformers env` output (#38274)\r\n    * switch to device agnostic device calling for test cases (#38247)\r\n    * enable large_gpu and torchao cases on XPU (#38355)\r\n    * enable more test cases on xpu (#38572)\r\n    * remove ipex_optimize_model usage (#38632)\r\n    * from 1.11.0, torchao.prototype.low_bit_optim is promoted to torchao.optim (#38689)\r\n    * enable misc test cases on XPU (#38852)\r\n    * align xpu's autocast behavior w/ cuda by using device agnostic torch APIs (#38284)\r\n    * add pytorch-xpu Dockerfile (#38875)\r\n* @vasqu\r\n    * 🔴🔴🔴 [`Attention`] Refactor Attention Interface for Bart-based Models (#38108)\r\n    * [`FlexAttention`] Reenable flex for encoder-decoder and make the test more robust (#38321)\r\n    * [`OPT`] Fix attention scaling (#38290)\r\n    * 🔴[`Attention`] Attention refactor for Whisper-based models (#38235)\r\n    * [`FlexAttn`] Fix models with unique characteristics (#38433)\r\n    * [`Attention`] Small fix on output attentions (#38948)\r\n* @itazap\r\n    * refactor can_save_slow_tokenizer (#37722)\r\n    * remove unhandled parameter (#38145)\r\n    * refactor create_token_type_ids_from_sequences (#37681)\r\n* @eustlb\r\n    * [CSM] infer codec model with no_grad + audio eos label (#38215)\r\n    * [CSM] update model id (#38211)\r\n    * [phi-4] use mel filters from audio utils (#36966)\r\n    * Add kyutai stt (#38909)\r\n    * [Kyutai-STT] correct model type + model id (#39035)\r\n* @RogerSinghChugh\r\n    * Updated BigBird Model card as per #36979. (#37959)\r\n    * Updated BERTweet model card. (#37981)\r\n    * New bart model card (#37858)\r\n    * New gpt neo model card (#38505)\r\n    * New canine model card (#38631)\r\n* @1himan\r\n    * Updated the Model docs - for the ALIGN model (#38072)\r\n    * Updated Aria model card (#38472)\r\n    * Updated aya_vision.md (#38749)\r\n* @Avasam\r\n    * Merge type hints from `microsoft/python-type-stubs` (post dropping support for Python 3.8) (#38335)\r\n* @remi-or\r\n    * [seamless_m4t] Skip some tests when speech is not available (#38430)\r\n    * [janus] Fix failing tests on mi3XX (#38426)\r\n    * Fixed a multiple-devices issue in SmolVLM model (#38736)\r\n    * Expectation fixes and added AMD expectations (#38729)\r\n    * Modernbert fixes (#38912)\r\n    * Break tie in Expectations and gemma3 fixes (#38943)\r\n* @tonywu71\r\n    * Add ColQwen2 to 🤗 transformers (#35778)\r\n* @geetu040\r\n    * Add support for MiniMax's MiniMax-Text-01 (#35831)\r\n    * Fix `MiniMax` (docs and integration tests checkpoint) (#38575)\r\n* @sbucaille\r\n    * Fixed modeling_auto.py MODEL_FOR_MASK_GENERATION_MAPPING_NAMES variable (#38664)\r\n    * Add LightGlue model (#31718)\r\n    * [LightGlue] Fixed attribute usage from descriptor_dim to keypoint_detector_descriptor_dim (#39021)\r\n* @samrae7\r\n    * 36978 | Fast image processor for DPT model (#37481)\r\n* @Crystalcareai\r\n    * Add Arcee model support (#38621)\r\n* @zRzRzRzRzRzRzR\r\n    * GLM-4.1V Model support (#38431)\r\n* @bzhangGo\r\n    * Encoder-Decoder Gemma (#38332)\r\n* @redmoe-moutain\r\n    * [Model] add dots1 (#38143)\r\n* @EduardDurech\r\n    * Support for Flash Attention 3 (#38972)\r\n",
      "publishedAt": "2025-06-26T16:07:22.000Z",
      "source": "Hugging Face GitHub",
      "sourceUrl": "https://github.com/huggingface/transformers/releases/tag/v4.53.0",
      "category": "tools",
      "company": "Hugging Face",
      "imageUrl": null,
      "tags": [
        "GPT",
        "Llama",
        "LLM",
        "AI",
        "ML"
      ],
      "featured": false
    },
    {
      "id": "mcif4xukej3z1m8jgrb",
      "title": "Anthropic: claude-codeの最新アップデート",
      "summary": "claude-codeリポジトリに新しい更新: chore: Update CHANGELOG.md...",
      "content": "chore: Update CHANGELOG.md",
      "publishedAt": "2025-06-25T21:45:26.000Z",
      "source": "Anthropic GitHub",
      "sourceUrl": "https://github.com/anthropics/claude-code",
      "category": "tools",
      "company": "Anthropic",
      "imageUrl": null,
      "tags": [
        "Claude"
      ],
      "featured": false
    },
    {
      "id": "mcif51i0tivqipvhjdk",
      "title": "Google: cookbookの最新アップデート",
      "summary": "cookbookリポジトリに新しい更新: Updating the Imagen notebooks for Imagen 4 models (#814)...",
      "content": "Updating the Imagen notebooks for Imagen 4 models (#814)",
      "publishedAt": "2025-06-24T23:19:46.000Z",
      "source": "Google GitHub",
      "sourceUrl": "https://github.com/google-gemini/cookbook",
      "category": "tools",
      "company": "Google",
      "imageUrl": null,
      "tags": [],
      "featured": false
    },
    {
      "id": "mcif5449evgjqvk272s",
      "title": "Hugging Face: Kyutai-STT (based on v4.52.4)",
      "summary": "A new model is added to transformers: Kyutai-STT\r\nIt is added on top of the v4.52.4 release, and can be installed from the following tag: `v4.52.4-Kyutai-STT-preview`.\r\n\r\nIn order to install this version, please install with the following command:\r\n\r\n```\r\npip install git+https://github.com/huggingfa...",
      "content": "A new model is added to transformers: Kyutai-STT\r\nIt is added on top of the v4.52.4 release, and can be installed from the following tag: `v4.52.4-Kyutai-STT-preview`.\r\n\r\nIn order to install this version, please install with the following command:\r\n\r\n```\r\npip install git+https://github.com/huggingface/transformers@v4.52.4-Kyutai-STT-preview\r\n```\r\n\r\nIf fixes are needed, they will be applied to this release; this installation may therefore be considered as stable and improving.\r\n\r\nAs the tag implies, this tag is a **_preview_** of the Kyutai-STT model. This tag is a tagged version of the `main` branch and does not follow semantic versioning. This model will be included in the next minor release: `v4.53.0`.\r\n\r\n## Kyutai-STT\r\n\r\n<img src=\"https://huggingface.co/datasets/eustlb/documentation-images/resolve/main/kyutai_stt.png\"/>\r\n\r\nKyutai STT is a speech-to-text model architecture based on the [Mimi codec](https://huggingface.co/docs/transformers/en/model_doc/mimi), which encodes audio into discrete tokens in a streaming fashion, and a [Moshi-like](https://huggingface.co/docs/transformers/en/model_doc/moshi) autoregressive decoder. Kyutai’s lab has released two model checkpoints:\r\n- [kyutai/stt-1b-en_fr](https://huggingface.co/kyutai/stt-1b-en_fr): a 1B-parameter model capable of transcribing both English and French\r\n- [kyutai/stt-2.6b-en](https://huggingface.co/kyutai/stt-2.6b-en): a 2.6B-parameter model focused solely on English, optimized for maximum transcription accuracy\r\n\r\n\r\n## Usage example\r\n\r\nKyutai-STT can be found on the [Huggingface Hub](https://huggingface.co/models?other=stt).\r\n\r\n### Inference\r\n\r\n```python\r\nimport torch\r\nfrom datasets import load_dataset, Audio\r\nfrom transformers import KyutaiSpeechToTextProcessor, KyutaiSpeechToTextForConditionalGeneration\r\n\r\n# 1. load the model and the processor\r\ntorch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\r\nmodel_id = \"kyutai/stt-2.6b-en\"\r\n\r\nprocessor = KyutaiSpeechToTextProcessor.from_pretrained(model_id)\r\nmodel = KyutaiSpeechToTextForConditionalGeneration.from_pretrained(model_id, device_map=torch_device)\r\n\r\n# 2. load audio samples\r\nds = load_dataset(\r\n    \"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\"\r\n)\r\nds = ds.cast_column(\"audio\", Audio(sampling_rate=24000))\r\n\r\n# 3. prepare the model inputs\r\ninputs = processor(\r\n    ds[0][\"audio\"][\"array\"],\r\n)\r\ninputs.to(torch_device)\r\n\r\n# 4. infer the model\r\noutput_tokens = model.generate(**inputs)\r\n\r\n# 5. decode the generated tokens\r\nprint(processor.batch_decode(output_tokens, skip_special_tokens=True))\r\n```\r\n\r\n### Batched Inference\r\n\r\n```python\r\nimport torch\r\nfrom datasets import load_dataset, Audio\r\nfrom transformers import KyutaiSpeechToTextProcessor, KyutaiSpeechToTextForConditionalGeneration\r\n\r\n# 1. load the model and the processor\r\ntorch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\r\nmodel_id = \"kyutai/stt-2.6b-en\"\r\n\r\nprocessor = KyutaiSpeechToTextProcessor.from_pretrained(model_id)\r\nmodel = KyutaiSpeechToTextForConditionalGeneration.from_pretrained(model_id, device_map=torch_device)\r\n\r\n# 2. load audio samples\r\nds = load_dataset(\r\n    \"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\"\r\n)\r\nds = ds.cast_column(\"audio\", Audio(sampling_rate=24000))\r\n\r\n# 3. prepare the model inputs\r\naudio_arrays = [ds[i][\"audio\"][\"array\"] for i in range(4)]\r\ninputs = processor(audio_arrays, return_tensors=\"pt\", padding=True)\r\ninputs = inputs.to(torch_device)\r\n\r\n# 4. infer the model\r\noutput_tokens = model.generate(**inputs)\r\n\r\n# 5. decode the generated tokens\r\ndecoded_outputs = processor.batch_decode(output_tokens, skip_special_tokens=True)\r\nfor output in decoded_outputs:\r\n    print(output)\r\n```",
      "publishedAt": "2025-06-24T16:05:00.000Z",
      "source": "Hugging Face GitHub",
      "sourceUrl": "https://github.com/huggingface/transformers/releases/tag/v4.52.4-Kyutai-STT-preview",
      "category": "tools",
      "company": "Hugging Face",
      "imageUrl": null,
      "tags": [
        "AI",
        "Transformer"
      ],
      "featured": false
    },
    {
      "id": "mcif4wxslf212phiiwm",
      "title": "Anthropic: v0.55.0",
      "summary": "## 0.55.0 (2025-06-23)\n\nFull Changelog: [v0.54.0...v0.55.0](https://github.com/anthropics/anthropic-sdk-python/compare/v0.54.0...v0.55.0)\n\n### Features\n\n* **api:** api update ([4b2134e](https://github.com/anthropics/anthropic-sdk-python/commit/4b2134e5ec3fecab7c56f483b8db87b403a08e05))\n* **api:** ap...",
      "content": "## 0.55.0 (2025-06-23)\n\nFull Changelog: [v0.54.0...v0.55.0](https://github.com/anthropics/anthropic-sdk-python/compare/v0.54.0...v0.55.0)\n\n### Features\n\n* **api:** api update ([4b2134e](https://github.com/anthropics/anthropic-sdk-python/commit/4b2134e5ec3fecab7c56f483b8db87b403a08e05))\n* **api:** api update ([2093bff](https://github.com/anthropics/anthropic-sdk-python/commit/2093bfff2a6c25573eaa2a4667f1e1d0e2d89e24))\n* **api:** manual updates ([c80fda8](https://github.com/anthropics/anthropic-sdk-python/commit/c80fda8cbd157fbbd23895d034cc7bb7a7614569))\n* **client:** add support for aiohttp ([3b03295](https://github.com/anthropics/anthropic-sdk-python/commit/3b03295f15a02ba629d1bdc77e330c2e6043b83e))\n\n\n### Bug Fixes\n\n* **client:** correctly parse binary response | stream ([d93817d](https://github.com/anthropics/anthropic-sdk-python/commit/d93817d9d761bd5e16b35f3c2973122a9c122240))\n* **internal:** revert unintentional changes ([bb3beab](https://github.com/anthropics/anthropic-sdk-python/commit/bb3beab10668be177d6bb573607ef6951a238b24))\n* **tests:** fix: tests which call HTTP endpoints directly with the example parameters ([ee69d74](https://github.com/anthropics/anthropic-sdk-python/commit/ee69d74cc40f749280a29afb12420c117d08ef34))\n* **tests:** suppress warnings in tests when running on the latest Python versions ([#982](https://github.com/anthropics/anthropic-sdk-python/issues/982)) ([740da21](https://github.com/anthropics/anthropic-sdk-python/commit/740da21b563c6ffe7618edf1dcd658bb894b2edf))\n\n\n### Chores\n\n* **ci:** enable for pull requests ([08f2dd2](https://github.com/anthropics/anthropic-sdk-python/commit/08f2dd2bd28958c08a3c82fcf00a0fc7d4e2807c))\n* **internal:** update conftest.py ([1174a62](https://github.com/anthropics/anthropic-sdk-python/commit/1174a6214624ff8cd64edb121d4ff09e9af6b717))\n* **internal:** version bump ([7241eaa](https://github.com/anthropics/anthropic-sdk-python/commit/7241eaa25b6f40bb55f61e766a996a3a18a53a02))\n* **readme:** update badges ([00661c2](https://github.com/anthropics/anthropic-sdk-python/commit/00661c275e120314f76bbd480c0267383e992638))\n* **tests:** add tests for httpx client instantiation & proxies ([b831d88](https://github.com/anthropics/anthropic-sdk-python/commit/b831d8833010c629143041b4b385929ca9c2198d))\n* **tests:** run tests in parallel ([4b24a79](https://github.com/anthropics/anthropic-sdk-python/commit/4b24a791b76c2176de1f35118901da533a10b991))\n\n\n### Documentation\n\n* **client:** fix httpx.Timeout documentation reference ([b0138b1](https://github.com/anthropics/anthropic-sdk-python/commit/b0138b1b2af3c73e568659c7e717fc955eb976b0))",
      "publishedAt": "2025-06-23T18:51:59.000Z",
      "source": "Anthropic GitHub",
      "sourceUrl": "https://github.com/anthropics/anthropic-sdk-python/releases/tag/v0.55.0",
      "category": "tools",
      "company": "Anthropic",
      "imageUrl": null,
      "tags": [
        "AI",
        "Anthropic"
      ],
      "featured": true
    },
    {
      "id": "mcif5d3nh7a3z6kmrfp",
      "title": "Show HN: Claude Slash Command Suite inspired by Anthropics best practices guide",
      "summary": "I built a collection of professional slash commands for Anthropic&#x27;s Claude Code that provide structured workflows for common software development tasks.<p><pre><code>  These commands are directly inspired by and adapted from Anthropic&#x27;s own claude-code-best-practices (https:&#x2F;&#x2F;www...",
      "content": "I built a collection of professional slash commands for Anthropic&#x27;s Claude Code that provide structured workflows for common software development tasks.<p><pre><code>  These commands are directly inspired by and adapted from Anthropic&#x27;s own claude-code-best-practices (https:&#x2F;&#x2F;www.anthropic.com&#x2F;engineering&#x2F;claude-code-best-practices) documentation, translating their recommendations into executable workflows.\n\n  The suite includes commands for:\n  • Comprehensive code reviews with security and performance analysis\n  • End-to-end feature development with planning, implementation, and testing\n  • Architectural analysis and design pattern assessment\n • Security audits and vulnerability scanning\n  • GitHub issue resolution with root cause analysis\n  • Performance optimization and build improvements\n\n  Each command follows a systematic approach based on Anthropic&#x27;s\n  best practices, breaking complex tasks into manageable steps.\n  Instead of ad-hoc AI interactions, you get consistent, thorough\n   workflows that adapt to any codebase.\n\n  The commands work through Claude Code&#x27;s slash command system -\n  just type `&#x2F;project:code-review` or `&#x2F;project:create-feature\n  user-authentication` and Claude follows the predefined\n  workflow.\n\n  Installation is straightforward with an interactive script that\n   can install project-specific or globally. The commands are\n  fully customizable markdown files, so you can adapt them to\n  your team&#x27;s specific requirements.\n\n</code></pre>\nI hope others find it useful!",
      "publishedAt": "2025-06-13T02:05:24.000Z",
      "source": "Hacker News Claude/Anthropic",
      "sourceUrl": "https://github.com/qdhenry/Claude-Command-Suite",
      "category": "companies",
      "company": "Anthropic",
      "imageUrl": null,
      "tags": [
        "Claude",
        "AI",
        "Anthropic"
      ],
      "featured": true
    },
    {
      "id": "mcif4wxsixsedj9pvyh",
      "title": "Anthropic: v0.54.0",
      "summary": "## 0.54.0 (2025-06-10)\n\nFull Changelog: [v0.53.0...v0.54.0](https://github.com/anthropics/anthropic-sdk-python/compare/v0.53.0...v0.54.0)\n\n### Features\n\n* **client:** add support for fine-grained-tool-streaming-2025-05-14 ([07ec081](https://github.com/anthropics/anthropic-sdk-python/commit/07ec08119...",
      "content": "## 0.54.0 (2025-06-10)\n\nFull Changelog: [v0.53.0...v0.54.0](https://github.com/anthropics/anthropic-sdk-python/compare/v0.53.0...v0.54.0)\n\n### Features\n\n* **client:** add support for fine-grained-tool-streaming-2025-05-14 ([07ec081](https://github.com/anthropics/anthropic-sdk-python/commit/07ec08119dbc328934fea5ec6eacd00c8dbda089))\n\n\n### Bug Fixes\n\n* **httpx:** resolve conflict between default transport and proxy settings ([#969](https://github.com/anthropics/anthropic-sdk-python/issues/969)) ([a6efded](https://github.com/anthropics/anthropic-sdk-python/commit/a6efdedcfef881ae3466bb77d92d0338c8338e20))\n* **tests:** update test ([99c2433](https://github.com/anthropics/anthropic-sdk-python/commit/99c243363e94f5f3f627cb8b80e3f238503c89f5))\n\n\n### Chores\n\n* **internal:** version bump ([45029f4](https://github.com/anthropics/anthropic-sdk-python/commit/45029f41c96f62f26ead99a5989c9ad974fc21b9))\n\n\n### Documentation\n\n* **contributing:** fix uv script for bootstrapping ([d2bde52](https://github.com/anthropics/anthropic-sdk-python/commit/d2bde52286ee8fa65995e73c579a8962087c1da4))",
      "publishedAt": "2025-06-11T02:45:52.000Z",
      "source": "Anthropic GitHub",
      "sourceUrl": "https://github.com/anthropics/anthropic-sdk-python/releases/tag/v0.54.0",
      "category": "tools",
      "company": "Anthropic",
      "imageUrl": null,
      "tags": [
        "AI",
        "Anthropic"
      ],
      "featured": false
    },
    {
      "id": "mcif4zkyppfe8cyh2kc",
      "title": "Meta: v0.2.0",
      "summary": "Llama 4 Support ( https://www.llama.com ) \r\n\r\n...",
      "content": "Llama 4 Support ( https://www.llama.com ) \r\n\r\n",
      "publishedAt": "2025-04-05T19:02:56.000Z",
      "source": "Meta GitHub",
      "sourceUrl": "https://github.com/meta-llama/llama-models/releases/tag/v0.2.0",
      "category": "research",
      "company": "Meta",
      "imageUrl": null,
      "tags": [
        "Llama"
      ],
      "featured": false
    },
    {
      "id": "mcif4zkyjqw3ze6azuh",
      "title": "Meta: v0.1.4",
      "summary": "## What's Changed\r\n* fix: do not use python_tag when encoding non-code_interpreter tool_calls by @ehhuang in https://github.com/meta-llama/llama-models/pull/283\r\n* fix: tool_call was not encoded by @ehhuang in https://github.com/meta-llama/llama-models/pull/284\r\n\r\n\r\n**Full Changelog**: https://githu...",
      "content": "## What's Changed\r\n* fix: do not use python_tag when encoding non-code_interpreter tool_calls by @ehhuang in https://github.com/meta-llama/llama-models/pull/283\r\n* fix: tool_call was not encoded by @ehhuang in https://github.com/meta-llama/llama-models/pull/284\r\n\r\n\r\n**Full Changelog**: https://github.com/meta-llama/llama-models/compare/v0.1.3...v0.1.4",
      "publishedAt": "2025-02-25T00:04:44.000Z",
      "source": "Meta GitHub",
      "sourceUrl": "https://github.com/meta-llama/llama-models/releases/tag/v0.1.4",
      "category": "research",
      "company": "Meta",
      "imageUrl": null,
      "tags": [
        "Llama",
        "Meta"
      ],
      "featured": false
    }
  ],
  "featuredCount": 8
}