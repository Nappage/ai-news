{
  "id": "mccackdqcj526zqwgwi",
  "title": "Inferring Reward Functions from Demonstrators with Unknown Biases",
  "summary": "Our goal is to infer reward functions from demonstrations. In order to infer the correct reward function, we must account for the systematic ways in which the demonstrator is suboptimal. Prior work in inverse reinforcement learning can account for specific, known biases, but cannot handle demonstrat...",
  "content": "Our goal is to infer reward functions from demonstrations. In order to infer the correct reward function, we must account for the systematic ways in which the demonstrator is suboptimal. Prior work in inverse reinforcement learning can account for specific, known biases, but cannot handle demonstrators with unknown biases. In this work, we explore the idea of learning the demonstrator's planning algorithm (including their unknown biases), along with their reward function. What makes this challenging is that any demonstration could be explained either by positing a term in the reward function, or by positing a particular systematic bias. We explore what assumptions are sufficient for avoiding this impossibility result: either access to tasks with known rewards which enable estimating the planner separately, or that the demonstrator is sufficiently close to optimal that this can serve as a regularizer. In our exploration with synthetic models of human biases, we find that it is possible to adapt to different biases and perform better than assuming a fixed model of the demonstrator, such as Boltzmann rationality.",
  "publishedAt": "1970-01-01T00:00:00.000Z",
  "source": "Papers with Code",
  "sourceUrl": "https://openreview.net/pdf?id=rkgqCiRqKQ",
  "category": "research",
  "company": "Community",
  "imageUrl": null,
  "tags": [
    "AI"
  ],
  "featured": false
}