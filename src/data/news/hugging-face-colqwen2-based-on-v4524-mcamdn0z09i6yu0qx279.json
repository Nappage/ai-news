{
  "id": "mcamdn0z09i6yu0qx279",
  "title": "Hugging Face: ColQwen2 (based on v4.52.4)",
  "summary": "A new model is added to transformers: ColQwen2\r\nIt is added on top of the v4.52.4 release, and can be installed from the following tag: `v4.52.4-ColQwen2-preview`.\r\n\r\nIn order to install this version, please install with the following command:\r\n\r\n```\r\npip install git+https://github.com/huggingface/t...",
  "content": "A new model is added to transformers: ColQwen2\r\nIt is added on top of the v4.52.4 release, and can be installed from the following tag: `v4.52.4-ColQwen2-preview`.\r\n\r\nIn order to install this version, please install with the following command:\r\n\r\n```\r\npip install git+https://github.com/huggingface/transformers@v4.52.4-ColQwen2-preview\r\n```\r\n\r\nIf fixes are needed, they will be applied to this release; this installation may therefore be considered as stable and improving.\r\n\r\nAs the tag implies, this tag is a **_preview_** of the ColQwen2 model. This tag is a tagged version of the `main` branch and does not follow semantic versioning. This model will be included in the next minor release: `v4.53.0`.\r\n\r\n## ColQwen2\r\n\r\n![image](https://github.com/user-attachments/assets/2e92bb68-eb97-4532-b5ae-4fa378a1ee01)\r\n\r\n[ColQwen2](https://doi.org/10.48550/arXiv.2407.01449) is a variant of the [ColPali](https://github.com/huggingface/transformers/blob/c72ba6944171e2e6dd4f4a93d61b24fa52b718f5/docs/source/en/model_doc/colpali) model designed to retrieve documents by analyzing their visual features. Unlike traditional systems that rely heavily on text extraction and OCR, ColQwen2 treats each page as an image. It uses the [Qwen2-VL](https://github.com/huggingface/transformers/blob/c72ba6944171e2e6dd4f4a93d61b24fa52b718f5/docs/source/en/model_doc/qwen2_vl) backbone to capture not only text, but also the layout, tables, charts, and other visual elements to create detailed multi-vector embeddings that can be used for retrieval by computing pairwise late interaction similarity scores. This offers a more comprehensive understanding of documents and enables more efficient and accurate retrieval.\r\n\r\n\r\n## Usage example\r\n\r\nColQwen2 can be found on the [Huggingface Hub](https://huggingface.co/models?other=colqwen2).\r\n\r\n```python\r\nimport requests\r\nimport torch\r\nfrom PIL import Image\r\n\r\nfrom transformers import ColQwen2ForRetrieval, ColQwen2Processor\r\nfrom transformers.utils.import_utils import is_flash_attn_2_available\r\n\r\n# Load the model and the processor\r\nmodel_name = \"vidore/colqwen2-v1.0-hf\"\r\n\r\nmodel = ColQwen2ForRetrieval.from_pretrained(\r\n    model_name,\r\n    torch_dtype=torch.bfloat16,\r\n    device_map=\"auto\",  # \"cpu\", \"cuda\", or \"mps\" for Apple Silicon\r\n    attn_implementation=\"flash_attention_2\" if is_flash_attn_2_available() else \"sdpa\",\r\n)\r\nprocessor = ColQwen2Processor.from_pretrained(model_name)\r\n\r\n# The document page screenshots from your corpus\r\nurl1 = \"https://upload.wikimedia.org/wikipedia/commons/8/89/US-original-Declaration-1776.jpg\"\r\nurl2 = \"https://upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Romeoandjuliet1597.jpg/500px-Romeoandjuliet1597.jpg\"\r\n\r\nimages = [\r\n    Image.open(requests.get(url1, stream=True).raw),\r\n    Image.open(requests.get(url2, stream=True).raw),\r\n]\r\n\r\n# The queries you want to retrieve documents for\r\nqueries = [\r\n    \"When was the United States Declaration of Independence proclaimed?\",\r\n    \"Who printed the edition of Romeo and Juliet?\",\r\n]\r\n\r\n# Process the inputs\r\ninputs_images = processor(images=images).to(model.device)\r\ninputs_text = processor(text=queries).to(model.device)\r\n\r\n# Forward pass\r\nwith torch.no_grad():\r\n    image_embeddings = model(**inputs_images).embeddings\r\n    query_embeddings = model(**inputs_text).embeddings\r\n\r\n# Score the queries against the images\r\nscores = processor.score_retrieval(query_embeddings, image_embeddings)\r\n\r\nprint(\"Retrieval scores (query x image):\")\r\nprint(scores)\r\n```\r\n\r\nIf you have issue with loading the images with PIL, you can use the following code to create dummy images:\r\n\r\n```python\r\nimages = [\r\n    Image.new(\"RGB\", (128, 128), color=\"white\"),\r\n    Image.new(\"RGB\", (64, 32), color=\"black\"),\r\n]\r\n```\r\n\r\nQuantization reduces the memory burden of large models by representing the weights in a lower precision. Refer to the [Quantization](../quantization/overview) overview for more available quantization backends.\r\n\r\nThe example below uses [bitsandbytes](../quantization/bitsandbytes.md) to quantize the weights to int4.\r\n\r\n```python\r\nimport requests\r\nimport torch\r\nfrom PIL import Image\r\n\r\nfrom transformers import BitsAndBytesConfig, ColQwen2ForRetrieval, ColQwen2Processor\r\n\r\nmodel_name = \"vidore/colqwen2-v1.0-hf\"\r\n\r\n# 4-bit quantization configuration\r\nbnb_config = BitsAndBytesConfig(\r\n    load_in_4bit=True,\r\n    bnb_4bit_use_double_quant=True,\r\n    bnb_4bit_quant_type=\"nf4\",\r\n    bnb_4bit_compute_dtype=torch.float16,\r\n)\r\n\r\nmodel = ColQwen2ForRetrieval.from_pretrained(\r\n    model_name,\r\n    quantization_config=bnb_config,\r\n    device_map=\"cuda\",\r\n).eval()\r\n\r\nprocessor = ColQwen2Processor.from_pretrained(model_name)\r\n\r\nurl1 = \"https://upload.wikimedia.org/wikipedia/commons/8/89/US-original-Declaration-1776.jpg\"\r\nurl2 = \"https://upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Romeoandjuliet1597.jpg/500px-Romeoandjuliet1597.jpg\"\r\n\r\nimages = [\r\n    Image.open(requests.get(url1, stream=True).raw),\r\n    Image.open(requests.get(url2, stream=True).raw),\r\n]\r\n\r\nqueries = [\r\n    \"When was the United States Declaration of Independence proclaimed?\",\r\n    \"Who printed the edition of Romeo and Juliet?\",\r\n]\r\n\r\n# Process the inputs\r\ninputs_images = processor(images=images, return_tensors=\"pt\").to(model.device)\r\ninputs_text = processor(text=queries, return_tensors=\"pt\").to(model.device)\r\n\r\n# Forward pass\r\nwith torch.no_grad():\r\n    image_embeddings = model(**inputs_images).embeddings\r\n    query_embeddings = model(**inputs_text).embeddings\r\n\r\n# Score the queries against the images\r\nscores = processor.score_retrieval(query_embeddings, image_embeddings)\r\n\r\nprint(\"Retrieval scores (query x image):\")\r\nprint(scores)\r\n```",
  "publishedAt": "2025-06-02T13:07:17.000Z",
  "source": "Hugging Face GitHub",
  "sourceUrl": "https://github.com/huggingface/transformers/releases/tag/v4.52.4-ColQwen2-preview",
  "category": "tools",
  "company": "Hugging Face",
  "imageUrl": null,
  "tags": [
    "AI",
    "Transformer"
  ],
  "featured": false
}