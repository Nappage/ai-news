{
  "id": "md7dm69oqgnldq05ejj",
  "title": "Using Transfer-based Language Models to Detect Hateful and Offensive Language Online",
  "summary": "Distinguishing hate speech from non-hate offensive language is challenging, as hate speech not always includes offensive slurs and offensive language not always express hate. Here, four deep learners based on the Bidirectional Encoder Representations from Transformers (BERT), with either general or ...",
  "content": "Distinguishing hate speech from non-hate offensive language is challenging, as hate speech not always includes offensive slurs and offensive language not always express hate. Here, four deep learners based on the Bidirectional Encoder Representations from Transformers (BERT), with either general or domain-specific language models, were tested against two datasets containing tweets labelled as either ‘Hateful’, ‘Normal’ or ‘Offensive’. The results indicate that the attention-based models profoundly confuse hate speech with offensive and normal language. However, the pre-trained models outperform state-of-the-art results in terms of accurately predicting the hateful instances.",
  "publishedAt": "1970-01-01T00:00:00.000Z",
  "source": "Papers with Code",
  "sourceUrl": "https://aclanthology.org/2020.alw-1.3.pdf",
  "category": "research",
  "company": "Community",
  "imageUrl": null,
  "tags": [
    "AI",
    "Transformer",
    "BERT"
  ],
  "featured": false
}